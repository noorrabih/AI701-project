{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":54662,"databundleVersionId":6169864,"sourceType":"competition"},{"sourceId":5632975,"sourceType":"datasetVersion","datasetId":3238926},{"sourceId":6146260,"sourceType":"datasetVersion","datasetId":3521629},{"sourceId":6359012,"sourceType":"datasetVersion","datasetId":3662908},{"sourceId":6476221,"sourceType":"datasetVersion","datasetId":3741139},{"sourceId":6536614,"sourceType":"datasetVersion","datasetId":3778974},{"sourceId":6537899,"sourceType":"datasetVersion","datasetId":3779689},{"sourceId":6602268,"sourceType":"datasetVersion","datasetId":3809212},{"sourceId":6980692,"sourceType":"datasetVersion","datasetId":4011592}],"dockerImageVersionId":30559,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RoBERTa + Wikipedia RAG\n","metadata":{}},{"cell_type":"code","source":"# Installing offline dependencies\n!pip install -U --no-deps /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install -U --no-deps /kaggle/input/datasets-214/datasets-2.14.5-py3-none-any.whl\n!pip install -U --no-deps /kaggle/input/optimum-113/optimum-1.13.2-py3-none-any.whl\n!pip install -U --no-deps /kaggle/input/transformers-432/transformers-4.32.1-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:08:00.484394Z","iopub.execute_input":"2023-11-16T11:08:00.484742Z","iopub.status.idle":"2023-11-16T11:08:09.967984Z","shell.execute_reply.started":"2023-11-16T11:08:00.484715Z","shell.execute_reply":"2023-11-16T11:08:09.966975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport logging\nfrom time import time\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor\nfrom threading import Condition\nimport ctypes\nfrom functools import partial\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\n# For RAG\nimport faiss\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_from_disk, Dataset\n\nNUM_TITLES = 5\nMAX_SEQ_LEN = 512\nMODEL_PATH = \"/kaggle/input/bge-small-faiss/\"\n\n# For LLM\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AutoModel\nfrom accelerate import init_empty_weights\nfrom accelerate.utils.modeling import set_module_tensor_to_device\nfrom safetensors.torch import load_file\nfrom optimum.bettertransformer import BetterTransformer\n\nN_BATCHES = 5\nMAX_LENGTH = 4096\nMAX_CONTEXT = 1200\n# With NUM_TITLES = 5, the median lenght of a context if 1100 tokens (Q1: 900, Q3: 1400)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:08:38.445571Z","iopub.execute_input":"2023-11-16T11:08:38.445910Z","iopub.status.idle":"2023-11-16T11:08:38.454204Z","shell.execute_reply.started":"2023-11-16T11:08:38.445885Z","shell.execute_reply":"2023-11-16T11:08:38.453060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to clean RAM & vRAM\ndef clean_memory():\n    gc.collect()\n    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n    torch.cuda.empty_cache()\n\n# # Variable used to avoid running the notebook for 3 hours when submitting. Credit : CPMP\n# df = pd.read_csv('/kaggle/input/bert-with-rag/train_context.csv')\n\n# IS_TEST_SET = len(df) != 200\n\n# Uncomment this to see results on the train set\ndf = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/train.csv\", index_col=\"id\")\nIS_TEST_SET = True\nN_BATCHES = 1","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:08:40.922432Z","iopub.execute_input":"2023-11-16T11:08:40.922798Z","iopub.status.idle":"2023-11-16T11:08:40.936135Z","shell.execute_reply.started":"2023-11-16T11:08:40.922768Z","shell.execute_reply":"2023-11-16T11:08:40.935287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# New SentenceTransformer class similar to the one used in @Mg√∂ksu notebook but relying on the transformers library only\n\nclass SentenceTransformer:\n    def __init__(self, checkpoint, device=\"cuda:0\"):\n        self.device = device\n        self.checkpoint = checkpoint\n        self.model = AutoModel.from_pretrained(checkpoint).to(self.device).half()\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n    def transform(self, batch):\n        tokens = self.tokenizer(batch[\"text\"], truncation=True, padding=True, return_tensors=\"pt\", max_length=MAX_SEQ_LEN)\n        return tokens.to(self.device)  \n\n    def get_dataloader(self, sentences, batch_size=32):\n        sentences = [\"Represent this sentence for searching relevant passages: \" + x for x in sentences]\n        dataset = Dataset.from_dict({\"text\": sentences})\n        dataset.set_transform(self.transform)\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n        return dataloader\n\n    def encode(self, sentences, show_progress_bar=False, batch_size=32):\n        dataloader = self.get_dataloader(sentences, batch_size=batch_size)\n        pbar = tqdm(dataloader) if show_progress_bar else dataloader\n\n        embeddings = []\n        for batch in pbar:\n            with torch.no_grad():\n                e = self.model(**batch).pooler_output\n                e = F.normalize(e, p=2, dim=1)\n                embeddings.append(e.detach().cpu().numpy())\n        embeddings = np.concatenate(embeddings, axis=0)\n        return embeddings","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:08:43.872882Z","iopub.execute_input":"2023-11-16T11:08:43.873279Z","iopub.status.idle":"2023-11-16T11:08:43.884358Z","shell.execute_reply.started":"2023-11-16T11:08:43.873247Z","shell.execute_reply":"2023-11-16T11:08:43.883323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if IS_TEST_SET:\n    # Load embedding model\nstart = time()\nprint(f\"Starting prompt embedding, t={time() - start :.1f}s\")\nmodel = SentenceTransformer(MODEL_PATH, device=\"cuda:0\")\n\n# Get embeddings of prompts\nf = lambda row : \" \".join([row[\"prompt\"], row[\"A\"], row[\"B\"], row[\"C\"], row[\"D\"], row[\"E\"]])\ninputs = df.apply(f, axis=1).values # better results than prompt only\nprompt_embeddings = model.encode(inputs, show_progress_bar=False)\n\n# Search closest sentences in the wikipedia index \nprint(f\"Loading faiss index, t={time() - start :.1f}s\")\nfaiss_index = faiss.read_index(MODEL_PATH + '/faiss.index')\n# faiss_index = faiss.index_cpu_to_all_gpus(faiss_index) # causes OOM, and not that long on CPU\n\nprint(f\"Starting text search, t={time() - start :.1f}s\")\nsearch_index = faiss_index.search(np.float32(prompt_embeddings), NUM_TITLES)[1]\n\nprint(f\"Starting context extraction, t={time() - start :.1f}s\")\ndataset = load_from_disk(\"/kaggle/input/all-paraphs-parsed-expanded\")\nfor i in range(len(df)):\n    df.loc[i, \"context\"] = \"-\" + \"\\n-\".join([dataset[int(j)][\"text\"] for j in search_index[i]])\n\n# Free memory\nfaiss_index.reset()\ndel faiss_index, prompt_embeddings, model, dataset\nclean_memory()\nprint(f\"Context added, t={time() - start :.1f}s\")","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:08:48.028155Z","iopub.execute_input":"2023-11-16T11:08:48.028544Z","iopub.status.idle":"2023-11-16T11:08:57.439442Z","shell.execute_reply.started":"2023-11-16T11:08:48.028515Z","shell.execute_reply":"2023-11-16T11:08:57.438482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nfeatures = ['prompt', 'A', 'B', 'C', 'D', 'E', 'context']\n\ny = df['answer']\nX = df[features]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:09:48.294000Z","iopub.execute_input":"2023-11-16T11:09:48.294886Z","iopub.status.idle":"2023-11-16T11:09:48.303891Z","shell.execute_reply.started":"2023-11-16T11:09:48.294851Z","shell.execute_reply":"2023-11-16T11:09:48.302942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n# comment the line below if you are finetuning RoBERTa\nmodel_dir = '/kaggle/input/finetuned-roberta-rag-k-fold/finetuned-roberta/fold_4/checkpoint-480' #you need to change this line based on the path to your finetuned model\n# comment the line below if you are using a finetuned RoBERTa\n# model_dir = 'roberta-base'\ntokenizer = AutoTokenizer.from_pretrained(model_dir)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:10:15.518682Z","iopub.execute_input":"2023-11-16T11:10:15.519382Z","iopub.status.idle":"2023-11-16T11:10:15.658677Z","shell.execute_reply.started":"2023-11-16T11:10:15.519347Z","shell.execute_reply":"2023-11-16T11:10:15.657858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"options = 'ABCDE'\nindices = list(range(5))\n\noption_index = {option: index for option, index in zip(options, indices)}\nindex_option = {index: option for option, index in zip(options, indices)}\ntrain_dataset = Dataset.from_pandas(df)\n\ndef preprocess(examples):\n    # adding context to our prompt\n    text = examples['context'] + examples['prompt']\n    first_sentences = [text] * 5\n    second_sentences = []\n    for option in options:\n        second_sentences.append(examples[option])\n    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n    tokenized_examples['label'] = option_index[examples['answer']]\n    return tokenized_examples\n\ntokenized_train_ds = train_dataset.map(preprocess, batched=False, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer', 'context'])","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:10:17.421680Z","iopub.execute_input":"2023-11-16T11:10:17.422058Z","iopub.status.idle":"2023-11-16T11:10:19.667151Z","shell.execute_reply.started":"2023-11-16T11:10:17.422004Z","shell.execute_reply":"2023-11-16T11:10:19.666269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Following datacollator (adapted from https://huggingface.co/docs/transformers/tasks/multiple_choice)\n# will dynamically pad our questions at batch-time so we don't have to make every question the length\n# of our longest question.\nfrom dataclasses import dataclass\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom typing import Optional, Union\nimport torch\n\n@dataclass\nclass DataCollatorForMultipleChoice:\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    \n    def __call__(self, features):\n        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n        labels = [feature.pop(label_name) for feature in features]\n        batch_size = len(features)\n        num_choices = len(features[0]['input_ids'])\n        flattened_features = [\n            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n        ]\n        flattened_features = sum(flattened_features, [])\n        \n        batch = self.tokenizer.pad(\n            flattened_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors='pt',\n        )\n        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n        return batch","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:10:22.041933Z","iopub.execute_input":"2023-11-16T11:10:22.042313Z","iopub.status.idle":"2023-11-16T11:10:22.053056Z","shell.execute_reply.started":"2023-11-16T11:10:22.042282Z","shell.execute_reply":"2023-11-16T11:10:22.052121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\nmodel = AutoModelForMultipleChoice.from_pretrained(model_dir)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:10:28.932069Z","iopub.execute_input":"2023-11-16T11:10:28.932431Z","iopub.status.idle":"2023-11-16T11:10:33.798346Z","shell.execute_reply.started":"2023-11-16T11:10:28.932402Z","shell.execute_reply":"2023-11-16T11:10:33.797328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\n\ndef compute_metrics(p):\n    # p.predictions are the predictions, p.label_ids are the true labels\n    preds = p.predictions.argmax(-1)\n    \n    accuracy = accuracy_score(p.label_ids, preds)\n    f1 = f1_score(p.label_ids, preds, average='weighted')  # Use 'binary' for binary classification\n    return {\n        'accuracy': accuracy,\n        'f1': f1\n    }","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:10:37.253698Z","iopub.execute_input":"2023-11-16T11:10:37.254430Z","iopub.status.idle":"2023-11-16T11:10:37.259707Z","shell.execute_reply.started":"2023-11-16T11:10:37.254393Z","shell.execute_reply":"2023-11-16T11:10:37.258826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# uncomment if your notebook has access to Internet and you want to measure CPU/GPU usage\n# !pip install psutil GPUtil","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:45:38.171934Z","iopub.execute_input":"2023-11-16T08:45:38.172637Z","iopub.status.idle":"2023-11-16T08:45:52.500492Z","shell.execute_reply.started":"2023-11-16T08:45:38.172600Z","shell.execute_reply":"2023-11-16T08:45:52.499508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# uncomment if your notebook has access to Internet and you want to measure CPU/GPU usage\n# import threading\n# import psutil\n# import GPUtil\n# import time\n# import matplotlib.pyplot as plt\n# # Function to monitor CPU and GPU usage\n# def monitor_resources(stop_event, interval=1):\n#     cpu_usage = []\n#     gpu_usage = []\n#     timestamps = []\n\n#     while not stop_event.is_set():\n#         timestamps.append(time.time())\n#         cpu_usage.append(psutil.cpu_percent(interval=None))\n\n#         gpus = GPUtil.getGPUs()\n#         gpu_usage.append(gpus[0].load * 100 if gpus else 0)  # Assumes one GPU\n\n#         time.sleep(interval)\n\n#     plt.figure(figsize=(10, 6))\n#     plt.plot(timestamps, cpu_usage, label='CPU Usage (%)')\n#     plt.plot(timestamps, gpu_usage, label='GPU Usage (%)')\n#     plt.xlabel('Time (s)')\n#     plt.ylabel('Usage (%)')\n#     plt.title('CPU and GPU Usage Over Time')\n#     plt.legend()\n#     plt.grid(True)\n#     plt.show()\n    \n#     print(timestamps)\n#     print(cpu_usage)\n#     print(gpu_usage)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:45:58.488106Z","iopub.execute_input":"2023-11-16T08:45:58.488512Z","iopub.status.idle":"2023-11-16T08:45:58.502978Z","shell.execute_reply.started":"2023-11-16T08:45:58.488478Z","shell.execute_reply":"2023-11-16T08:45:58.502050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from datasets import Dataset\n\nfrom transformers import TrainerCallback\n\n# this class allows us to save the path to the best performing model\nclass BestModelTracker(TrainerCallback):\n    def __init__(self):\n        self.best_loss = float('inf')\n        self.best_epoch = 0\n        self.best_model_path = None\n\n    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n        # Check if the current eval loss is lower than the best loss\n        if metrics and metrics.get(\"eval_loss\", float('inf')) < self.best_loss:\n            self.best_loss = metrics[\"eval_loss\"]\n            self.best_epoch = state.epoch\n            self.best_model_path = f'{args.output_dir}/checkpoint-{state.global_step}'\n            print(f\"New best model found at epoch {self.best_epoch} with eval loss: {self.best_loss}\")\n\n# uncomment when you are finetuning the base model\n\n# model_dir = '/kaggle/working/finetuned-roberta'\n# best_model_tracker = BestModelTracker()\n            \n# def k_fold_cross_validation(dataset, num_folds=5):\n#     # Split the dataset into k folds\n#     dataset = dataset.shuffle()  # Ensure the dataset is randomly shuffled\n#     fold_size = len(dataset) // num_folds\n\n#     for fold in range(num_folds):\n#         model = AutoModelForMultipleChoice.from_pretrained('roberta-base')\n#         # Split the dataset into training and validation for the current fold\n#         train_indices = list(range(0, fold * fold_size)) + list(range((fold + 1) * fold_size, len(dataset)))\n#         val_indices = list(range(fold * fold_size, (fold + 1) * fold_size))\n\n#         train_dataset = dataset.select(train_indices)\n#         eval_dataset = dataset.select(val_indices)\n\n#         # Define the training arguments\n#         training_args = TrainingArguments(\n#             output_dir=f'{model_dir}/fold_{fold}',\n#             evaluation_strategy=\"epoch\",\n#             logging_strategy=\"epoch\",\n#             save_strategy=\"epoch\",\n#             metric_for_best_model='eval_loss',  # Choose the metric to determine the best model\n#             greater_is_better=False,  # Set to True if higher metric score is better\n#             load_best_model_at_end=True,\n#             learning_rate=5e-5,\n#             per_device_train_batch_size=4,\n#             per_device_eval_batch_size=4,\n#             num_train_epochs=15,\n#             weight_decay=0.01,\n#             report_to='none',\n#             save_total_limit=1\n#         )\n\n#         # Initialize the Trainer\n#         trainer = Trainer(\n#             model=model,\n#             args=training_args,\n#             train_dataset=train_dataset,\n#             eval_dataset=eval_dataset,\n#             tokenizer=tokenizer,\n#             data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n#             compute_metrics=compute_metrics,\n#             callbacks=[best_model_tracker]\n#         )\n        \n#         print(f'Start training for {fold} fold')\n#         # Monitoring setup\n#         stop_event = threading.Event()\n#         monitor_thread = threading.Thread(target=monitor_resources, args=(stop_event,))\n\n#         # Start monitoring\n#         monitor_thread.start()\n\n#         # Train the model\n#         trainer.train()\n        \n#         # Stop monitoring\n#         stop_event.set()\n#         monitor_thread.join()\n        \n#         print(f'Stopped training for {fold} fold')\n\n#         # Optionally, save the model after each fold\n# #         model.save_pretrained(f'{model_dir}/fold_{fold}_model')\n        \n# k_fold_cross_validation(tokenized_train_ds)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:46:43.615558Z","iopub.execute_input":"2023-11-16T08:46:43.616311Z","iopub.status.idle":"2023-11-16T09:48:07.684686Z","shell.execute_reply.started":"2023-11-16T08:46:43.616278Z","shell.execute_reply":"2023-11-16T09:48:07.683733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# uncomment if you want to run a finetuned model\nmodel = AutoModelForMultipleChoice.from_pretrained(model_dir)\ntraining_args = TrainingArguments(\n        output_dir=f'{model_dir}',\n        evaluation_strategy=\"epoch\",\n        logging_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        metric_for_best_model='eval_loss',  # Choose the metric to determine the best model\n        greater_is_better=False,  # Set to True if higher metric score is better\n        load_best_model_at_end=True,\n        learning_rate=5e-5,\n        per_device_train_batch_size=4,\n        per_device_eval_batch_size=4,\n        num_train_epochs=15,\n        weight_decay=0.01,\n        report_to='none',\n        save_total_limit=1\n    )\n\n    # Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_ds,\n    eval_dataset=tokenized_train_ds,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:11:13.389482Z","iopub.execute_input":"2023-11-16T11:11:13.389821Z","iopub.status.idle":"2023-11-16T11:11:13.541342Z","shell.execute_reply.started":"2023-11-16T11:11:13.389795Z","shell.execute_reply":"2023-11-16T11:11:13.540349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# uncomment if you want to run the best model from k folds\n# model = AutoModelForMultipleChoice.from_pretrained(best_model_tracker.best_model_path)\n# training_args = TrainingArguments(\n#         output_dir=f'{model_dir}',\n#         evaluation_strategy=\"epoch\",\n#         logging_strategy=\"epoch\",\n#         save_strategy=\"epoch\",\n#         metric_for_best_model='eval_loss',  # Choose the metric to determine the best model\n#         greater_is_better=False,  # Set to True if higher metric score is better\n#         load_best_model_at_end=True,\n#         learning_rate=5e-5,\n#         per_device_train_batch_size=4,\n#         per_device_eval_batch_size=4,\n#         num_train_epochs=15,\n#         weight_decay=0.01,\n#         report_to='none',\n#         save_total_limit=1\n#     )\n\n#     # Initialize the Trainer\n# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=tokenized_train_ds,\n#     eval_dataset=tokenized_train_ds,\n#     tokenizer=tokenizer,\n#     data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n#     compute_metrics=compute_metrics,\n# )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# uncomment if you want to make predictions on the training set and measure CPU/GPU usage\n# # Monitoring setup\n# stop_event = threading.Event()\n# monitor_thread = threading.Thread(target=monitor_resources, args=(stop_event,))\n\n# # Start monitoring\n# monitor_thread.start()\n\n# # Start training\n# predictions = trainer.predict(tokenized_train_ds)\n\n# # Stop monitoring\n# stop_event.set()\n# monitor_thread.join()\n\n# # At this point, the monitoring thread has completed\n# print(\"Prediction inference and monitoring completed.\")","metadata":{"execution":{"iopub.status.busy":"2023-11-16T09:52:25.011476Z","iopub.execute_input":"2023-11-16T09:52:25.012275Z","iopub.status.idle":"2023-11-16T09:52:43.535195Z","shell.execute_reply.started":"2023-11-16T09:52:25.012233Z","shell.execute_reply":"2023-11-16T09:52:43.534290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#function to generate three letter options\nimport numpy as np\ndef predictions_to_map_output(predictions):\n    sorted_answer_indices = np.argsort(-predictions)\n    top_answer_indices = sorted_answer_indices[:,:3] # Get the first three answers in each row\n    top_answers = np.vectorize(index_option.get)(top_answer_indices)\n    return np.apply_along_axis(lambda row: ' '.join(row), 1, top_answers)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:11:33.641116Z","iopub.execute_input":"2023-11-16T11:11:33.641989Z","iopub.status.idle":"2023-11-16T11:11:33.647787Z","shell.execute_reply.started":"2023-11-16T11:11:33.641954Z","shell.execute_reply":"2023-11-16T11:11:33.646744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#estimate MAP@3\ndef map_at_3(predictions, true_answers):\n    # Convert predictions to top 3 answers\n    top_3_predictions = predictions_to_map_output(predictions.predictions)\n\n    # Calculate average precision for each instance\n    average_precisions = []\n    for i in range(len(true_answers)):\n        true_answer = true_answers[i]\n        true_answer = options[true_answer]\n        predicted_answers = top_3_predictions[i].split(\" \")\n\n        if true_answer in predicted_answers:\n            index_of_true_answer = predicted_answers.index(true_answer)\n            precision_at_index = 1 / (index_of_true_answer + 1)\n            average_precisions.append(precision_at_index)\n        else:\n            average_precisions.append(0)\n\n    # Calculate mean average precision at 3\n    map_3 = np.mean(average_precisions)\n    return map_3","metadata":{"execution":{"iopub.status.busy":"2023-11-16T09:53:18.810494Z","iopub.execute_input":"2023-11-16T09:53:18.811338Z","iopub.status.idle":"2023-11-16T09:53:18.821609Z","shell.execute_reply.started":"2023-11-16T09:53:18.811306Z","shell.execute_reply":"2023-11-16T09:53:18.820705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#uncomment if you want to get MAP@3 score for the training set\n# true_answers = tokenized_train_ds['label']\n# map_3_score = map_at_3(predictions, true_answers)\n# print(f\"MAP@3 Score: {map_3_score}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:11:39.345808Z","iopub.execute_input":"2023-11-16T11:11:39.346189Z","iopub.status.idle":"2023-11-16T11:11:39.359325Z","shell.execute_reply.started":"2023-11-16T11:11:39.346159Z","shell.execute_reply":"2023-11-16T11:11:39.358406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get context for a test dataset\nimport time\nstart = time.time()\nprint(f\"Starting prompt embedding, t={time.time() - start :.1f}s\")\nmodel = SentenceTransformer(MODEL_PATH, device=\"cuda:0\")\n\n# Get embeddings of prompts\nf = lambda row : \" \".join([row[\"prompt\"], row[\"A\"], row[\"B\"], row[\"C\"], row[\"D\"], row[\"E\"]])\ninputs = test_df.apply(f, axis=1).values # better results than prompt only\nprompt_embeddings = model.encode(inputs, show_progress_bar=False)\n\n# Search closest sentences in the wikipedia index \nprint(f\"Loading faiss index, t={time.time() - start :.1f}s\")\nfaiss_index = faiss.read_index(MODEL_PATH + '/faiss.index')\n# faiss_index = faiss.index_cpu_to_all_gpus(faiss_index) # causes OOM, and not that long on CPU\n\nprint(f\"Starting text search, t={time.time() - start :.1f}s\")\nsearch_index = faiss_index.search(np.float32(prompt_embeddings), NUM_TITLES)[1]\n\nprint(f\"Starting context extraction, t={time.time() - start :.1f}s\")\ndataset = load_from_disk(\"/kaggle/input/all-paraphs-parsed-expanded\")\nfor i in range(len(test_df)):\n    test_df.loc[i, \"context\"] = \"-\" + \"\\n-\".join([dataset[int(j)][\"text\"] for j in search_index[i]])\n\n# Free memory\nfaiss_index.reset()\ndel faiss_index, prompt_embeddings, model, dataset\nclean_memory()\nprint(f\"Context added, t={time.time() - start :.1f}s\")","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:11:41.228782Z","iopub.execute_input":"2023-11-16T11:11:41.229622Z","iopub.status.idle":"2023-11-16T11:11:50.473994Z","shell.execute_reply.started":"2023-11-16T11:11:41.229588Z","shell.execute_reply":"2023-11-16T11:11:50.472973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['answer'] = 'A'\n\n# Other than that we'll preprocess it in the same way we preprocessed test.csv\ntest_ds = Dataset.from_pandas(test_df)\ntokenized_test_ds = test_ds.map(preprocess, batched=False, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:11:55.305584Z","iopub.execute_input":"2023-11-16T11:11:55.306212Z","iopub.status.idle":"2023-11-16T11:11:57.361964Z","shell.execute_reply.started":"2023-11-16T11:11:55.306172Z","shell.execute_reply":"2023-11-16T11:11:57.361046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# uncomment this if you want to measure CPU/GPU usage of making predictions on the test set\n\n# stop_event = threading.Event()\n# monitor_thread = threading.Thread(target=monitor_resources, args=(stop_event,))\n\n# # Start monitoring\n# monitor_thread.start()\n\n# # Start training\n# test_predictions = trainer.predict(tokenized_test_ds)\n# # Stop monitoring\n# stop_event.set()\n# monitor_thread.join()\n\n# # At this point, the monitoring thread has completed\n# print(\"Test prediction inference and monitoring completed.\")\n\n# run only this to make predicitons on the test set\ntest_predictions = trainer.predict(tokenized_test_ds)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:12:04.765303Z","iopub.execute_input":"2023-11-16T11:12:04.766034Z","iopub.status.idle":"2023-11-16T11:12:22.943451Z","shell.execute_reply.started":"2023-11-16T11:12:04.765995Z","shell.execute_reply":"2023-11-16T11:12:22.942541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a submission file\nsubmission_df = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/sample_submission.csv')\ntest_df['prediction'] = predictions_to_map_output(test_predictions.predictions)\nsubmission_df['prediction'] = test_df['prediction']\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:12:24.160283Z","iopub.execute_input":"2023-11-16T11:12:24.161100Z","iopub.status.idle":"2023-11-16T11:12:24.176108Z","shell.execute_reply.started":"2023-11-16T11:12:24.161065Z","shell.execute_reply":"2023-11-16T11:12:24.175282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:12:28.936781Z","iopub.execute_input":"2023-11-16T11:12:28.937240Z","iopub.status.idle":"2023-11-16T11:12:28.943138Z","shell.execute_reply.started":"2023-11-16T11:12:28.937208Z","shell.execute_reply":"2023-11-16T11:12:28.942283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# uncomment this if you are finetuning the model. this will ensure that only the best version of the model out of all k folds will be saved\n# import os\n\n# def remove_folder_contents(folder, path_to_keep):\n#     for the_file in os.listdir(folder):\n#         file_path = os.path.join(folder, the_file)\n#         # Skip the path that we want to keep\n#         if file_path == path_to_keep:\n#             continue\n#         try:\n#             if os.path.isfile(file_path):\n#                 os.unlink(file_path)\n#             elif os.path.isdir(file_path):\n#                 remove_folder_contents(file_path, path_to_keep)\n#                 os.rmdir(file_path)\n#         except Exception as e:\n#             print(e)\n\n# folder_path = '/kaggle/working'\n# path_to_keep = best_model_tracker.best_model_path\n\n# remove_folder_contents(folder_path, path_to_keep)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T09:57:54.760825Z","iopub.execute_input":"2023-11-16T09:57:54.761553Z","iopub.status.idle":"2023-11-16T09:57:55.417763Z","shell.execute_reply.started":"2023-11-16T09:57:54.761519Z","shell.execute_reply":"2023-11-16T09:57:55.416677Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
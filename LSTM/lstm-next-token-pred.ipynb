{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":54662,"databundleVersionId":6169864,"sourceType":"competition"},{"sourceId":42887,"sourceType":"datasetVersion","datasetId":32801},{"sourceId":5632975,"sourceType":"datasetVersion","datasetId":3238926},{"sourceId":6146260,"sourceType":"datasetVersion","datasetId":3521629},{"sourceId":6359012,"sourceType":"datasetVersion","datasetId":3662908},{"sourceId":6363321,"sourceType":"datasetVersion","datasetId":3638263},{"sourceId":6536614,"sourceType":"datasetVersion","datasetId":3778974},{"sourceId":6537899,"sourceType":"datasetVersion","datasetId":3779689},{"sourceId":6602268,"sourceType":"datasetVersion","datasetId":3809212}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-11-25T16:30:39.096309Z","iopub.execute_input":"2023-11-25T16:30:39.096726Z","iopub.status.idle":"2023-11-25T16:30:39.695135Z","shell.execute_reply.started":"2023-11-25T16:30:39.096692Z","shell.execute_reply":"2023-11-25T16:30:39.693711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Answer generation with LSTM from context and prompt","metadata":{}},{"cell_type":"code","source":"data1 = \"/kaggle/input/sci-or-not-sci-hypthesis-testing-pack/6000_all_categories_questions_with_excerpts.csv\"\ndata2=\"/kaggle/input/sci-or-not-sci-hypthesis-testing-pack/6000_wiki_en_sci_questions_with_excerpts.csv\"\ndata = \"/kaggle/input/kaggle-llm-science-exam/train.csv\"","metadata":{"execution":{"iopub.status.busy":"2023-11-25T16:30:39.697558Z","iopub.execute_input":"2023-11-25T16:30:39.698108Z","iopub.status.idle":"2023-11-25T16:30:39.703971Z","shell.execute_reply.started":"2023-11-25T16:30:39.698073Z","shell.execute_reply":"2023-11-25T16:30:39.702523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = pd.concat([pd.read_csv(data1),pd.read_csv(data2)])\n# df.reset_index(drop=True, inplace=True)\ndf = pd.read_csv(data1)[:2000]","metadata":{"execution":{"iopub.status.busy":"2023-11-25T16:30:47.362647Z","iopub.execute_input":"2023-11-25T16:30:47.363135Z","iopub.status.idle":"2023-11-25T16:30:47.557124Z","shell.execute_reply.started":"2023-11-25T16:30:47.363097Z","shell.execute_reply":"2023-11-25T16:30:47.555844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def concat_all(x):\n    ans = x[x['answer']]\n    concat_str = x['wikipedia_excerpt'].split(':')[1].strip() + '. '+ x['prompt'] + ' <ANS>' + str(ans)\n    return concat_str","metadata":{"execution":{"iopub.status.busy":"2023-11-25T16:30:51.513633Z","iopub.execute_input":"2023-11-25T16:30:51.514128Z","iopub.status.idle":"2023-11-25T16:30:51.520832Z","shell.execute_reply.started":"2023-11-25T16:30:51.514082Z","shell.execute_reply":"2023-11-25T16:30:51.519227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['concat_prompt_ans_exc'] = df.apply(lambda x: concat_all(x), axis = 1)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T16:30:51.525667Z","iopub.execute_input":"2023-11-25T16:30:51.526548Z","iopub.status.idle":"2023-11-25T16:30:51.608448Z","shell.execute_reply.started":"2023-11-25T16:30:51.526499Z","shell.execute_reply":"2023-11-25T16:30:51.607065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\ndef generate_random_numbers(count,lgth):\n    random_numbers = [random.randint(0, lgth) for _ in range(count)]\n    return random_numbers\n\n# Example: Generate a list of 10 random numbers\nlgth = len(df)-1\nrandom_list = generate_random_numbers(int(0.2*len(df)),lgth)\ndf['test']=False\ndf['test'].loc[random_list]=True","metadata":{"execution":{"iopub.status.busy":"2023-11-25T16:30:51.611030Z","iopub.execute_input":"2023-11-25T16:30:51.611470Z","iopub.status.idle":"2023-11-25T16:30:51.628804Z","shell.execute_reply.started":"2023-11-25T16:30:51.611431Z","shell.execute_reply":"2023-11-25T16:30:51.627546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tmp","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-25T16:30:51.630332Z","iopub.execute_input":"2023-11-25T16:30:51.631466Z","iopub.status.idle":"2023-11-25T16:30:51.639784Z","shell.execute_reply.started":"2023-11-25T16:30:51.631412Z","shell.execute_reply":"2023-11-25T16:30:51.638869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nimport regex as re\ndef clean_txt(article):\n  article = re.sub(r'[^\\w\\s]', '', article)\n  article = re.sub(r'[_]', '', article)\n  tokens = article.split()\n  tokens = [t for t in tokens if t not in string.punctuation]\n#   tokens = [t for t in tokens if t.isalpha()]\n  tokens = [t.lower() for t in tokens]\n  return tokens\narticles = []\ndescription = df['concat_prompt_ans_exc']\n\nfor i in range(len(description)):\n  art = clean_txt(description[i])\n  article = []\n  for word in art:\n    article.append(word)\n  articles.append(article)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T16:30:51.642538Z","iopub.execute_input":"2023-11-25T16:30:51.643137Z","iopub.status.idle":"2023-11-25T16:30:51.874143Z","shell.execute_reply.started":"2023-11-25T16:30:51.643102Z","shell.execute_reply":"2023-11-25T16:30:51.872720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"articles_flatten = [k for a in articles for k in a ]","metadata":{"execution":{"iopub.status.busy":"2023-11-25T16:30:51.875708Z","iopub.execute_input":"2023-11-25T16:30:51.876218Z","iopub.status.idle":"2023-11-25T16:30:51.891856Z","shell.execute_reply.started":"2023-11-25T16:30:51.876169Z","shell.execute_reply":"2023-11-25T16:30:51.890177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('total Tokens',len(articles_flatten)) #total Tokens 11503\nprint('total Tokens',len(set(articles_flatten))) #total Tokens 1918","metadata":{"execution":{"iopub.status.busy":"2023-11-25T16:30:51.894299Z","iopub.execute_input":"2023-11-25T16:30:51.894817Z","iopub.status.idle":"2023-11-25T16:30:51.928926Z","shell.execute_reply.started":"2023-11-25T16:30:51.894772Z","shell.execute_reply":"2023-11-25T16:30:51.927548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maxlen = 0\nfor i in articles:\n    if maxlen<len(i):\n        maxlen = len(i)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T16:30:51.930589Z","iopub.execute_input":"2023-11-25T16:30:51.931008Z","iopub.status.idle":"2023-11-25T16:30:51.944080Z","shell.execute_reply.started":"2023-11-25T16:30:51.930975Z","shell.execute_reply":"2023-11-25T16:30:51.942541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"length = 50\nseq = []\nfor i in articles:\n    if length >= len(i):\n        lines = ' '.join(i)\n        seq.append(lines)\n    else:\n        for j in range(length, len(i)+1):\n            sequence = i[j-length:j]\n            lines = ' '.join(sequence)\n            seq.append(lines)\nprint('Total Sequence',len(seq)) #Total Sequence 11497","metadata":{"execution":{"iopub.status.busy":"2023-11-25T16:30:51.945920Z","iopub.execute_input":"2023-11-25T16:30:51.946708Z","iopub.status.idle":"2023-11-25T16:30:52.150939Z","shell.execute_reply.started":"2023-11-25T16:30:51.946661Z","shell.execute_reply":"2023-11-25T16:30:52.149477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Back","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(seq)\ntokens = tokenizer.texts_to_sequences(seq)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T16:30:52.152853Z","iopub.execute_input":"2023-11-25T16:30:52.153542Z","iopub.status.idle":"2023-11-25T16:31:14.577650Z","shell.execute_reply.started":"2023-11-25T16:30:52.153496Z","shell.execute_reply":"2023-11-25T16:31:14.575950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(tokenizer.word_index) + 1\nprint(vocab_size) # 1919","metadata":{"execution":{"iopub.status.busy":"2023-11-25T16:31:14.581878Z","iopub.execute_input":"2023-11-25T16:31:14.582731Z","iopub.status.idle":"2023-11-25T16:31:14.589173Z","shell.execute_reply.started":"2023-11-25T16:31:14.582693Z","shell.execute_reply":"2023-11-25T16:31:14.588252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maxlen = 0\nfor i in tokens:\n    if maxlen<len(i):\n        maxlen = len(i)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T16:31:14.590830Z","iopub.execute_input":"2023-11-25T16:31:14.591538Z","iopub.status.idle":"2023-11-25T16:31:14.613846Z","shell.execute_reply.started":"2023-11-25T16:31:14.591503Z","shell.execute_reply":"2023-11-25T16:31:14.612648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\nencoded = pad_sequences(tokens,maxlen=maxlen,truncating='pre')","metadata":{"execution":{"iopub.status.busy":"2023-11-25T16:31:14.615220Z","iopub.execute_input":"2023-11-25T16:31:14.615623Z","iopub.status.idle":"2023-11-25T16:31:15.027257Z","shell.execute_reply.started":"2023-11-25T16:31:14.615590Z","shell.execute_reply":"2023-11-25T16:31:15.026036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X,y = encoded[:,:-1],encoded[:,-1]","metadata":{"execution":{"iopub.status.busy":"2023-11-25T16:31:15.029955Z","iopub.execute_input":"2023-11-25T16:31:15.030982Z","iopub.status.idle":"2023-11-25T16:31:15.037263Z","shell.execute_reply.started":"2023-11-25T16:31:15.030933Z","shell.execute_reply":"2023-11-25T16:31:15.035682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils import to_categorical","metadata":{"execution":{"iopub.status.busy":"2023-11-25T16:31:15.039340Z","iopub.execute_input":"2023-11-25T16:31:15.040618Z","iopub.status.idle":"2023-11-25T16:31:15.049829Z","shell.execute_reply.started":"2023-11-25T16:31:15.040559Z","shell.execute_reply":"2023-11-25T16:31:15.048820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = to_categorical(y,num_classes=vocab_size)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T16:31:15.051565Z","iopub.execute_input":"2023-11-25T16:31:15.051938Z","iopub.status.idle":"2023-11-25T16:31:15.332294Z","shell.execute_reply.started":"2023-11-25T16:31:15.051910Z","shell.execute_reply":"2023-11-25T16:31:15.331409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-25T16:31:15.333407Z","iopub.execute_input":"2023-11-25T16:31:15.334785Z","iopub.status.idle":"2023-11-25T16:31:15.340156Z","shell.execute_reply.started":"2023-11-25T16:31:15.334687Z","shell.execute_reply":"2023-11-25T16:31:15.339278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_index = dict()\nf = open('/kaggle/input/glove6b/glove.6B.200d.txt',mode='rt',encoding='utf-8')\nimport numpy as np\nfor line in f:\n    values = line.split()\n    words = values[0]\n    coefs = np.asarray(values[1:],dtype='float32')\n    embeddings_index[words] = coefs\nf.close()\nprint('Loaded word vectors',len(embeddings_index)) \n#Loaded word vectors 400000","metadata":{"execution":{"iopub.status.busy":"2023-11-25T16:31:15.341196Z","iopub.execute_input":"2023-11-25T16:31:15.341728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size,200))\nfor word,i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if(embedding_vector is not None):\n        embedding_matrix[i] = embedding_vector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(vocab_size,seq_length):\n  model = Sequential()\n  model.add(Embedding(vocab_size,200,weights=[embedding_matrix],trainable=False,input_length=seq_length))\n  \n  model.add(LSTM(100,return_sequences=True))\n  model.add(LSTM(100))\n  \n  model.add(Dense(100, activation='relu'))\n  model.add(Dense(vocab_size, activation='softmax'))\n  # compile network\n  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n  # summarize defined model\n  model.summary()\n  return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = tf.keras.Sequential([\n# tf.keras.layers.Embedding(input_dim = total_unique_words, output_dim=100, weights=[embeddings_matrix], input_length=max_seq_length-1, trainable=False),\n# tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)),\n# tf.keras.layers.Dropout(0.2), \n# tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256)),\n# tf.keras.layers.Dropout(0.2),\n# tf.keras.layers.Dense(128, activation='relu'),\n# tf.keras.layers.Dense(total_unique_words , activation='softmax')])\n# model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# history = model.fit(x_values, y_values, epochs=120, validation_split=0.2, verbose=1, batch_size=256)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_model(vocab_size,maxlen-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X,y,batch_size=8,epochs=25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Installing offline dependencies\n!pip install -U --no-deps /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n# !pip install -U --no-deps /kaggle/input/datasets-214/datasets-2.14.5-py3-none-any.whl\n# !pip install -U --no-deps /kaggle/input/optimum-113/optimum-1.13.2-py3-none-any.whl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SentenceTransformer:\n    def __init__(self, checkpoint, device=\"cuda:1\"):\n        self.device = device\n        self.checkpoint = checkpoint\n        self.model = AutoModel.from_pretrained(checkpoint).to(self.device).half()\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n    def transform(self, batch):\n        tokens = self.tokenizer(batch[\"text\"], truncation=True, padding=True, return_tensors=\"pt\", max_length=MAX_SEQ_LEN)\n        return tokens.to(self.device)  \n\n    def get_dataloader(self, sentences, batch_size=32):\n        sentences = [\"Represent this sentence for searching relevant passages: \" + x for x in sentences]\n        dataset = Dataset.from_dict({\"text\": sentences})\n        dataset.set_transform(self.transform)\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n        return dataloader\n\n    def encode(self, sentences, show_progress_bar=False, batch_size=1):\n        dataloader = self.get_dataloader(sentences, batch_size=batch_size)\n        pbar = tqdm(dataloader) if show_progress_bar else dataloader\n\n        embeddings = []\n        for batch in pbar:\n            with torch.no_grad():\n                e = self.model(**batch).pooler_output\n                e = F.normalize(e, p=2, dim=1)\n                embeddings.append(e.detach().cpu().numpy())\n        embeddings = np.concatenate(embeddings, axis=0)\n        return embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from time import time\nMODEL_PATH = \"/kaggle/input/bge-small-faiss/\"\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AutoModel\nimport gc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For RAG\nimport faiss\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_from_disk, Dataset\nMAX_SEQ_LEN = 512\nimport torch\nimport ctypes\ndef clean_memory():\n    gc.collect()\n    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n    torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_TITLES = 5\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load embedding model\nstart = time()\nprint(f\"Starting prompt embedding, t={time() - start :.1f}s\")\nmodel_faiss = SentenceTransformer(MODEL_PATH, device=\"cuda:1\")\n\n# Get embeddings of prompts\nf = lambda row : \" \".join([row[\"prompt\"], str(row[\"A\"]), str(row[\"B\"]), str(row[\"C\"]), str(row[\"D\"]), str(row[\"E\"]))\ninputs = df_test.apply(f, axis=1).values # better results than prompt only\nprompt_embeddings = model_faiss.encode(inputs, show_progress_bar=False)\n\n# Search closest sentences in the wikipedia index \nprint(f\"Loading faiss index, t={time() - start :.1f}s\")\nfaiss_index = faiss.read_index(MODEL_PATH + '/faiss.index')\n# faiss_index = faiss.index_cpu_to_all_gpus(faiss_index) # causes OOM, and not that long on CPU\n\nprint(f\"Starting text search, t={time() - start :.1f}s\")\nsearch_index = faiss_index.search(np.float32(prompt_embeddings), NUM_TITLES)[1]\n\nprint(f\"Starting context extraction, t={time() - start :.1f}s\")\ndataset = load_from_disk(\"/kaggle/input/all-paraphs-parsed-expanded\")\nfor i in range(len(df_test)):\n    df_test.loc[i, \"context\"] = \"-\" + \"\\n-\".join([dataset[int(j)][\"text\"] for j in search_index[i]])\n\n# Free memory\nfaiss_index.reset()\ndel faiss_index, prompt_embeddings, model_faiss, dataset\nclean_memory()\nprint(f\"Context added, t={time() - start :.1f}s\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\ndef generate_seq(model,tokenizer,seq_length,seed_txt,n_words):\n  in_txt = seed_txt\n  result = list()\n  for _ in range(n_words):\n    encoded = tokenizer.texts_to_sequences([in_txt])[0]\n    encoded = pad_sequences([encoded],maxlen=seq_length,truncating='pre')\n    yhat = model.predict(encoded, verbose = None)\n    yhat=np.argmax(yhat,axis=1)\n    out_word = ''\n    for word,index in tokenizer.word_index.items():\n      if index == yhat:\n        out_word = word\n        break\n    in_txt += ' '+out_word\n    result.append(out_word)\n  return ' '.join(result),in_txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def concat_prompt_cont(x):\n    concat_str =  x['context'] + x['prompt'] \n    return concat_str","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['concat_prompt_exc'] = df_test.apply(lambda x: concat_prompt_cont(x), axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['answer']= df_train['answer']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for text in df_test[:10].iterrows():\n    text = text[1]\n    input_pr = text['concat_prompt_exc']\n    print(text['prompt'])\n    ans_len = min([len(text['A'].split()),  len(text['B'].split()),len(text['C'].split()),len(text['D'].split()),len(text['E'].split())]) \n    generated,in_txt = generate_seq(model, tokenizer, 50, input_pr, ans_len)\n    print('Predicted Answer:')\n    print(generated)\n    print('Correct Answer;')\n    print(text[text['answer']])\n    print('================')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for text in df_test['concat_prompt_exc'][:10]:\n    text = text[-195:]\n    \n    generated,in_txt = generate_seq(model, tokenizer, 196, text, 10)\n    print('~After~')\n    print(in_txt)\n    print('================')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
id,text
"Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed ""missing baryonic mass"" discrepancy in galaxy clusters?","Answer the following question with A, B, C, D, or E. You can use the context provided. Question: Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed ""missing baryonic mass"" discrepancy in galaxy clusters?
 Context: -MOND is an example of a class of theories known as modified gravity, and is an alternative to the hypothesis that the dynamics of galaxies are determined by massive, invisible dark matter halos. Since Milgrom's original proposal, proponents of MOND have claimed to successfully predict a variety of galactic phenomena that they state are difficult to understand as consequences of dark matter.Though MOND explains the anomalously great rotational velocities of galaxies at their perimeters, it does not fully explain the velocity dispersions of individual galaxies within galaxy clusters. MOND reduces the discrepancy between the velocity dispersions and clusters' observed missing baryonic mass from a factor of around 10 to a factor of about 2. However, the residual discrepancy cannot be accounted for by MOND, requiring that other explanations close the gap such as the presence of as-yet undetected missing baryonic matter.The accurate measurement of the speed of gravitational waves compared to the speed of light in 2017 ruled out a certain class of modified gravity theories but concluded that other MOND theories that dispense with the need for dark matter remained viable. Two years later, theories put forth by Constantinos Skordis and Tom Zlosnik were consistent with gravitational waves that always travel at the speed of light. Later still in 2021, Skordis and Zlosnik developed a subclass of their theory called ""RMOND"", for ""relativistic MOND"", which had ""been shown to reproduce in great detail the main observations in cosmology, including the cosmic-microwave-background power spectrum, and the matter structure power spectrum."" 
-Modified Newtonian dynamics (MOND) is a hypothesis that proposes a modification of Newton's law of universal gravitation to account for observed properties of galaxies. It is an alternative to the hypothesis of dark matter in terms of explaining why galaxies do not appear to obey the currently understood laws of physics.
-MOND Modified Newtonian Dynamics (MOND) is a relatively modern proposal to explain the galaxy rotation problem based on a variation of Newton's Second Law of Dynamics at low accelerations. This would produce a large-scale variation of Newton's universal theory of gravity. A modification of Newton's theory would also imply a modification of general relativistic cosmology in as much as Newtonian cosmology is the limit of Friedman cosmology. While almost all astrophysicists today reject MOND in favor of dark matter, a small number of researchers continue to enhance it, recently incorporating Brans–Dicke theories into treatments that attempt to account for cosmological observations.
-There have been a number of attempts to solve the problem of galaxy rotation by modifying gravity without invoking dark matter. One of the most discussed is modified Newtonian dynamics (MOND), originally proposed by Mordehai Milgrom in 1983, which modifies the Newtonian force law at low accelerations to enhance the effective gravitational attraction. MOND has had a considerable amount of success in predicting the rotation curves of low-surface-brightness galaxies, matching the baryonic Tully–Fisher relation, and the velocity dispersions of the small satellite galaxies of the Local Group.Using data from the Spitzer Photometry and Accurate Rotation Curves (SPARC) database, a group has found that the radial acceleration traced by rotation curves could be predicted just from the observed baryon distribution (that is, including stars and gas but not dark matter). The same relation provided a good fit for 2693 samples in 153 rotating galaxies, with diverse shapes, masses, sizes, and gas fractions. Brightness in the near infrared, where the more stable light from red giants dominates, was used to estimate the density contribution due to stars more consistently. The results are consistent with MOND, and place limits on alternative explanations involving dark matter alone. However, cosmological simulations within a Lambda-CDM framework that include baryonic feedback effects reproduce the same relation, without the need to invoke new dynamics (such as MOND). Thus, a contribution due to dark matter itself can be fully predictable from that of the baryons, once the feedback effects due to the dissipative collapse of baryons are taken into account. MOND is not a relativistic theory, although relativistic theories which reduce to MOND have been proposed, such as tensor–vector–scalar gravity (TeVeS), scalar–tensor–vector gravity (STVG), the f(R) theory of Capozziello and De Laurentis, not to mention a version of Superfluid Vacuum theory based on the Logarithmic Schrödinger equation.A model of galaxy based on a general relativity metric was also proposed, showing that the rotation curves for the Milky Way, NGC 3031, NGC 3198 and NGC 7331 are consistent with the mass density distributions of the visible matter, avoiding the need for a massive halo of exotic dark matter.According to a 2020 analysis of the data produced by the Gaia spacecraft, it would seem possible to explain at least the Milky Way's rotation curve without requiring any dark matter if instead of a Newtonian approximation the entire set of equations of general relativity is adopted.In March 2021, Gerson Otto Ludwig published a model based on general relativity that explains galaxy rotation curves with gravitoelectromagnetism.
-Outstanding problems for MOND The most serious problem facing Milgrom's law is that it cannot eliminate the need for dark matter in all astrophysical systems: galaxy clusters show a residual mass discrepancy even when analyzed using MOND. The fact that some form of unseen mass must exist in these systems detracts from the adequacy of MOND as a solution to the missing mass problem, although the amount of extra mass required is a fifth that of a Newtonian analysis, and there is no requirement that the missing mass be non-baryonic. It has been speculated that 2 eV neutrinos could account for the cluster observations in MOND while preserving the hypothesis's successes at the galaxy scale. Indeed, analysis of sharp lensing data for the galaxy cluster Abell 1689 shows that MOND only becomes distinctive at Mpc distance from the center, so that Zwicky's conundrum remains, and 1.8 eV neutrinos are needed in clusters.The 2006 observation of a pair of colliding galaxy clusters known as the ""Bullet Cluster"", poses a significant challenge for all theories proposing a modified gravity solution to the missing mass problem, including MOND. Astronomers measured the distribution of stellar and gas mass in the clusters using visible and X-ray light, respectively, and in addition mapped the inferred dark matter density using gravitational lensing. In MOND, one would expect the ""missing mass"" to be centred on regions of visible mass which experience accelerations lower than a0 (assuming the external field effect is negligible). In ΛCDM, on the other hand, one would expect the dark matter to be significantly offset from the visible mass because the halos of the two colliding clusters would pass through each other (assuming, as is conventional, that dark matter is collisionless), whilst the cluster gas would interact and end up at the centre. An offset is clearly seen in the observations. It has been suggested, however, that MOND-based models may be able to generate such an offset in strongly non-spherically symmetric systems, such as the Bullet Cluster.A significant piece of evidence in favor of standard dark matter is the observed anisotropies in the cosmic microwave background. While ΛCDM is able to explain the observed angular power spectrum, MOND has a much harder time, though recently it has been shown that MOND can fit the observations too. MOND also encounters difficulties explaining structure formation, with density perturbations in MOND perhaps growing so rapidly that too much structure is formed by the present epoch. However, forming galaxies more rapidly than in ΛCDM can be a good thing to some extent.Several other studies have noted observational difficulties with MOND. For example, it has been claimed that MOND offers a poor fit to the velocity dispersion profile of globular clusters and the temperature profile of galaxy clusters, that different values of a0 are required for agreement with different galaxies' rotation curves, and that MOND is naturally unsuited to forming the basis of cosmology. Furthermore, many versions of MOND predict that the speed of light is different from the speed of gravity, but in 2017 the speed of gravitational waves was measured to be equal to the speed of light to high precision. This is well understood in modern relativistic theories of MOND, with the constraint from gravitational waves actually helping by substantially restricting how a covariant theory might be constructed.Besides these observational issues, MOND and its relativistic generalizations are plagued by theoretical difficulties. Several ad hoc and inelegant additions to general relativity are required to create a theory compatible with a non-Newtonian non-relativistic limit, though the predictions in this limit are rather clear. This is the case for the more commonly used modified gravity versions of MOND, but some formulations (most prominently those based on modified inertia) have long suffered from poor compatibility with cherished physical principles such as conservation laws. Researchers working on MOND generally do not interpret it as a modification of inertia, with only very limited work done on this area.
 A. MOND is a theory that reduces the observed missing baryonic mass in galaxy clusters by postulating the existence of a new form of matter called ""fuzzy dark matter.""
 B. MOND is a theory that increases the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 20.
 C. MOND is a theory that explains the missing baryonic mass in galaxy clusters that was previously considered dark matter by demonstrating that the mass is in the form of neutrinos and axions.
 D. MOND is a theory that reduces the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 2.
 E. MOND is a theory that eliminates the observed missing baryonic mass in galaxy clusters by imposing a new mathematical formulation of gravity that does not require the existence of dark matter. "
Which of the following is an accurate definition of dynamic scaling in self-similar systems?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: Which of the following is an accurate definition of dynamic scaling in self-similar systems?
 Context: -In such systems we can define a certain time-dependent stochastic variable  x . We are interested in computing the probability distribution of  x at various instants of time i.e.  f(x,t) . The numerical value of  f and the typical or mean value of  x generally changes over time. The question is: what happens to the corresponding dimensionless variables? If the numerical values of the dimensional quantities change, but corresponding dimensionless quantities remain invariant then we can argue that snapshots of the system at different times are similar. When this happens we say that the system is self-similar.  One way of verifying dynamic scaling is to plot dimensionless variables  f/tθ as a function of  x/tz of the data extracted at various different time. Then if all the plots of  f vs  x obtained at different times collapse onto a single universal curve then it is said that the systems at different time are similar and it obeys dynamic scaling. The idea of data collapse is deeply rooted to the Buckingham Pi theorem. Essentially such systems can be termed as temporal self-similarity since the same system is similar at different times.
-Dynamic scaling (sometimes known as Family-Vicsek scaling) is a litmus test that shows whether an evolving system exhibits self-similarity. In general a function is said to exhibit dynamic scaling if it satisfies: f(x,t)∼tθφ(xtz).
-In computer networks, self-similarity is a feature of network data transfer dynamics. When modeling network data dynamics the traditional time series models, such as an autoregressive moving average model are not appropriate. This is because these models only provide a finite number of parameters in the model and thus interaction in a finite time window, but the network data usually have a long-range dependent temporal structure. A self-similar process is one way of modeling network data dynamics with such a long range correlation. This article defines and describes network data transfer dynamics in the context of a self-similar process. Properties of the process are shown and methods are given for graphing and estimating parameters modeling the self-similarity of network data.
-Many phenomena investigated by physicists are not static but evolve probabilistically with time (i.e. Stochastic process). The universe itself is perhaps one of the best examples. It has been expanding ever since the Big Bang. Similarly, growth of networks like the Internet are also ever growing systems. Another example is polymer degradation where degradation does not occur in a blink of an eye but rather over quite a long time. Spread of biological and computer viruses too does not happen over night.  Many other seemingly disparate systems which are found to exhibit dynamic scaling. For example: kinetics of aggregation described by Smoluchowski coagulation equation,  complex networks described by Barabasi–Albert model, the kinetic and stochastic Cantor set, the growth model within the Kardar–Parisi–Zhang (KPZ) universality class; one find that the width of the surface  W(L,t) exhibits dynamic scaling.
-Time-varying networks are inherently dynamic, and used for modeling spreading processes on networks. Whether using time-varying networks will be worth the added complexity depends on the relative time scales in question. Time-varying networks are most useful in describing systems where the spreading process on a network and the network itself evolve at similar timescales.Let the characteristic timescale for the evolution of the network be  tN , and the characteristic timescale for the evolution of the spreading process be  tP . A process on a network will fall into one of three categories: Static approximation – where  tN≫tP . The network evolves relatively slowly, so the dynamics of the process can be approximated using a static version of the network.
 A. Dynamic scaling refers to the evolution of self-similar systems, where data obtained from snapshots at fixed times exhibits similarity to the respective data taken from snapshots of any earlier or later time. This similarity is tested by a certain time-dependent stochastic variable x.
 B. Dynamic scaling refers to the non-evolution of self-similar systems, where data obtained from snapshots at fixed times is similar to the respective data taken from snapshots of any earlier or later time. This similarity is tested by a certain time-dependent stochastic variable x.
 C. Dynamic scaling refers to the evolution of self-similar systems, where data obtained from snapshots at fixed times is dissimilar to the respective data taken from snapshots of any earlier or later time. This dissimilarity is tested by a certain time-independent stochastic variable y.
 D. Dynamic scaling refers to the non-evolution of self-similar systems, where data obtained from snapshots at fixed times is dissimilar to the respective data taken from snapshots of any earlier or later time. This dissimilarity is tested by a certain time-independent stochastic variable y.
 E. Dynamic scaling refers to the evolution of self-similar systems, where data obtained from snapshots at fixed times is independent of the respective data taken from snapshots of any earlier or later time. This independence is tested by a certain time-dependent stochastic variable z. "
Which of the following statements accurately describes the origin and significance of the triskeles symbol?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: Which of the following statements accurately describes the origin and significance of the triskeles symbol?
 Context: -Classical Antiquity The triskeles proper, composed of three human legs, is younger than the triple spiral, found in decorations on Greek pottery especially as a design shown on hoplite shields, and later also minted on Greek and Anatolian coinage.
An early example is found on the shield of Achilles in an Attic hydria of the late 6th century BCE.
It is found on coinage in Lycia, and on staters of Pamphylia (at Aspendos, 370–333 BCE) and Pisidia. The meaning of the Greek triskeles is not recorded directly.
The Duc de Luynes in his 1835 study noted the co-occurrence of the symbol with the eagle, the cockerel, the head of Medusa, Perseus, three crescent moons, three ears of corn, and three grains of corn.
From this, he reconstructed feminine divine triad which he identified with the ""triple goddess"" Hecate.
The triskeles was adopted as emblem by the rulers of Syracuse. It is possible that this usage is related with the Greek name of the island of Sicily, Trinacria (Τρινακρία 'having three headlands').
The Sicilian triskeles is shown with the head of Medusa at the center.
The ancient symbol has been re-introduced in modern flags of Sicily since 1848. The oldest find of a triskeles in Sicily is a vase dated to 700 BCE, for which researchers assume a Minoan-Mycenaean origin.
Roman period and Late Antiquity Late examples of the triple spiral symbols are found in Iron Age Europe, e.g. carved in rock in Castro Culture settlement in Galicia, Asturias and Northern Portugal.
In Ireland before the 5th century, in Celtic Christianity the symbol took on new meaning, as a symbol of the Trinity (Father, Son, and Holy Spirit).
-A triskelion or triskeles is an ancient motif consisting of a triple spiral exhibiting rotational symmetry or other patterns in triplicate that emanate from a common center.
The spiral design can be based on interlocking Archimedean spirals, or represent three bent human legs. It is found in artifacts of the European Neolithic and Bronze Age with continuation into the Iron Age especially in the context of the La Tène culture and related Celtic traditions.
The actual triskeles symbol of three human legs is found especially in Greek antiquity, beginning in archaic pottery and continued in coinage of the classical period.
In the Hellenistic period, the symbol becomes associated with the island of Sicily, appearing on coins minted under Dionysius I of Syracuse beginning in c. 382 BCE.
It later appears in heraldry, and, other than in the flag of Sicily, came to be used in the flag of the Isle of Man (known as ny tree cassyn 'the three legs').Greek τρισκελής (triskelḗs) means 'three-legged'.
While the Greek adjective τρισκελής 'three-legged (e.g., of a table)' is ancient, use of the term for the symbol is modern, introduced in 1835 by Honoré Théodoric d'Albert de Luynes as French triskèle, and adopted in the spelling triskeles following Otto Olshausen (1886).
The form triskelion (as it were Greek τρισκέλιον) is a diminutive which entered English usage in numismatics in the late 19th century.
The form consisting of three human legs (as opposed to the triple spiral) has also been called a ""triquetra of legs"", also triskelos or triskel.
-The triskeles was included in the design of the Army Gold Medal awarded to British Army majors and above who had taken a key part in the Battle of Maida (1806).
An early flag of Sicily, proposed in 1848, included the Sicilian triskeles or ""Trinacria symbol"".
Later versions of Sicilian flags have retained the emblem, including the one officially adopted in 2000.
The Flag of the Isle of Man (1932) shows a heraldic design of a triskeles of three armoured legs.
-The spiral and triple spiral motif is a Neolithic symbol in Europe (Megalithic Temples of Malta). The Celtic symbol the triple spiral is in fact a pre-Celtic symbol. It is carved into the rock of a stone lozenge near the main entrance of the prehistoric Newgrange monument in County Meath, Ireland. Newgrange was built around 3200 BCE predating the Celts and the triple spirals were carved at least 2,500 years before the Celts reached Ireland but has long since been incorporated into Celtic culture. The triskelion symbol, consisting of three interlocked spirals or three bent human legs, appears in many early cultures, including Mycenaean vessels, on coinage in Lycia, on staters of Pamphylia (at Aspendos, 370–333 BC) and Pisidia, as well as on the heraldic emblem on warriors' shields depicted on Greek pottery.Spirals can be found throughout pre-Columbian art in Latin and Central America. The more than 1,400 petroglyphs (rock engravings) in Las Plazuelas, Guanajuato Mexico, dating 750-1200 AD, predominantly depict spirals, dot figures and scale models. In Colombia monkeys, frog and lizard like figures depicted in petroglyphs or as gold offering figures frequently includes spirals, for example on the palms of hands. In Lower Central America spirals along with circles, wavy lines, crosses and points are universal petroglyphs characters. Spirals can also be found among the Nazca Lines in the coastal desert of Peru, dating from 200 BC to 500 AD. The geoglyphs number in the thousands and depict animals, plants and geometric motifs, including spirals.Spiral shapes, including the swastika, triskele, etc., have often been interpreted as solar symbols.
-In the Bavarian town of Füssen, Germany the flag and coat of arms of the town contains a triskele, as does the flag of the Russian autonomous region of Ust-Orda Buryat Okrug.The spiral is used by some polytheistic reconstructionist or neopagan groups. As a ""Celtic symbol"", it is used primarily by groups with a Celtic cultural orientation and, less frequently, can also be found in use by various eclectic or syncretic traditions such as Neopaganism. The spiral triskele is one of the primary symbols of Celtic Reconstructionist Paganism, used to represent a variety of triplicities in cosmology and theology; it is also a favored symbol due to its association with the god Manannán mac Lir.Other uses of triskelion-like emblems include the logo for the Trisquel Linux distribution and the seal of the United States Department of Transportation.A specific version of the triskele featuring a conglomeration composed of three sevens has been adopted by neo-nazis, having been originally used (although without the sevens) by the 27th SS Volunteer Division Langemarck on their shoulder strap. In South Africa the Afrikaner Weerstandsbeweging (AWB), an Afrikaner nationalist, neo-Nazi organization and political party (founded 1973), uses a triskele composed of three sevens as its symbol in place of a swastika. The Blood & Honour group also utilises it. Usage of the symbol can be a prosecutable offence under German law, depending on the context in which the symbol is used.
 A. The triskeles symbol was reconstructed as a feminine divine triad by the rulers of Syracuse, and later adopted as an emblem. Its usage may also be related to the Greek name of Sicily, Trinacria, which means ""having three headlands."" The head of Medusa at the center of the Sicilian triskeles represents the three headlands.
 B. The triskeles symbol is a representation of three interlinked spirals, which was adopted as an emblem by the rulers of Syracuse. Its usage in modern flags of Sicily has its origins in the ancient Greek name for the island, Trinacria, which means ""Sicily with three corners."" The head of Medusa at the center is a representation of the island's rich cultural heritage.
 C. The triskeles symbol is a representation of a triple goddess, reconstructed by the rulers of Syracuse, who adopted it as an emblem. Its significance lies in the fact that it represents the Greek name for Sicily, Trinacria, which contains the element ""tria,"" meaning three. The head of Medusa at the center of the Sicilian triskeles represents the three headlands.
 D. The triskeles symbol represents three interlocked spiral arms, which became an emblem for the rulers of Syracuse. Its usage in modern flags of Sicily is due to the island's rich cultural heritage, which dates back to ancient times. The head of Medusa at the center represents the lasting influence of Greek mythology on Sicilian culture.
 E. The triskeles symbol is a representation of the Greek goddess Hecate, reconstructed by the rulers of Syracuse. Its adoption as an emblem was due to its cultural significance, as it represented the ancient Greek name for Sicily, Trinacria. The head of Medusa at the center of the Sicilian triskeles represents the island's central location in the Mediterranean. "
What is the significance of regularization in terms of renormalization problems in physics?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the significance of regularization in terms of renormalization problems in physics?
 Context: -Regularization: Classical physics theory breaks down at small scales, e.g., the difference between an electron and a point particle shown above. Addressing this problem requires new kinds of additional physical constraints. For instance, in this case, assuming a finite electron radius (i.e., regularizing the electron mass-energy) suffices to explain the system below a certain size. Similar regularization arguments work in other renormalization problems. For example, a theory may hold under one narrow set of conditions, but due to calculations involving infinities or singularities, it may breakdown under other conditions or scales. In the case of the electron, another way to avoid infinite mass-energy while retaining the point nature of the particle is to postulate tiny additional dimensions over which the particle could 'spread out' rather than restrict its motion solely over 3D space. This is precisely the motivation behind string theory and other multi-dimensional models including multiple time dimensions. Rather than the existence of unknown new physics, assuming the existence of particle interactions with other surrounding particles in the environment, renormalization offers an alternatively strategy to resolve infinities in such classical problems.
-However, the result usually includes terms proportional to expressions like  1/ϵ which are not well-defined in the limit  ϵ→0 . Regularization is the first step towards obtaining a completely finite and meaningful result; in quantum field theory it must be usually followed by a related, but independent technique called renormalization. Renormalization is based on the requirement that some physical quantities — expressed by seemingly divergent expressions such as  1/ϵ — are equal to the observed values. Such a constraint allows one to calculate a finite value for many other quantities that looked divergent.
-Renormalization is distinct from regularization, another technique to control infinities by assuming the existence of new unknown physics at new scales.
-Conceptual problem Perturbative predictions by quantum field theory about quantum scattering of elementary particles, implied by a corresponding Lagrangian density, are computed using the Feynman rules, a regularization method to circumvent ultraviolet divergences so as to obtain finite results for Feynman diagrams containing loops, and a renormalization scheme. Regularization method results in regularized n-point Green's functions (propagators), and a suitable limiting procedure (a renormalization scheme) then leads to perturbative S-matrix elements. These are independent of the particular regularization method used, and enable one to model perturbatively the measurable physical processes (cross sections, probability amplitudes, decay widths and lifetimes of excited states). However, so far no known regularized n-point Green's functions can be regarded as being based on a physically realistic theory of quantum-scattering since the derivation of each disregards some of the basic tenets of conventional physics (e.g., by not being Lorentz-invariant, by introducing either unphysical particles with a negative metric or wrong statistics, or discrete space-time, or lowering the dimensionality of space-time, or some combination thereof). So the available regularization methods are understood as formalistic technical devices, devoid of any direct physical meaning. In addition, there are qualms about renormalization. For a history and comments on this more than half-a-century old open conceptual problem, see e.g.
-Regularization The metrics is often called image energy; people usually add energy that comes from mechanics assumptions as the Laplacian of displacement (a special case of Tikhonov regularization ) or even finite element problems. As one decided not to solve the Gauss-Newton problem for most of cases this solution is far from being CPU efficient. Cachier et al. demonstrated that the problem of minimizing image and mechanical energy can be reformulated in solving the energy image then applying a Gaussian filter at each iteration. We use this strategy in Yadics and we add the median filter as it is massively used in PIV. One notes that the median filter avoids local minima while preserving discontinuities.
 A. Regularizing the mass-energy of an electron with a finite radius can theoretically simplify calculations involving infinities or singularities, thereby providing explanations that would otherwise be impossible to achieve.
 B. Regularizing the mass-energy of an electron with an infinite radius allows for the breakdown of a theory that is valid under one set of conditions. This approach can be applied to other renormalization problems as well.
 C. Regularizing the mass-energy of an electron with a finite radius is a means of demonstrating that a system below a certain size can be explained without the need for further calculations. This approach can be applied to other renormalization problems as well.
 D. Regularizing the mass-energy of an electron with an infinite radius can be used to provide a highly accurate description of a system under specific conditions. This approach can be transferred to other renormalization problems as well.
 E. Regularizing the mass-energy of an electron with an infinite radius is essential for explaining how a system below a certain size operates. This approach can be applied to other renormalization problems as well. "
Which of the following statements accurately describes the relationship between the dimensions of a diffracting object and the angular spacing of features in the diffraction pattern?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: Which of the following statements accurately describes the relationship between the dimensions of a diffracting object and the angular spacing of features in the diffraction pattern?
 Context: -Several qualitative observations can be made of diffraction in general: The angular spacing of the features in the diffraction pattern is inversely proportional to the dimensions of the object causing the diffraction. In other words: the smaller the diffracting object, the wider the resulting diffraction pattern, and vice versa. (More precisely, this is true of the sines of the angles.) The diffraction angles are invariant under scaling; that is, they depend only on the ratio of the wavelength to the size of the diffracting object.
-Several qualitative observations can be made of diffraction in general: The angular spacing of the features in the diffraction pattern is inversely proportional to the dimensions of the object causing the diffraction. In other words: The smaller the diffracting object, the 'wider' the resulting diffraction pattern, and vice versa. (More precisely, this is true of the sines of the angles.) The diffraction angles are invariant under scaling; that is, they depend only on the ratio of the wavelength to the size of the diffracting object.
-A grating whose elements are separated by S diffracts a normally incident beam of light into a set of beams, at angles θn given by: This is known as the grating equation. The finer the grating spacing, the greater the angular separation of the diffracted beams.
If the light is incident at an angle θ0, the grating equation is: The detailed structure of the repeating pattern determines the form of the individual diffracted beams, as well as their relative intensity while the grating spacing always determines the angles of the diffracted beams.
The image on the right shows a laser beam diffracted by a grating into n = 0, and ±1 beams. The angles of the first order beams are about 20°; if we assume the wavelength of the laser beam is 600 nm, we can infer that the grating spacing is about 1.8 μm.
-In practice, all slits are of finite size so produce diffraction on the both transverse directions, along the x (width W defined) and y (height H defined) axes. If the height H of the slit is much greater than its width W, then the spacing of the vertical (along the height or the y axis) diffraction fringes is much less than the spacing of the horizontal (along the width or x axis) fringes. If the vertical fringe spacing is so less by a relatively so large H, then the observation of the vertical fringes is so hard that a person observing the diffracted wave intensity pattern on the plane of observation or the image plane recognizes only the horizontal fringes with their narrow height. This is the reason why a height-long slit or slit array such as a diffraction grating is typically analyzed only in the dimension along the width. If the illuminating beam does not illuminate the whole height of the slit, then the spacing of the vertical fringes is determined by the dimension of the laser beam along the slit height. Close examination of the two-slit pattern below shows that there are very fine vertical diffraction fringes above and below the main spots, as well as the more obvious horizontal fringes.
-The interference fringe maxima occur at angles where λ is the wavelength of the light. The angular spacing of the fringes is given by When the distance between the slits and the viewing plane is z, the spacing of the fringes is equal to zθ and is the same as above: Diffraction by a grating A grating is defined in Born and Wolf as ""any arrangement which imposes on an incident wave a periodic variation of amplitude or phase, or both"".
 A. The angular spacing of features in the diffraction pattern is indirectly proportional to the dimensions of the object causing the diffraction. Therefore, if the diffracting object is smaller, the resulting diffraction pattern will be narrower.
 B. The angular spacing of features in the diffraction pattern is directly proportional to the dimensions of the object causing the diffraction. Therefore, if the diffracting object is smaller, the resulting diffraction pattern will be narrower.
 C. The angular spacing of features in the diffraction pattern is independent of the dimensions of the object causing the diffraction. Therefore, if the diffracting object is smaller, the resulting diffraction pattern will be the same as if it were big.
 D. The angular spacing of features in the diffraction pattern is inversely proportional to the dimensions of the object causing the diffraction. Therefore, if the diffracting object is smaller, the resulting diffraction pattern will be wider.
 E. The angular spacing of features in the diffraction pattern is directly proportional to the square root of the dimensions of the object causing the diffraction. Therefore, if the diffracting object is smaller, the resulting diffraction pattern will be slightly narrower. "
"Which of the following statements accurately depicts the relationship between Gauss's law, electric flux, electric field, and symmetry in electric fields?","Answer the following question with A, B, C, D, or E. You can use the context provided. Question: Which of the following statements accurately depicts the relationship between Gauss's law, electric flux, electric field, and symmetry in electric fields?
 Context: -While the electric flux is not affected by charges that are not within the closed surface, the net electric field, E can be affected by charges that lie outside the closed surface. While Gauss's law holds for all situations, it is most useful for ""by hand"" calculations when high degrees of symmetry exist in the electric field. Examples include spherical and cylindrical symmetry.
-For a closed Gaussian surface, electric flux is given by: where E is the electric field, S is any closed surface, Q is the total electric charge inside the surface S, ε0 is the electric constant (a universal constant, also called the ""permittivity of free space"") (ε0 ≈ 8.854187817×10−12 F/m)This relation is known as Gauss' law for electric fields in its integral form and it is one of Maxwell's equations.
-In terms of fields of force Gauss's theorem can be interpreted in terms of the lines of force of the field as follows: The flux through a closed surface is dependent upon both the magnitude and direction of the electric field lines penetrating the surface. In general a positive flux is defined by these lines leaving the surface and negative flux by lines entering this surface. This results in positive charges causing a positive flux and negative charges creating a negative flux. These electric field lines will extend to infinity decreasing in strength by a factor of one over the distance from the source of the charge squared. The larger the number of field lines emanating from a charge the larger the magnitude of the charge is, and the closer together the field lines are the greater the magnitude of the electric field. This has the natural result of the electric field becoming weaker as one moves away from a charged particle, but the surface area also increases so that the net electric field exiting this particle will stay the same. In other words the closed integral of the electric field and the dot product of the derivative of the area will equal the net charge enclosed divided by permittivity of free space.
-Integral form Gauss's law may be expressed as: where ΦE is the electric flux through a closed surface S enclosing any volume V, Q is the total charge enclosed within V, and ε0 is the electric constant. The electric flux ΦE is defined as a surface integral of the electric field: ΦE= S E⋅dA where E is the electric field, dA is a vector representing an infinitesimal element of area of the surface, and · represents the dot product of two vectors.
-By way of contrast, Gauss's law for electric fields, another of Maxwell's equations, is ΦE= S E⋅dS=Qε0 where E is the electric field, S is any closed surface, Q is the total electric charge inside the surface S, ε0 is the electric constant (a universal constant, also called the ""permittivity of free space"").The flux of E through a closed surface is not always zero; this indicates the presence of ""electric monopoles"", that is, free positive or negative charges.
 A. Gauss's law holds only for situations involving symmetric electric fields, like those with spherical or cylindrical symmetry, and doesn't apply to other field types. Electric flux, as an expression of the total electric field passing through a closed surface, is influenced only by charges within the surface and unaffected by distant charges located outside it. The scalar quantity electric flux is strictly measured in SI fundamental quantities as kg·m3·s−3·A.
 B. Gauss's law holds in all cases, but it is most useful for calculations involving symmetric electric fields, like those with spherical or cylindrical symmetry, as they allow for simpler algebraic manipulations. Electric flux is not affected by distant charges outside the closed surface, whereas the net electric field, E, can be influenced by any charges positioned outside of the closed surface. In SI base units, the electric flux is expressed as kg·m3·s−3·A−1.
 C. Gauss's law, which applies equally to all electric fields, is typically most useful when dealing with symmetric field configurations, like those with spherical or cylindrical symmetry, since it makes it easier to calculate the total electric flux. Electric flux, an expression of the total electric field through a closed surface, is unaffected by charges outside the surface, while net electric field, E, may be influenced by charges located outside the closed surface. Electric flux is expressed in SI base units as kg·m3·s−1·C.
 D. Gauss's law only holds for electric fields with cylindrical symmetry, like those of a straight long wire; it is not applicable to fields with other types of symmetry. Electric flux, which measures the total electric field across a closed surface, is influenced by all charges within the surface as well as by those located outside it. The unit of electric flux in SI base units is kg·m2·s−2·A−1.
 E. Gauss's law, which holds for all situations, is most beneficial when applied to electric fields that exhibit higher degrees of symmetry, like those with cylindrical and spherical symmetry. While electric flux is unaffected by charges outside of a given closed surface, the net electric field, E, may be affected by them. The unit of electric flux in SI base units is kg·m2·s−1·C. "
Which of the following statements accurately describes the dimension of an object in a CW complex?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: Which of the following statements accurately describes the dimension of an object in a CW complex?
 Context: -An inductive dimension may be defined inductively as follows. Consider a discrete set of points (such as a finite collection of points) to be 0-dimensional. By dragging a 0-dimensional object in some direction, one obtains a 1-dimensional object. By dragging a 1-dimensional object in a new direction, one obtains a 2-dimensional object. In general one obtains an (n + 1)-dimensional object by dragging an n-dimensional object in a new direction. The inductive dimension of a topological space may refer to the small inductive dimension or the large inductive dimension, and is based on the analogy that, in the case of metric spaces, (n + 1)-dimensional balls have n-dimensional boundaries, permitting an inductive definition based on the dimension of the boundaries of open sets. Moreover, the boundary of a discrete set of points is the empty set, and therefore the empty set can be taken to have dimension -1.Similarly, for the class of CW complexes, the dimension of an object is the largest n for which the n-skeleton is nontrivial. Intuitively, this can be described as follows: if the original space can be continuously deformed into a collection of higher-dimensional triangles joined at their faces with a complicated surface, then the dimension of the object is the dimension of those triangles.
-A 1-dimensional CW complex is constructed by taking the disjoint union of a 0-dimensional CW complex with one or more copies of the unit interval. For each copy, there is a map that ""glues"" its boundary (its two endpoints) to elements of the 0-dimensional complex (the points). The topology of the CW complex is the topology of the quotient space defined by these gluing maps.
-The construction, in words The CW complex construction is a straightforward generalization of the following process: A 0-dimensional CW complex is just a set of zero or more discrete points (with the discrete topology).
-The first analysis of contraction hierarchy performance relies in part on a quantity known as the highway dimension. While the definition of this quantity is technical, intuitively a graph has a small highway dimension if for every  r>0 there is a sparse set of vertices  Sr such that every shortest path of length greater than  r includes a vertex from  Sr . Calculating the exact value of the highway dimension is NP-hard and W[1]-hard, but for grids it is known that the highway dimension  h∈Θ(n) .An alternative analysis was presented in the Customizable Contraction Hierarchy line of work. Query running times can be bounded by  O(td2) . As the tree-depth can be bounded in terms of the tree-width,  log ⁡n)2) is also a valid upper bound. The main source is but the consequences for the worst case running times are better detailed in.
-In the notation of topological space operators, the matrix elements can be expressed also as The dimension of empty sets (∅) are denoted as −1 or F (false). The dimension of non-empty sets (¬∅) are denoted with the maximum number of dimensions of the intersection, specifically 0 for points, 1 for lines, 2 for areas. Then, the domain of the model is {0,1,2,F}.
 A. The dimension of an object in a CW complex is the largest n for which the n-skeleton is nontrivial, where the empty set is considered to have dimension -1 and the boundary of a discrete set of points is the empty set.
 B. The dimension of an object in a CW complex is determined by the number of critical points the object contains. The boundary of a discrete set of points is considered to have dimension 1, while the empty set is given a dimension of 0.
 C. The dimension of an object in a CW complex is the smallest n for which the n-skeleton is nontrivial. The empty set is given a dimension of -1, while the boundary of a discrete set of points is assigned a dimension of 0.
 D. The dimension of an object in a CW complex is calculated by counting the number of cells of all dimensions in the object. The empty set is given a dimension of 0, while the boundary of a discrete set of points is assigned a dimension of -1.
 E. The dimension of an object in a CW complex depends on the number of singularities in the object. The empty set and the boundary of a discrete set of points are both assigned a dimension of 0. "
Which of the following statements accurately describes the blocking temperature of an antiferromagnetic layer in a spin valve?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: Which of the following statements accurately describes the blocking temperature of an antiferromagnetic layer in a spin valve?
 Context: -Magnetic blocking temperature The so-called magnetic blocking temperature, TB, is defined as the temperature below which the relaxation of the magnetization becomes slow compared to the time scale of a particular investigation technique. Historically, the blocking temperature for single-molecule magnets has been defined as the temperature at which the molecule's magnetic relaxation time, τ, is 100 seconds. This definition is the current standard for comparison of single-molecule magnet properties, but otherwise is not technologically significant. There is typically a correlation between increasing an SMM's blocking temperature and energy barrier. The average blocking temperature for SMMs is 4K. Dy-metallocenium salts are the most recent SMM to achieve the highest temperature of magnetic hysteresis, greater than that of liquid nitrogen.
-Antiferromagnets can couple to ferromagnets, for instance, through a mechanism known as exchange bias, in which the ferromagnetic film is either grown upon the antiferromagnet or annealed in an aligning magnetic field, causing the surface atoms of the ferromagnet to align with the surface atoms of the antiferromagnet. This provides the ability to ""pin"" the orientation of a ferromagnetic film, which provides one of the main uses in so-called spin valves, which are the basis of magnetic sensors including modern hard disk drive read heads. The temperature at or above which an antiferromagnetic layer loses its ability to ""pin"" the magnetization direction of an adjacent ferromagnetic layer is called the blocking temperature of that layer and is usually lower than the Néel temperature.
-Equivalently, blocking temperature is the temperature below which a material shows slow relaxation of magnetization.
-Blocking temperature The reason the characteristics of the field are conserved comes from the concept of blocking temperature (also known as closure temperature in geochronology). This temperature is where the system becomes blocked against thermal agitation at lower temperatures. Therefore, some minerals exhibit remnant magnetization. One problem that arises in the determination of remnant (or fossil) magnetization is that if the temperature rises above this point, the magnetic history is destroyed. However, in theory it should be possible to relate the magnetic blocking temperature to the isotopic closure temperature, such that it could be checked whether or not a sample can be used.
-The state of the nanoparticle (superparamagnetic or blocked) depends on the measurement time. A transition between superparamagnetism and blocked state occurs when  τm=τN . In several experiments, the measurement time is kept constant but the temperature is varied, so the transition between superparamagnetism and blocked state is seen as a function of the temperature. The temperature for which  τm=τN is called the blocking temperature: ln ⁡(τmτ0) For typical laboratory measurements, the value of the logarithm in the previous equation is in the order of 20–25.
 A. The blocking temperature of an antiferromagnetic layer in a spin valve is the temperature at which the magnetization of the ferromagnetic layer becomes aligned with the magnetic field. The blocking temperature is typically higher than the Néel temperature.
 B. The blocking temperature of an antiferromagnetic layer in a spin valve is the temperature below which the layer loses its ability to ""pin"" the magnetization direction of an adjacent ferromagnetic layer. The blocking temperature is typically higher than the Néel temperature.
 C. The blocking temperature of an antiferromagnetic layer in a spin valve is the temperature at which the ferromagnetic layer becomes completely demagnetized. The blocking temperature is typically higher than the Néel temperature.
 D. The blocking temperature of an antiferromagnetic layer in a spin valve is the temperature at or above which the layer ceases to prevent the orientation of an adjacent ferromagnetic layer. The blocking temperature is typically lower than the Néel temperature.
 E. The blocking temperature of an antiferromagnetic layer in a spin valve is the temperature at which the ferromagnetic layer loses its ability to ""pin"" the magnetization direction of an adjacent antiferromagnetic layer. The blocking temperature is typically higher than the Néel temperature. "
What is the term used in astrophysics to describe light-matter interactions resulting in energy shifts in the radiation field?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the term used in astrophysics to describe light-matter interactions resulting in energy shifts in the radiation field?
 Context: -The interactions and phenomena summarized in the subjects of radiative transfer and physical optics can result in shifts in the wavelength and frequency of electromagnetic radiation. In such cases, the shifts correspond to a physical energy transfer to matter or other photons rather than being by a transformation between reference frames. Such shifts can be from such physical phenomena as coherence effects or the scattering of electromagnetic radiation whether from charged elementary particles, from particulates, or from fluctuations of the index of refraction in a dielectric medium as occurs in the radio phenomenon of radio whistlers. While such phenomena are sometimes referred to as ""redshifts"" and ""blueshifts"", in astrophysics light-matter interactions that result in energy shifts in the radiation field are generally referred to as ""reddening"" rather than ""redshifting"" which, as a term, is normally reserved for the effects discussed above.In many circumstances scattering causes radiation to redden because entropy results in the predominance of many low-energy photons over few high-energy ones (while conserving total energy). Except possibly under carefully controlled conditions, scattering does not produce the same relative change in wavelength across the whole spectrum; that is, any calculated z is generally a function of wavelength. Furthermore, scattering from random media generally occurs at many angles, and z is a function of the scattering angle. If multiple scattering occurs, or the scattering particles have relative motion, then there is generally distortion of spectral lines as well.In interstellar astronomy, visible spectra can appear redder due to scattering processes in a phenomenon referred to as interstellar reddening—similarly Rayleigh scattering causes the atmospheric reddening of the Sun seen in the sunrise or sunset and causes the rest of the sky to have a blue color. This phenomenon is distinct from redshifting because the spectroscopic lines are not shifted to other wavelengths in reddened objects and there is an additional dimming and distortion associated with the phenomenon due to photons being scattered in and out of the line of sight.
-When a photon is the incident particle, there is an inelastic scattering process called Raman scattering. In this scattering process, the incident photon interacts with matter (gas, liquid, and solid) and the frequency of the photon is shifted towards red or blue. A red shift can be observed when part of the energy of the photon is transferred to the interacting matter, where it adds to its internal energy in a process called Stokes Raman scattering. The blue shift can be observed when internal energy of the matter is transferred to the photon; this process is called anti-Stokes Raman scattering.
-The term calorescence is rarely seen in use today, whereas the term fluorescence is common. One reason is that there isn't a physical explanation for calorescence that's specific to calorescence. Relatedly, the physical explanations for some types of fluorescence behavior are also explanations for calorescence and the word fluorescence has been preferred and expanded in customary usage to include calorescence. Another reason is that there isn't a widely used practical application attached to the word calorescence, whereas there is for fluorescence. A related item of physics terminology today is the so-called ""Anti-Stokes Shift"". A Stokes shift refers to molecular absorptions of radiant energy of higher frequencies followed by emissions of lower frequencies; and an anti-Stokes shift refers to absorptions of lower frequencies followed by emissions of higher frequencies. With this terminology, practical applications are attached to the term ""anti-Stokes photoluminescence"" in materials science including semiconductors (see examples). Equal terminology in use in laser science is ""infrared upconversion"", ""upconversion luminescence"", or simply ""upconversion"" (see examples). This terminology is usually contemplating luminescence, as opposed to incandescence, whereas the word Calorescence belongs to the 19th century when the only known upconversion methods were of the incandescent kind.
-Dark radiation (also dark electromagnetism) is a postulated type of radiation that mediates interactions of dark matter.
By analogy to the way photons mediate electromagnetic interactions between particles in the Standard Model (called baryonic matter in cosmology), dark radiation is proposed to mediate interactions between dark matter particles. Similar to dark matter particles, the hypothetical dark radiation does not interact with Standard Model particles.
-Photons have the largest range of energy and central in a variety of energy conversions. Photons interact with electric and magnetic entities. For example, electric dipole which in turn are excited by optical phonons or fluid particle vibration, or transition dipole moments of electronic transitions. In heat transfer physics, the interaction kinetics of phonon is treated using the perturbation theory (the Fermi golden rule) and the interaction Hamiltonian. The photon-electron interaction is where pe is the dipole moment vector and a† and a are the creation and annihilation of internal motion of electron. Photons also participate in ternary interactions, e.g., phonon-assisted photon absorption/emission (transition of electron energy level). The vibrational mode in fluid particles can decay or become excited by emitting or absorbing photons. Examples are solid and molecular gas laser cooling.Using ab initio calculations based on the first principles along with EM theory, various radiative properties such as dielectric function (electrical permittivity, εe,ω), spectral absorption coefficient (σph,ω), and the complex refraction index (mω), are calculated for various interactions between photons and electric/magnetic entities in matter. For example, the imaginary part (εe,c,ω) of complex dielectric function (εe,ω = εe,r,ω + i εe,c,ω) for electronic transition across a bandgap is  where V is the unit-cell volume, VB and CB denote the valence and conduction bands, wκ is the weight associated with a κ-point, and pij is the transition momentum matrix element.
 A. Blueshifting
 B. Redshifting
 C. Reddening
 D. Whitening
 E. Yellowing "
What is the role of axioms in a formal theory?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the role of axioms in a formal theory?
 Context: -Thus, an axiom is an elementary basis for a formal logic system that together with the rules of inference define a deductive system.
Examples This section gives examples of mathematical theories that are developed entirely from a set of non-logical axioms (axioms, henceforth). A rigorous treatment of any of these topics begins with a specification of these axioms.
-In mathematical logic, the concepts of theorems and proofs have been formalized in order to allow mathematical reasoning about them. In this context, statements become well-formed formulas of some formal language. A theory consists of some basis statements called axioms, and some deducing rules (sometimes included in the axioms). The theorems of the theory are the statements that can be derived from the axioms by using the deducing rules. This formalization led to proof theory, which allows proving general theorems about theorems and proofs. In particular, Gödel's incompleteness theorems show that every consistent theory containing the natural numbers has true statements on natural numbers that are not theorems of the theory (that is they cannot be proved inside the theory).
-An axiom is called independent if it can not be proved or disproved from the other axioms of the axiomatic system. An axiomatic system is said to be independent if each of its axioms is independent. If a true statement is a logical consequence of an axiomatic system, then it will be a true statement in every model of that system. To prove that an axiom is independent of the remaining axioms of the system, it is sufficient to find two models of the remaining axioms, for which the axiom is a true statement in one and a false statement in the other. Independence is not always a desirable property from a pedagogical viewpoint.
-Axioms are statements about these primitives; for example, any two points are together incident with just one line (i.e. that for any two points, there is just one line which passes through both of them). Axioms are assumed true, and not proven. They are the building blocks of geometric concepts, since they specify the properties that the primitives have.From a given set of axioms, synthesis proceeds as a carefully constructed logical argument. When a significant result is proved rigorously, it becomes a theorem.
-Axioms (or postulates) are statements about these primitives; for example, any two points are together incident with just one line (i.e. that for any two points, there is just one line which passes through both of them). Axioms are assumed true, and not proven. They are the building blocks of geometric concepts, since they specify the properties that the primitives have.
 A. Basis statements called axioms form the foundation of a formal theory and, together with the deducing rules, help in deriving a set of statements called theorems using proof theory.
 B. Axioms are supplementary statements added to a formal theory that break down otherwise complex statements into more simple ones.
 C. Axioms are redundant statements that can be derived from other statements in a formal theory, providing additional perspective to theorems derived from the theory.
 D. The axioms in a theory are used for experimental validation of the theorems derived from the statements in the theory.
 E. The axioms in a formal theory are added to prove that the statements derived from the theory are true, irrespective of their validity in the real world. "
What did Fresnel predict and verify with regards to total internal reflections?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What did Fresnel predict and verify with regards to total internal reflections?
 Context: -Similarly, Fresnel calculated and verified the angle of incidence that would give a 90° phase difference after three reflections at the same angle, and four reflections at the same angle. In each case there were two solutions, and in each case he reported that the larger angle of incidence gave an accurate circular polarization (for an initial linear polarization at 45° to the plane of reflection). For the case of three reflections he also tested the smaller angle, but found that it gave some coloration due to the proximity of the critical angle and its slight dependence on wavelength. (Compare Fig. 2 above, which shows that the phase difference δ is more sensitive to the refractive index for smaller angles of incidence.) For added confidence, Fresnel predicted and verified that four total internal reflections at 68°27' would give an accurate circular polarization if two of the reflections had water as the external medium while the other two had air, but not if the reflecting surfaces were all wet or all dry.
-For glass with a refractive index of 1.51, Fresnel calculated that a 45° phase difference between the two reflection coefficients (hence a 90° difference after two reflections) required an angle of incidence of 48°37' or 54°37'. He cut a rhomb to the latter angle and found that it performed as expected. Thus the specification of the Fresnel rhomb was completed. Similarly, Fresnel calculated and verified the angle of incidence that would give a 90° phase difference after three reflections at the same angle, and four reflections at the same angle. In each case there were two solutions, and in each case he reported that the larger angle of incidence gave an accurate circular polarization (for an initial linear polarization at 45° to the plane of reflection). For the case of three reflections he also tested the smaller angle, but found that it gave some coloration due to the proximity of the critical angle and its slight dependence on wavelength. (Compare Fig. 13 above, which shows that the phase difference δ is more sensitive to the refractive index for smaller angles of incidence.) For added confidence, Fresnel predicted and verified that four total internal reflections at 68°27' would give an accurate circular polarization if two of the reflections had water as the external medium while the other two had air, but not if the reflecting surfaces were all wet or all dry.Fresnel's deduction of the phase shift in TIR is thought to have been the first occasion on which a physical meaning was attached to the argument of a complex number. Although this reasoning was applied without the benefit of knowing that light waves were electromagnetic, it passed the test of experiment, and survived remarkably intact after James Clerk Maxwell changed the presumed nature of the waves. Meanwhile, Fresnel's success inspired James MacCullagh and Augustin-Louis Cauchy, beginning in 1836, to analyze reflection from metals by using the Fresnel equations with a complex refractive index. The imaginary part of the complex index represents absorption.The term critical angle, used for convenience in the above narrative, is anachronistic: it apparently dates from 1873.In the 20th century, quantum electrodynamics reinterpreted the amplitude of an electromagnetic wave in terms of the probability of finding a photon. In this framework, partial transmission and frustrated TIR concern the probability of a photon crossing a boundary, and attenuated total reflectance concerns the probability of a photon being absorbed on the other side.
-In 1816, Fresnel offered his first attempt at a wave-based theory of chromatic polarization. Without (yet) explicitly invoking transverse waves, his theory treated the light as consisting of two perpendicularly polarized components. In 1817 he noticed that plane-polarized light seemed to be partly depolarized by total internal reflection, if initially polarized at an acute angle to the plane of incidence. By including total internal reflection in a chromatic-polarization experiment, he found that the apparently depolarized light was a mixture of components polarized parallel and perpendicular to the plane of incidence, and that the total reflection introduced a phase difference between them. Choosing an appropriate angle of incidence (not yet exactly specified) gave a phase difference of 1/8 of a cycle. Two such reflections from the ""parallel faces"" of ""two coupled prisms"" gave a phase difference of 1/4 of a cycle. In that case, if the light was initially polarized at 45° to the plane of incidence and reflection, it appeared to be completely depolarized after the two reflections. These findings were reported in a memoir submitted and read to the French Academy of Sciences in November 1817.In 1821, Fresnel derived formulae equivalent to his sine and tangent laws (Eqs. (19) and (20), above)  by modeling light waves as transverse elastic waves with vibrations perpendicular to what had previously been called the plane of polarization. Using old experimental data, he promptly confirmed that the equations correctly predicted the direction of polarization of the reflected beam when the incident beam was polarized at 45° to the plane of incidence, for light incident from air onto glass or water. The experimental confirmation was reported in a ""postscript"" to the work in which Fresnel expounded his mature theory of chromatic polarization, introducing transverse waves. Details of the derivation were given later, in a memoir read to the academy in January 1823. The derivation combined conservation of energy with continuity of the tangential vibration at the interface, but failed to allow for any condition on the normal component of vibration.Meanwhile, in a memoir submitted in December 1822, Fresnel coined the terms linear polarization, circular polarization, and elliptical polarization. For circular polarization, the two perpendicular components were a quarter-cycle (±90°) out of phase.
-Total internal reflection alters only the mutual phase between s- and p-polarized light. Under well chosen angle of incidence, this phase is close to  π/4 .  Fresnel rhomb uses this effect to achieve conversion between circular and linear polarisation. This phase difference is not explicitly dependent on wavelength, but only on refractive index, so Fresnel rhombs made of low-dispersion glasses achieve much broader spectral range than quarter-wave plates. They displace the beam, however.
-Between 1817 and 1823, Augustin-Jean Fresnel discovered that total internal reflection is accompanied by a non-trivial phase shift (that is, a phase shift that is not restricted to 0° or 180°), as the Fresnel reflection coefficient acquires a non-zero imaginary part. We shall now explain this effect for electromagnetic waves in the case of linear, homogeneous, isotropic, non-magnetic media. The phase shift turns out to be an advance, which grows as the incidence angle increases beyond the critical angle, but which depends on the polarization of the incident wave.
 A. Fresnel predicted and verified that three total internal reflections at 75°27' would give a precise circular polarization if two of the reflections had water as the external medium and the third had air, but not if the reflecting surfaces were all wet or all dry.
 B. Fresnel predicted and verified that eight total internal reflections at 68°27' would give an accurate circular polarization if four of the reflections had water as the external medium while the other four had air, but not if the reflecting surfaces were all wet or all dry.
 C. Fresnel predicted and verified that four total internal reflections at 30°27' would result in circular polarization if two of the reflections had water as the external medium while the other two had air, regardless if the reflecting surfaces were all wet or all dry.
 D. Fresnel predicted and verified that two total internal reflections at 68°27' would give an accurate linear polarization if one of the reflections had water as the external medium and the other had air, but not if the reflecting surfaces were all wet or all dry.
 E. Fresnel predicted and verified that four total internal reflections at 68°27' would give a precise circular polarization if two of the reflections had water as the external medium while the other two had air, but not if the reflecting surfaces were all wet or all dry. "
What is the relationship between the Wigner function and the density matrix operator?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the relationship between the Wigner function and the density matrix operator?
 Context: -The density matrix operator may also be realized in phase space. Under the Wigner map, the density matrix transforms into the equivalent Wigner function, W(x,p)=def1πℏ∫−∞∞ψ∗(x+y)ψ(x−y)e2ipy/ℏdy.
The equation for the time evolution of the Wigner function, known as Moyal equation, is then the Wigner-transform of the above von Neumann equation, ∂W(x,p,t)∂t=−{{W(x,p,t),H(x,p)}}, where  H(x,p) is the Hamiltonian, and  {{⋅,⋅}} is the Moyal bracket, the transform of the quantum commutator.
The evolution equation for the Wigner function is then analogous to that of its classical limit, the Liouville equation of classical physics. In the limit of vanishing Planck's constant  ℏ ,  W(x,p,t) reduces to the classical Liouville probability density function in phase space.
-It is symmetric in x and p: W(x,p)=1πℏ∫−∞∞φ∗(p+q)φ(p−q)e−2ixq/ℏdq, where φ is the normalized momentum-space wave function, proportional to the Fourier transform of ψ.
In 3D, W(r→,p→)=1(2π)3∫ψ∗(r→+ℏs→/2)ψ(r→−ℏs→/2)eip→⋅s→d3s.
In the general case, which includes mixed states, it is the Wigner transform of the density matrix: where ⟨x|ψ⟩ = ψ(x). This Wigner transformation (or map) is the inverse of the Weyl transform, which maps phase-space functions to Hilbert-space operators, in Weyl quantization.
Thus, the Wigner function is the cornerstone of quantum mechanics in phase space.
In 1949, José Enrique Moyal elucidated how the Wigner function provides the integration measure (analogous to a probability density function) in phase space, to yield expectation values from phase-space c-number functions g(x, p) uniquely associated to suitably ordered operators Ĝ through Weyl's transform (see Wigner–Weyl transform and property 7 below), in a manner evocative of classical probability theory.
Specifically, an operator's Ĝ expectation value is a ""phase-space average"" of the Wigner transform of that operator: 
-The Wigner transformation is a general invertible transformation of an operator Ĝ on a Hilbert space to a function g(x, p) on phase space and is given by g(x,p)=∫−∞∞dseips/ℏ⟨x−s2|G^|x+s2⟩.
Hermitian operators map to real functions. The inverse of this transformation, from phase space to Hilbert space, is called the Weyl transformation: ⟨x|G^|y⟩=∫−∞∞dpheip(x−y)/ℏg(x+y2,p) (not to be confused with the distinct Weyl transformation in differential geometry).
The Wigner function W(x, p) discussed here is thus seen to be the Wigner transform of the density matrix operator ρ̂. Thus the trace of an operator with the density matrix Wigner-transforms to the equivalent phase-space integral overlap of g(x, p) with the Wigner function.
-The Liouville theorem of classical mechanics fails, to the extent that, locally, the phase space volume is not preserved in time.  In fact, the quantum phase flow does not preserve all differential forms  ω2s defined by exterior powers of  ω2=Ikldξk⋏dξl .  The Wigner function represents a quantum system in a more general form than the wave function. Wave functions describe pure states, while the Wigner function characterizes ensembles of quantum states. Any Hermitian operator can be diagonalized:  f^=∑sλs|s⟩⟨s| .Those operators whose eigenvalues  λs are non-negative and sum to a finite number can be mapped to density matrices, i.e., to some physical states. The Wigner function is an image of the density matrix, so the Wigner functions admit a similar decomposition: W(ξ)=∑sλsWs(ξ), with  λs≥0 and  Ws(ξ)⋆Wr(ξ)=δsrWs(ξ) Quantum Hamilton's equations The Quantum Hamilton's equations can be obtained applying the Wigner transform to the evolution equations for Heisenberg operators of canonical coordinates and momenta, ∂∂τqi(ξ,τ)={ζi,H(ζ)}|ζ=⋆q(ξ,τ).
-Wave functions are not always the most convenient way to describe quantum systems and their behavior. When the preparation of a system is only imperfectly known, or when the system under investigation is a part of a larger whole, density matrices may be used instead.: 74  A density matrix is a positive semi-definite operator whose trace is equal to 1. (The term ""density operator"" is also used, particularly when the underlying Hilbert space is infinite-dimensional.) The set of all density matrices is convex, and the extreme points are the operators that project onto vectors in the Hilbert space. These are the density-matrix representations of wave functions; in Dirac notation, they are written  The density-matrix analogue of the Schrödinger equation for wave functions is where the brackets denote a commutator. This is variously known as the von Neumann equation, the Liouville–von Neumann equation, or just the Schrödinger equation for density matrices.: 312  If the Hamiltonian is time-independent, this equation can be easily solved to yield More generally, if the unitary operator  U^(t) describes wave function evolution over some time interval, then the time evolution of a density matrix over that same interval is given by Unitary evolution of a density matrix conserves its von Neumann entropy.: 267 
 A. The Wigner function W(x, p) is the Wigner transform of the density matrix operator ρ̂, and the trace of an operator with the density matrix Wigner-transforms to the equivalent phase-space integral overlap of g(x,p) with the Wigner function.
 B. The Wigner function W(x, p) is a source function used for the density matrix operator ρ̂ and the product of these two functions creates the phase space wave function g(x, p).
 C. The Wigner function W(x, p) is the derivative of the density matrix operator ρ̂ with respect to the phase space coordinate.
 D. The Wigner function W(x, p) represents the Hamiltonian H(x,p) of the density matrix operator ρ̂, while the Moyal bracket {{⋅, ⋅}} represents the Poisson bracket in the phase space.
 E. The Wigner function W(x, p) is the time derivative of the density matrix operator ρ̂ with respect to the phase space coordinate. "
What is one of the examples of the models proposed by cosmologists and theoretical physicists without the cosmological or Copernican principles that can be used to address specific issues in the Lambda-CDM model and distinguish between current models and other possible models?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is one of the examples of the models proposed by cosmologists and theoretical physicists without the cosmological or Copernican principles that can be used to address specific issues in the Lambda-CDM model and distinguish between current models and other possible models?
 Context: -The standard model of cosmology, the Lambda-CDM model, assumes the Copernican principle and the more general cosmological principle. Some cosmologists and theoretical physicists have created models without the cosmological or Copernican principles to constrain the values of observational results, to address specific known issues in the Lambda-CDM model, and to propose tests to distinguish between current models and other possible models.
-A prominent example in this context is inhomogeneous cosmology, to model the observed accelerating universe and cosmological constant. Instead of using the current accepted idea of dark energy, this model proposes the universe is much more inhomogeneous than currently assumed, and instead, we are in an extremely large low-density void. To match observations we would have to be very close to the centre of this void, immediately contradicting the Copernican principle.
-The Copernican principle has never been proven, and in the most general sense cannot be proven, but it is implicit in many modern theories of physics. Cosmological models are often derived with reference to the cosmological principle, slightly more general than the Copernican principle, and many tests of these models can be considered tests of the Copernican principle.
-In physical cosmology, the Copernican principle states that humans, on the Earth or in the Solar System, are not privileged observers of the universe, that observations from the Earth are representative of observations from the average position in the universe. Named for Copernican heliocentrism, it is a working assumption that arises from a modified cosmological extension of Copernicus' argument of a moving Earth.
-Copernican and cosmological principles The Copernican principle, named after Nicolaus Copernicus, states that the Earth is not in a central, specially favored position. Hermann Bondi named the principle after Copernicus in the mid-20th century, although the principle itself dates back to the 16th-17th century paradigm shift away from the geocentric Ptolemaic system.
The cosmological principle is an extension of the Copernican principle which states that the Universe is homogeneous (the same observational evidence is available to observers at different locations in the Universe) and isotropic (the same observational evidence is available by looking in any direction in the Universe). A homogeneous, isotropic Universe does not have a center.
 A. The Copernican principle, which proposes that Earth, the Solar System, and the Milky Way are not at the centre of the universe, but instead, the universe is expanding equally in all directions. This principle is a modification of the Lambda-CDM model and has been shown to explain several observational results.
 B. Inhomogeneous cosmology, which states that the universe is entirely homogeneous and isotropic, directly proportional to the density of matter and radiation. This model proposes that everything in the universe is completely uniform, but it does not match observations.
 C. Inhomogeneous cosmology, which models the universe as an extremely large, low-density void, instead of using the concept of dark energy. According to the model, this theory can match the observed accelerating universe and cosmological constant, but it contradicts the Copernican principle.
 D. The cosmological principle, which proposes that Earth, the Solar System, and the Milky Way are at the centre of the universe. This principle is a modification of the Lambda-CDM model and has been shown to explain several observational results.
 E. The principle of dark energy, which proposes that a new form of energy, not previously detected, is responsible for the acceleration of the expansion of the universe. This principle is a modification of the Lambda-CDM model and has been shown to explain several observational results. "
What is the Roche limit?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the Roche limit?
 Context: -In celestial mechanics, the Roche limit, also called Roche radius, is the distance from a celestial body within which a second celestial body, held together only by its own force of gravity, will disintegrate because the first body's tidal forces exceed the second body's self-gravitation. Inside the Roche limit, orbiting material disperses and forms rings, whereas outside the limit, material tends to coalesce. The Roche radius depends on the radius of the first body and on the ratio of the bodies' densities.
-But note that, as defined above, the Roche limit refers to a body held together solely by the gravitational forces which cause otherwise unconnected particles to coalesce, thus forming the body in question. The Roche limit is also usually calculated for the case of a circular orbit, although it is straightforward to modify the calculation to apply to the case (for example) of a body passing the primary on a parabolic or hyperbolic trajectory.
-The Roche limit typically applies to a satellite's disintegrating due to tidal forces induced by its primary, the body around which it orbits. Parts of the satellite that are closer to the primary are attracted more strongly by gravity from the primary than parts that are farther away; this disparity effectively pulls the near and far parts of the satellite apart from each other, and if the disparity (combined with any centrifugal effects due to the object's spin) is larger than the force of gravity holding the satellite together, it can pull the satellite apart. Some real satellites, both natural and artificial, can orbit within their Roche limits because they are held together by forces other than gravitation. Objects resting on the surface of such a satellite would be lifted away by tidal forces. A weaker satellite, such as a comet, could be broken up when it passes within its Roche limit.
-When a body (body 1) is acted on by the gravity of another body (body 2), the field can vary significantly on body 1 between the side of the body facing body 2 and the side facing away from body 2. Figure 4 shows the differential force of gravity on a spherical body (body 1) exerted by another body (body 2). These so-called tidal forces cause strains on both bodies and may distort them or even, in extreme cases, break one or the other apart. The Roche limit is the distance from a planet at which tidal effects would cause an object to disintegrate because the differential force of gravity from the planet overcomes the attraction of the parts of the object for one another. These strains would not occur if the gravitational field were uniform, because a uniform field only causes the entire body to accelerate together in the same direction and at the same rate.
-The Roche limit for a rigid spherical satellite is the distance,  d , from the primary at which the gravitational force on a test mass at the surface of the object is exactly equal to the tidal force pulling the mass away from the object: d=RM(2ρMρm)13 where  RM is the radius of the primary,  ρM is the density of the primary, and  ρm is the density of the satellite. This can be equivalently written as d=Rm(2MMMm)13 where  Rm is the radius of the secondary,  MM is the mass of the primary, and  Mm is the mass of the secondary.
 A. The Roche limit is the distance at which tidal effects would cause an object to rotate since the forces exerted by two massive bodies produce a torque on a third object.
 B. The Roche limit is the distance at which tidal effects would cause an object to unite since differential force from a planet results in parts becoming attracted to one another.
 C. The Roche limit is the distance at which tidal effects would cause a planet to disintegrate since differential force from an object overcomes the planet's core.
 D. The Roche limit is the distance at which tidal effects would cause an object to disintegrate since differential force from a planet overcomes the attraction of the parts between them.
 E. The Roche limit is the distance at which tidal effects would cause an object to break apart due to differential force from the planet overcoming the attraction of the parts of the object for one another, which depends on the object's density and composition, as well as the mass and size of the planet. "
What is Martin Heidegger's view on the relationship between time and human existence?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is Martin Heidegger's view on the relationship between time and human existence?
 Context: -Henri Bergson believed that time was neither a real homogeneous medium nor a mental construct, but possesses what he referred to as Duration. Duration, in Bergson's view, was creativity and memory as an essential component of reality.According to Martin Heidegger we do not exist inside time, we are time. Hence, the relationship to the past is a present awareness of having been, which allows the past to exist in the present. The relationship to the future is the state of anticipating a potential possibility, task, or engagement. It is related to the human propensity for caring and being concerned, which causes ""being ahead of oneself"" when thinking of a pending occurrence. Therefore, this concern for a potential occurrence also allows the future to exist in the present. The present becomes an experience, which is qualitative instead of quantitative. Heidegger seems to think this is the way that a linear relationship with time, or temporal existence, is broken or transcended.
-In Being and Time (1927; transl. 1962), Martin Heidegger argues that the concept of time prevalent in all Western thought has largely remained unchanged since the definition offered by Aristotle in the Physics. Heidegger says, ""Aristotle's essay on time is the first detailed Interpretation of this phenomenon [time] which has come down to us. Every subsequent account of time, including Henri Bergson's, has been essentially determined by it."" Aristotle defined time as ""the number of movement in respect of before and after"". By defining time in this way Aristotle privileges what is present-at-hand, namely the ""presence"" of time. Heidegger argues in response that ""entities are grasped in their Being as 'presence'; this means that they are understood with regard to a definite mode of time – the 'Present'"". Central to Heidegger's own philosophical project is the attempt to gain a more authentic understanding of time. Heidegger considers time to be the unity of three ecstases: the past, the present, and the future.
-The presence to which Heidegger refers is both a presence as in a ""now"" and also a presence as in an eternal present, as one might associate with God or the ""eternal"" laws of science. This hypostatized (underlying) belief in presence is undermined by novel phenomenological ideas, such that presence itself does not subsist, but comes about primordially through the action of our futural projection, our realization of finitude and the reception or rejection of the traditions of our time.In his short work Intuition of the Instant, Gaston Bachelard attempts to navigate beyond, or parallel to, the Western concept of 'time as duration' – as the imagined trajectorial space of movement. He distinguishes between two foundations of time: time viewed as a duration, and time viewed as an instant. Bachelard then follows this second phenomenon of time and concludes that time as a duration does not exist, but is created as a necessary mediation for increasingly complex beings to persist. The reality of time for existence, though, is in fact a reprisal of the instant, the gestation of all existence every instant, the eternal death that gives life.
-Heidegger's being-for-death The German philosopher Martin Heidegger wrote about death as something conclusively determined, in the sense that it is inevitable for every human being, while on the other hand, it unmasks its indeterminate nature via the truth that one never knows when or how death is going to come. Heidegger does not engage in speculation about whether being after death is possible. He argues that all human existence is embedded in time: past, present, future, and when considering the future, we encounter the notion of death. This then creates angst. Angst can create a clear understanding in one that death is a possible mode of existence, which Heidegger described as ""clearing"". Thus, angst can lead to a freedom about existence, but only if people can stop denying their mortality (as expressed in Heidegger's terminology as ""stop denying being-for-death"").
-Heidegger Martin Heidegger, meanwhile, argued that ""the surrounding world is different for each of us, and notwithstanding that we move about in a common world"". The world, for Heidegger, was that into which we are always already ""thrown"" and with which we, as beings-in-the-world, must come to terms. His conception of ""world disclosure"" was most notably elaborated in his 1927 work Being and Time.
 A. Martin Heidegger believes that humans exist within a time continuum that is infinite and does not have a defined beginning or end. The relationship to the past involves acknowledging it as a historical era, and the relationship to the future involves creating a world that will endure beyond one's own time.
 B. Martin Heidegger believes that humans do not exist inside time, but that they are time. The relationship to the past is a present awareness of having been, and the relationship to the future involves anticipating a potential possibility, task, or engagement.
 C. Martin Heidegger does not believe in the existence of time or that it has any effect on human consciousness. The relationship to the past and the future is insignificant, and human existence is solely based on the present.
 D. Martin Heidegger believes that the relationship between time and human existence is cyclical. The past and present are interconnected and the future is predetermined. Human beings do not have free will.
 E. Martin Heidegger believes that time is an illusion, and the past, present, and future are all happening simultaneously. Humans exist outside of this illusion and are guided by a higher power. "
"What is the ""ultraviolet catastrophe""?","Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the ""ultraviolet catastrophe""?
 Context: -This formula is obtained from the equipartition theorem of classical statistical mechanics which states that all harmonic oscillator modes (degrees of freedom) of a system at equilibrium have an average energy of  kBT The ""ultraviolet catastrophe"" is the expression of the fact that the formula misbehaves at higher frequencies, i.e.  Bν(T)→∞ as  ν→∞ An example, from Mason's A History of the Sciences, illustrates multi-mode vibration via a piece of string. As a natural vibrator, the string will oscillate with specific modes (the standing waves of a string in harmonic resonance), dependent on the length of the string. In classical physics, a radiator of energy will act as a natural vibrator. Additionally, since each mode will have the same energy, most of the energy in a natural vibrator will be in the smaller wavelengths and higher frequencies, where most of the modes are.
-In the longer wavelengths this deviation is not so noticeable, as  hν and  nhν are very small. In the shorter wavelengths of the ultraviolet range, however, classical theory predicts the energy emitted tends to infinity, hence the ultraviolet catastrophe. The theory even predicted that all bodies would emit most of their energy in the ultraviolet range, clearly contradicted by the experimental data which showed a different peak wavelength at different temperatures (see also Wien's law). Instead, in the quantum treatment of this problem, the numbers of the energy modes are quantized, attenuating the spectrum at high frequency in agreement with experimental observation and resolving the catastrophe. The modes that had more energy than the thermal energy of the substance itself were not considered, and because of quantization modes having infinitesimally little energy were excluded.
-The ultraviolet catastrophe, also called the Rayleigh–Jeans catastrophe, was the prediction of late 19th century/early 20th century classical physics that an ideal black body at thermal equilibrium would emit an unbounded quantity of energy as wavelength decreased into the ultraviolet range.: 6–7 The term ""ultraviolet catastrophe"" was first used in 1911 by Paul Ehrenfest, but the concept originated with the 1900 statistical derivation of the Rayleigh–Jeans law.  The phrase refers to the fact that the empirically derived Rayleigh–Jeans law, which accurately predicted experimental results at large wavelengths, failed to do so for short wavelengths. (See the image for further elaboration.) As the theory diverged from empirical observations when these frequencies reached the ultraviolet region of the electromagnetic spectrum, there was a problem. This problem was later found to be due to a property of quanta as proposed by Max Planck: There could be no fraction of a discrete energy package already carrying minimal energy.
-The optical breakdown is a very ""violent"" phenomenon and changes drastically the structure of the surrounding medium. To the naked eye, optical breakdown looks like a spark and if the event happens in air or some other fluid, it is even possible to hear a short noise (burst) caused by the explosive plasma expansion.
-Ultraviolet (UV) is a form of electromagnetic radiation with wavelength shorter than that of visible light, but longer than X-rays. UV radiation is present in sunlight, and constitutes about 10% of the total electromagnetic radiation output from the Sun. It is also produced by electric arcs; Cherenkov radiation; and specialized lights; such as mercury-vapor lamps, tanning lamps, and black lights. Although long-wavelength ultraviolet is not considered an ionizing radiation because its photons lack the energy to ionize atoms, it can cause chemical reactions and causes many substances to glow or fluoresce. Many practical applications, including chemical and biological effects, derive from the way that UV radiation can interact with organic molecules. These interactions can involve absorption or adjusting energy states in molecules, but do not necessarily involve heating.Short-wave ultraviolet light damages DNA and sterilizes surfaces with which it comes into contact. For humans, suntan and sunburn are familiar effects of exposure of the skin to UV light, along with an increased risk of skin cancer. The amount of UV light produced by the Sun means that the Earth would not be able to sustain life on dry land if most of that light were not filtered out by the atmosphere. More energetic, shorter-wavelength ""extreme"" UV below 121 nm ionizes air so strongly that it is absorbed before it reaches the ground. However, ultraviolet light (specifically, UVB) is also responsible for the formation of vitamin D in most land vertebrates, including humans. The UV spectrum, thus, has effects both beneficial and harmful to life.
 A. It is a phenomenon that occurs only in multi-mode vibration.
 B. It is the misbehavior of a formula for higher frequencies.
 C. It is the standing wave of a string in harmonic resonance.
 D. It is a flaw in classical physics that results in the misallocation of energy.
 E. It is a disproven theory about the distribution of electromagnetic radiation. "
What is the most popular explanation for the shower-curtain effect?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the most popular explanation for the shower-curtain effect?
 Context: -Bernoulli effect hypothesis The most popular explanation given for the shower-curtain effect is Bernoulli's principle. Bernoulli's principle states that an increase in velocity results in a decrease in pressure. This theory presumes that the water flowing out of a shower head causes the air through which the water moves to start flowing in the same direction as the water. This movement would be parallel to the plane of the shower curtain. If air is moving across the inside surface of the shower curtain, Bernoulli's principle says the air pressure there will drop. This would result in a pressure differential between the inside and outside, causing the curtain to move inward. It would be strongest when the gap between the bather and the curtain is smallest, resulting in the curtain attaching to the bather.
-Often Bernoulli's principle is used to explain the topspin effect, as the difference in speed between ball surface and air is greater on the top of the ball. For example, if the air flowing past the bottom of the ball is moving faster than the air flowing past the top then Bernoulli's principle implies that the pressure on the surfaces of the ball will be lower below than above. In other words, since there is more air friction occurring on the top surface of the ball compared to the bottom, this differential causes a greater pressure to be applied on the top of the ball, resulting in the ball being pushed down.
-A correct explanation of why the paper rises would observe that the plume follows the curve of the paper and that a curved streamline will develop a pressure gradient perpendicular to the direction of flow, with the lower pressure on the inside of the curve. Bernoulli's principle predicts that the decrease in pressure is associated with an increase in speed; in other words, as the air passes over the paper, it speeds up and moves faster than it was moving when it left the demonstrator's mouth. But this is not apparent from the demonstration.Other common classroom demonstrations, such as blowing between two suspended spheres, inflating a large bag, or suspending a ball in an airstream are sometimes explained in a similarly misleading manner by saying ""faster moving air has lower pressure"".
-Suction is the result of air pressure differential between areas.  Removing air from a space results in a pressure differential. Suction pressure is therefore limited by external air pressure. Even a perfect vacuum cannot suck with more pressure than is available in the surrounding environment. Suctions can form on the sea, for example, when a ship founders.
-Coandă effect The Coandă effect, also known as ""boundary layer attachment"", is the tendency of a moving fluid to adhere to an adjacent wall.
Condensation A hot shower will produce steam that condenses on the shower side of the curtain, lowering the pressure there. In a steady state the steam will be replaced by new steam delivered by the shower but in reality the water temperature will fluctuate and lead to times when the net steam production is negative.
Air pressure Colder dense air outside and hot less dense air inside causes higher air pressure on the outside to force the shower curtain inwards to equalise the air pressure, this can be observed simply when the bathroom door is open allowing cold air into the bathroom.
 A. The pressure differential between the inside and outside of the shower
 B. The decrease in velocity resulting in an increase in pressure
 C. The movement of air across the outside surface of the shower curtain
 D. The use of cold water
 E. Bernoulli's principle "
What is the butterfly effect?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the butterfly effect?
 Context: -A related way to interpret the butterfly effect is to see it as highlighting the difference between the application of the notion of causality in physics and a more general use of causality as represented by Mackie's INUS conditions. In classical (Newtonian) physics, in general, only those conditions are (explicitly) taken into account, that are both necessary and sufficient. For instance, when a massive sphere is caused to roll down a slope starting from a point of unstable equilibrium, then its velocity is assumed to be caused by the force of gravity accelerating it; the small push that was needed to set it into motion is not explicitly dealt with as a cause. In order to be a physical cause there must be a certain proportionality with the ensuing effect. A distinction is drawn between triggering and causation of the ball's motion. By the same token the butterfly can be seen as triggering a tornado, its cause being assumed to be seated in the atmospherical energies already present beforehand, rather than in the movements of a butterfly.
-Butterfly effect The butterfly effect is the notion that small events can have large, widespread consequences. The term describes events observed in chaos theory where a very small change in initial conditions results in vastly different outcomes. The term was coined by mathematician Edward Lorenz years after the phenomenon was first described.The butterfly effect has found its way into popular imagination. For example, in Ray Bradbury's 1952 short story A Sound of Thunder, the killing of a single insect millions of years in the past drastically changes the world, and in the 2004 film The Butterfly Effect, the protagonist's small changes to his past results in extreme changes.
-In chaos theory, the butterfly effect is the sensitive dependence on initial conditions in which a small change in one state of a deterministic nonlinear system can result in large differences in a later state.
-Theories in physics like the butterfly effect from chaos theory open up the possibility of a type of distributed parameter systems in causality. The butterfly effect theory proposes: ""Small variations of the initial condition of a nonlinear dynamical system may produce large variations in the long term behavior of the system."" This opens up the opportunity to understand a distributed causality.
-The butterfly effect describes a phenomenon in chaos theory whereby a minor change in circumstances can cause a large change in outcome. The scientific concept is attributed to Edward Lorenz, a mathematician and meteorologist who used the metaphor to describe his research findings related to chaos theory and weather prediction, initially in a 1972 paper titled ""Predictability: Does the Flap of a Butterfly's Wings in Brazil Set Off a Tornado in Texas?"" The butterfly metaphor is attributed to the 1952 Ray Bradbury short story ""A Sound of Thunder"".The concept has been widely adopted by popular culture, and interpreted to mean that small events have a rippling effect that cause much larger events to occur, and has become a common reference.
 A. The butterfly effect is a physical cause that occurs when a massive sphere is caused to roll down a slope starting from a point of unstable equilibrium, and its velocity is assumed to be caused by the force of gravity accelerating it.
 B. The butterfly effect is a distributed causality that opens up the opportunity to understand the relationship between necessary and sufficient conditions in classical (Newtonian) physics.
 C. The butterfly effect is a proportionality between the cause and the effect of a physical phenomenon in classical (Newtonian) physics.
 D. The butterfly effect is a small push that is needed to set a massive sphere into motion when it is caused to roll down a slope starting from a point of unstable equilibrium.
 E. The butterfly effect is a phenomenon that highlights the difference between the application of the notion of causality in physics and a more general use of causality as represented by Mackie's INUS conditions. "
What is the 'reactive Leidenfrost effect' observed in non-volatile materials?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the 'reactive Leidenfrost effect' observed in non-volatile materials?
 Context: -Non-volatile materials were discovered in 2015 to also exhibit a 'reactive Leidenfrost effect', whereby solid particles were observed to float above hot surfaces and skitter around erratically. Detailed characterization of the reactive Leidenfrost effect was completed for small particles of cellulose (~0.5 mm) on high temperature polished surfaces by high speed photography. Cellulose was shown to decompose to short-chain oligomers which melt and wet smooth surfaces with increasing heat transfer associated with increasing surface temperature. Above 675 °C (1,247 °F), cellulose was observed to exhibit transition boiling with violent bubbling and associated reduction in heat transfer. Liftoff of the cellulose droplet (depicted at the right) was observed to occur above about 750 °C (1,380 °F), associated with a dramatic reduction in heat transfer.High speed photography of the reactive Leidenfrost effect of cellulose on porous surfaces (macroporous alumina) was also shown to suppress the reactive Leidenfrost effect and enhance overall heat transfer rates to the particle from the surface. The new phenomenon of a 'reactive Leidenfrost (RL) effect' was characterized by a dimensionless quantity, (φRL= τconv/τrxn), which relates the time constant of solid particle heat transfer to the time constant of particle reaction, with the reactive Leidenfrost effect occurring for 10−1< φRL< 10+1. The reactive Leidenfrost effect with cellulose will occur in numerous high temperature applications with carbohydrate polymers, including biomass conversion to biofuels, preparation and cooking of food, and tobacco use.The Leidenfrost effect has also been used as a means to promote chemical change of various organic liquids through their conversion by thermal decomposition into various products. Examples include decomposition of ethanol, diethyl carbonate, and glycerol.
-The Leidenfrost effect is a physical phenomenon in which a liquid, close to a surface that is significantly hotter than the liquid's boiling point, produces an insulating vapor layer that keeps the liquid from boiling rapidly. Because of this repulsive force, a droplet hovers over the surface, rather than making physical contact with it. The effect is named after the German doctor Johann Gottlob Leidenfrost, who described it in A Tract About Some Qualities of Common Water.
-The Leidenfrost point may also be taken to be the temperature for which the hovering droplet lasts longest.It has been demonstrated that it is possible to stabilize the Leidenfrost vapor layer of water by exploiting superhydrophobic surfaces. In this case, once the vapor layer is established, cooling never collapses the layer, and no nucleate boiling occurs; the layer instead slowly relaxes until the surface is cooled.Droplets of different liquids with different boiling temperatures will also exhibit a Leidenfrost effect with respect to each other and repel each other.The Leidenfrost effect has been used for the development of high sensitivity ambient mass spectrometry. Under the influence of the Leidenfrost condition, the levitating droplet does not release molecules, and the molecules are enriched inside the droplet. At the last moment of droplet evaporation, all the enriched molecules release in a short time period and thereby increase the sensitivity.A heat engine based on the Leidenfrost effect has been prototyped; it has the advantage of extremely low friction.The effect also applies when the surface is at room temperature but the liquid is cryogenic, allowing liquid nitrogen droplets to harmlessly roll off exposed skin. Conversely, the inverse Leidenfrost effect lets drops of relatively warm liquid levitate on a bath of liquid nitrogen.
-Henry developed a model for Leidenfrost phenomenon which includes transient wetting and microlayer evaporation. Since the Leidenfrost phenomenon is a special case of film boiling, the Leidenfrost temperature is related to the minimum film boiling temperature via a relation which factors in the properties of the solid being used. While the Leidenfrost temperature is not directly related to the surface tension of the fluid, it is indirectly dependent on it through the film boiling temperature. For fluids with similar thermophysical properties, the one with higher surface tension usually has a higher Leidenfrost temperature.
-The Leidenfrost point signifies the onset of stable film boiling. It represents the point on the boiling curve where the heat flux is at the minimum and the surface is completely covered by a vapor blanket. Heat transfer from the surface to the liquid occurs by conduction and radiation through the vapour. In 1756, Leidenfrost observed that water droplets supported by the vapor film slowly evaporate as they move about on the hot surface. As the surface temperature is increased, radiation through the vapor film becomes more significant and the heat flux increases with increasing excess temperature.
 A. The 'reactive Leidenfrost effect' is a phenomenon where solid particles float above hot surfaces and move erratically, observed in non-volatile materials.
 B. The 'reactive Leidenfrost effect' is a phenomenon where solid particles float above hot surfaces and move erratically, observed in volatile materials.
 C. The 'reactive Leidenfrost effect' is a phenomenon where solid particles sink into hot surfaces and move slowly, observed in non-volatile materials.
 D. The 'reactive Leidenfrost effect' is a phenomenon where solid particles float above cold surfaces and move erratically, observed in non-volatile materials.
 E. The 'reactive Leidenfrost effect' is a phenomenon where solid particles sink into cold surfaces and move slowly, observed in non-volatile materials. "
What is reciprocal length or inverse length?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is reciprocal length or inverse length?
 Context: -Reciprocal length or inverse length is a quantity or measurement used in several branches of science and mathematics. As the reciprocal of length, common units used for this measurement include the reciprocal metre or inverse metre (symbol: m−1), the reciprocal centimetre or inverse centimetre (symbol: cm−1).
-The inverse second or reciprocal second (s−1), also called per second, is a unit defined as the multiplicative inverse of the second (a unit of time). It is applicable for physical quantities of dimension reciprocal time, such as frequency and strain rate.
-The inverse minute or reciprocal minute (min−1), also called per minute, is 60−1 s−1, as 1 min = 60 s; it is used in quantities of type ""counts per minute"", such as: Actions per minute Beats per minute Counts per minute Revolutions per minute (rpm) Words per minute 
-The energy is inversely proportional to the size of the unit of which the reciprocal is used, and is proportional to the number of reciprocal length units. For example, in terms of energy, one reciprocal metre equals 10−2 (one hundredth) as much as a reciprocal centimetre. Five reciprocal metres are five times as much energy as one reciprocal metre.
-Quantities measured in reciprocal length include: absorption coefficient or attenuation coefficient, in materials science curvature of a line, in mathematics gain, in laser physics magnitude of vectors in reciprocal space, in crystallography more generally any spatial frequency e.g. in cycles per unit length optical power of a lens, in optics rotational constant of a rigid rotor, in quantum mechanics wavenumber, or magnitude of a wavevector, in spectroscopy density of a linear feature in hydrology and other fields; see kilometre per square kilometre surface area to volume ratioIn optics, the dioptre is a unit equivalent to reciprocal metre.
 A. Reciprocal length or inverse length is a quantity or measurement used in physics and chemistry. It is the reciprocal of time, and common units used for this measurement include the reciprocal second or inverse second (symbol: s−1), the reciprocal minute or inverse minute (symbol: min−1).
 B. Reciprocal length or inverse length is a quantity or measurement used in geography and geology. It is the reciprocal of area, and common units used for this measurement include the reciprocal square metre or inverse square metre (symbol: m−2), the reciprocal square kilometre or inverse square kilometre (symbol: km−2).
 C. Reciprocal length or inverse length is a quantity or measurement used in biology and medicine. It is the reciprocal of mass, and common units used for this measurement include the reciprocal gram or inverse gram (symbol: g−1), the reciprocal kilogram or inverse kilogram (symbol: kg−1).
 D. Reciprocal length or inverse length is a quantity or measurement used in economics and finance. It is the reciprocal of interest rate, and common units used for this measurement include the reciprocal percent or inverse percent (symbol: %−1), the reciprocal basis point or inverse basis point (symbol: bp−1).
 E. Reciprocal length or inverse length is a quantity or measurement used in several branches of science and mathematics. It is the reciprocal of length, and common units used for this measurement include the reciprocal metre or inverse metre (symbol: m−1), the reciprocal centimetre or inverse centimetre (symbol: cm−1). "
Which of the following statements is true about the categorization of planetary systems according to their orbital dynamics?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: Which of the following statements is true about the categorization of planetary systems according to their orbital dynamics?
 Context: -Orbital dynamics Planetary systems can be categorized according to their orbital dynamics as resonant, non-resonant-interacting, hierarchical, or some combination of these. In resonant systems the orbital periods of the planets are in integer ratios. The Kepler-223 system contains four planets in an 8:6:4:3 orbital resonance.
Giant planets are found in mean-motion resonances more often than smaller planets.
In interacting systems the planets orbits are close enough together that they perturb the orbital parameters. The Solar System could be described as weakly interacting. In strongly interacting systems Kepler's laws do not hold.
In hierarchical systems the planets are arranged so that the system can be gravitationally considered as a nested system of two-bodies, e.g. in a star with a close-in hot jupiter with another gas giant much further out, the star and hot jupiter form a pair that appears as a single object to another planet that is far enough out.
Other, as yet unobserved, orbital possibilities include: double planets; various co-orbital planets such as quasi-satellites, trojans and exchange orbits; and interlocking orbits maintained by precessing orbital planes.
-The classes of TNO have no universally agreed precise definitions, the boundaries are often unclear and the notion of resonance is not defined precisely. The Deep Ecliptic Survey introduced formally defined dynamical classes based on long-term forward integration of orbits under the combined perturbations from all four giant planets. (see also formal definition of classical KBO) In general, the mean-motion resonance may involve not only orbital periods of the form  p⋅λ−q⋅λN where p and q are small integers, λ and λN are respectively the mean longitudes of the object and Neptune, but can also involve the longitude of the perihelion and the longitudes of the nodes (see orbital resonance, for elementary examples) An object is resonant if for some small integers (p,q,n,m,r,s), the argument (angle) defined below is librating (i.e. is bounded): ϕ=p⋅λ−q⋅λN−m⋅ϖ−n⋅Ω−r⋅ϖN−s⋅ΩN where the  ϖ are the longitudes of perihelia and the  Ω are the longitudes of the ascending nodes, for Neptune (with subscripts ""N"") and the resonant object (no subscripts).
-Orbital In celestial mechanics, an orbital resonance occurs when two orbiting bodies exert a regular, periodic gravitational influence on each other, usually due to their orbital periods being related by a ratio of two small integers. Orbital resonances greatly enhance the mutual gravitational influence of the bodies. In most cases, this results in an unstable interaction, in which the bodies exchange momentum and shift orbits until the resonance no longer exists. Under some circumstances, a resonant system can be stable and self-correcting, so that the bodies remain in resonance. Examples are the 1:2:4 resonance of Jupiter's moons Ganymede, Europa, and Io, and the 2:3 resonance between Pluto and Neptune. Unstable resonances with Saturn's inner moons give rise to gaps in the rings of Saturn. The special case of 1:1 resonance (between bodies with similar orbital radii) causes large Solar System bodies to clear the neighborhood around their orbits by ejecting nearly everything else around them; this effect is used in the current definition of a planet.
-In celestial mechanics, orbital resonance occurs when orbiting bodies exert regular, periodic gravitational influence on each other, usually because their orbital periods are related by a ratio of small integers. Most commonly, this relationship is found between a pair of objects (binary resonance). The physical principle behind orbital resonance is similar in concept to pushing a child on a swing, whereby the orbit and the swing both have a natural frequency, and the body doing the ""pushing"" will act in periodic repetition to have a cumulative effect on the motion. Orbital resonances greatly enhance the mutual gravitational influence of the bodies (i.e., their ability to alter or constrain each other's orbits). In most cases, this results in an unstable interaction, in which the bodies exchange momentum and shift orbits until the resonance no longer exists. Under some circumstances, a resonant system can be self-correcting and thus stable. Examples are the 1:2:4 resonance of Jupiter's moons Ganymede, Europa and Io, and the 2:3 resonance between Neptune and Pluto. Unstable resonances with Saturn's inner moons give rise to gaps in the rings of Saturn. The special case of 1:1 resonance between bodies with similar orbital radii causes large planetary system bodies to eject most other bodies sharing their orbits; this is part of the much more extensive process of clearing the neighbourhood, an effect that is used in the current definition of a planet.A binary resonance ratio in this article should be interpreted as the ratio of number of orbits completed in the same time interval, rather than as the ratio of orbital periods, which would be the inverse ratio. Thus, the 2:3 ratio above means that Pluto completes two orbits in the time it takes Neptune to complete three. In the case of resonance relationships among three or more bodies, either type of ratio may be used (whereby the smallest whole-integer ratio sequences are not necessarily reversals of each other), and the type of ratio will be specified.
-The category dwarf planet arose from a conflict between dynamical and geophysical ideas of what a useful conception of a planet would be. In terms of the dynamics of the Solar System, the major distinction is between bodies that gravitationally dominate their neighbourhood (Mercury through Neptune) and those that do not (such as the asteroids and Kuiper belt objects). A celestial body may have a dynamic (planetary) geology at approximately the mass required for its mantle to become plastic under its own weight, which results in the body acquiring a round shape. Because this requires a much lower mass than gravitationally dominating the region of space near their orbit, there are a population of objects that are massive enough to have a world-like appearance and planetary geology, but not massive enough to clear their neighborhood. Examples are Ceres in the asteroid belt and Pluto in the Kuiper belt.Dynamicists usually prefer using gravitational dominance as the threshold for planethood, because from their perspective smaller bodies are better grouped with their neighbours, e.g. Ceres as simply a large asteroid and Pluto as a large Kuiper belt object. Geoscientists usually prefer roundness as the threshold, because from their perspective the internally driven geology of a body like Ceres makes it more similar to a classical planet like Mars, than to a small asteroid that lacks internally driven geology. This necessitated the creation of the category of dwarf planets to describe this intermediate class.
 A. Planetary systems cannot be categorized based on their orbital dynamics.
 B. Planetary systems can be categorized as resonant, non-resonant-interacting, hierarchical, or some combination of these, but only based on the number of planets in the system.
 C. Planetary systems can only be categorized as resonant or non-resonant-interacting.
 D. Planetary systems can be categorized as resonant, non-resonant-interacting, hierarchical, or some combination of these.
 E. Planetary systems can only be categorized as hierarchical or non-hierarchical. "
What is the propagation constant in sinusoidal waves?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the propagation constant in sinusoidal waves?
 Context: -Propagation constant The propagation constant of the sinusoidal electromagnetic wave is a measure of the change undergone by the amplitude and phase of the wave as it propagates in a given direction. The quantity being measured can be the voltage, the current in a circuit, or a field vector such as electric field strength or flux density. The propagation constant itself measures the change per unit length, but it is otherwise dimensionless. In the context of two-port networks and their cascades, propagation constant measures the change undergone by the source quantity as it propagates from one port to the next.
-The propagation constant of a sinusoidal electromagnetic wave is a measure of the change undergone by the amplitude and phase of the wave as it propagates in a given direction. The quantity being measured can be the voltage, the current in a circuit, or a field vector such as electric field strength or flux density. The propagation constant itself measures the change per unit length, but it is otherwise dimensionless. In the context of two-port networks and their cascades, propagation constant measures the change undergone by the source quantity as it propagates from one port to the next.
-The propagation constant's value is expressed logarithmically, almost universally to the base e, rather than the more usual base 10 that is used in telecommunications in other situations. The quantity measured, such as voltage, is expressed as a sinusoidal phasor. The phase of the sinusoid varies with distance which results in the propagation constant being a complex number, the imaginary part being caused by the phase change.
-The term propagation constant or propagation function is applied to filters and other two-port networks used for signal processing. In these cases, however, the attenuation and phase coefficients are expressed in terms of nepers and radians per network section rather than per unit length. Some authors make a distinction between per unit length measures (for which ""constant"" is used) and per section measures (for which ""function"" is used).
-Equation (1.1) has an analytical solution given by exp 1.2 ) Where k is the wave number. When the wave propagates in inhomogeneous seismic media the propagation constant k must be a complex value that includes not only an imaginary part, the frequency-dependent attenuation coefficient, but also a real part, the dispersive wavenumber. We can call this K(w) a propagation constant in line with Futterman.
 A. The propagation constant is a measure of the amplitude of the sinusoidal wave that varies with distance.
 B. The propagation constant is a real number that remains constant with distance due to the phase change in the sinusoidal wave.
 C. The propagation constant is a real number that varies with distance due to the phase change in the sinusoidal wave.
 D. The propagation constant is a complex number that varies with distance due to the phase change in the sinusoidal wave.
 E. The propagation constant is a complex number that remains constant with distance due to the phase change in the sinusoidal wave. "
What is the gravitomagnetic interaction?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the gravitomagnetic interaction?
 Context: -Some higher-order gravitomagnetic effects can reproduce effects reminiscent of the interactions of more conventional polarized charges. For instance, if two wheels are spun on a common axis, the mutual gravitational attraction between the two wheels will be greater if they spin in opposite directions than in the same direction. This can be expressed as an attractive or repulsive gravitomagnetic component.
-The test particle is not drawn to the bottom stream because of a velocity-dependent force that serves to repel a particle that is moving in the same direction as the bottom stream. This velocity-dependent gravitational effect is gravitomagnetism.: 245–253 Matter in motion through a gravitomagnetic field is hence subject to so-called frame-dragging effects analogous to electromagnetic induction. It has been proposed that such gravitomagnetic forces underlie the generation of the relativistic jets (Fig. 5-8) ejected by some rotating supermassive black holes.
-Some scientists hypothesize that a fifth force might exist, but these hypotheses remain speculative.Each of the known fundamental interactions can be described mathematically as a field. The gravitational force is attributed to the curvature of spacetime, described by Einstein's general theory of relativity. The other three are discrete quantum fields, and their interactions are mediated by elementary particles described by the Standard Model of particle physics.Within the Standard Model, the strong interaction is carried by a particle called the gluon and is responsible for quarks binding together to form hadrons, such as protons and neutrons. As a residual effect, it creates the nuclear force that binds the latter particles to form atomic nuclei. The weak interaction is carried by particles called W and Z bosons, and also acts on the nucleus of atoms, mediating radioactive decay. The electromagnetic force, carried by the photon, creates electric and magnetic fields, which are responsible for the attraction between orbital electrons and atomic nuclei which holds atoms together, as well as chemical bonding and electromagnetic waves, including visible light, and forms the basis for electrical technology. Although the electromagnetic force is far stronger than gravity, it tends to cancel itself out within large objects, so over large (astronomical) distances gravity tends to be the dominant force, and is responsible for holding together the large scale structures in the universe, such as planets, stars, and galaxies.
-Electromagnetic interaction: the familiar interaction that acts on electrically charged particles. The photon is the exchange particle for this force.
Weak interaction: a short-range interaction responsible for some forms of radioactivity, that acts on electrons, neutrinos, and quarks. It is mediated by the W and Z bosons.
Gravitational interaction: a long-range attractive interaction that acts on all particles. The postulated exchange particle has been named the graviton.Modern unified field theory attempts to bring these four forces and matter together into a single framework.
-Just as electric charge and current multipoles contribute to the electromagnetic field, mass and mass-current multipoles contribute to the gravitational field in general relativity, causing the so-called gravitomagnetic effects. Changing mass-current multipoles can also give off gravitational radiation. However, contributions from the current multipoles will typically be much smaller than that of the mass quadrupole.
 A. The gravitomagnetic interaction is a force that is produced by the rotation of atoms in materials with linear properties that enhance time-varying gravitational fields.
 B. The gravitomagnetic interaction is a force that acts against gravity, produced by materials that have nonlinear properties that enhance time-varying gravitational fields.
 C. The gravitomagnetic interaction is a new force of nature generated by rotating matter, whose intensity is proportional to the rate of spin, according to the general theory of relativity.
 D. The gravitomagnetic interaction is a force that occurs in neutron stars, producing a gravitational analogue of the Meissner effect.
 E. The gravitomagnetic interaction is a force that is produced by the rotation of atoms in materials of different gravitational permeability. "
What did Newton's manuscripts of the 1660s show?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What did Newton's manuscripts of the 1660s show?
 Context: -In regard to evidence that still survives of the earlier history, manuscripts written by Newton in the 1660s show that Newton himself had, by 1669, arrived at proofs that in a circular case of planetary motion, ""endeavour to recede"" (what was later called centrifugal force) had an inverse-square relation with distance from the center. After his 1679–1680 correspondence with Hooke, Newton adopted the language of inward or centripetal force. According to Newton scholar J. Bruce Brackenridge, although much has been made of the change in language and difference of point of view, as between centrifugal or centripetal forces, the actual computations and proofs remained the same either way. They also involved the combination of tangential and radial displacements, which Newton was making in the 1660s. The lesson offered by Hooke to Newton here, although significant, was one of perspective and did not change the analysis. This background shows there was basis for Newton to deny deriving the inverse square law from Hooke.
-Newton's early work on motion In the 1660s Newton studied the motion of colliding bodies and deduced that the centre of mass of two colliding bodies remains in uniform motion. Surviving manuscripts of the 1660s also show Newton's interest in planetary motion and that by 1669 he had shown, for a circular case of planetary motion, that the force he called ""endeavour to recede"" (now called centrifugal force) had an inverse-square relation with distance from the center. After his 1679–1680 correspondence with Hooke, described below, Newton adopted the language of inward or centripetal force. According to Newton scholar J. Bruce Brackenridge, although much has been made of the change in language and difference of point of view, as between centrifugal or centripetal forces, the actual computations and proofs remained the same either way. They also involved the combination of tangential and radial displacements, which Newton was making in the 1660s. The difference between the centrifugal and centripetal points of view, though a significant change of perspective, did not change the analysis. Newton also clearly expressed the concept of linear inertia in the 1660s: for this Newton was indebted to Descartes' work published 1644.
-Newton's acknowledgment On the other hand, Newton did accept and acknowledge, in all editions of the Principia, that Hooke (but not exclusively Hooke) had separately appreciated the inverse square law in the solar system. Newton acknowledged Wren, Hooke, and Halley in this connection in the Scholium to Proposition 4 in Book 1. Newton also acknowledged to Halley that his correspondence with Hooke in 1679–80 had reawakened his dormant interest in astronomical matters, but that did not mean, according to Newton, that Hooke had told Newton anything new or original: ""yet am I not beholden to him for any light into that business but only for the diversion he gave me from my other studies to think on these things & for his dogmaticalness in writing as if he had found the motion in the Ellipsis, which inclined me to try it ..."" Modern priority controversy Since the time of Newton and Hooke, scholarly discussion has also touched on the question of whether Hooke's 1679 mention of 'compounding the motions' provided Newton with something new and valuable, even though that was not a claim actually voiced by Hooke at the time. As described above, Newton's manuscripts of the 1660s do show him actually combining tangential motion with the effects of radially directed force or endeavour, for example in his derivation of the inverse square relation for the circular case. They also show Newton clearly expressing the concept of linear inertia—for which he was indebted to Descartes' work, published in 1644 (as Hooke probably was). These matters do not appear to have been learned by Newton from Hooke.
-Later, in 1686, when Newton's Principia had been presented to the Royal Society, Hooke claimed from this correspondence the credit for some of Newton's content in the Principia, and said Newton owed the idea of an inverse-square law of attraction to him – although at the same time, Hooke disclaimed any credit for the curves and trajectories that Newton had demonstrated on the basis of the inverse square law.Newton, who heard of this from Halley, rebutted Hooke's claim in letters to Halley, acknowledging only an occasion of reawakened interest. Newton did acknowledge some prior work of others, including Ismaël Bullialdus, who suggested (but without demonstration) that there was an attractive force from the Sun in the inverse square proportion to the distance, and Giovanni Alfonso Borelli, who suggested (again without demonstration) that there was a tendency towards the Sun like gravity or magnetism that would make the planets move in ellipses; but that the elements Hooke claimed were due either to Newton himself, or to other predecessors of them both such as Bullialdus and Borelli, but not Hooke. Wren and Halley were both skeptical of Hooke's claims, recalling an occasion when Hooke had claimed to have a derivation of planetary motions under an inverse square law, but had failed to produce it even under the incentive of a prize.There has been scholarly controversy over exactly what if anything Newton really gained from Hooke, apart from the stimulus that Newton acknowledged.About thirty years after Newton's death in 1727, Alexis Clairaut, one of Newton's early and eminent successors in the field of gravitational studies, wrote after reviewing Hooke's work that it showed ""what a distance there is between a truth that is glimpsed and a truth that is demonstrated"".
-Newton's work and claims Newton, faced in May 1686 with Hooke's claim on the inverse square law, denied that Hooke was to be credited as author of the idea. Among the reasons, Newton recalled that the idea had been discussed with Sir Christopher Wren previous to Hooke's 1679 letter. Newton also pointed out and acknowledged prior work of others, including Bullialdus, (who suggested, but without demonstration, that there was an attractive force from the Sun in the inverse square proportion to the distance), and Borelli (who suggested, also without demonstration, that there was a centrifugal tendency in counterbalance with a gravitational attraction towards the Sun so as to make the planets move in ellipses). D T Whiteside has described the contribution to Newton's thinking that came from Borelli's book, a copy of which was in Newton's library at his death.Newton further defended his work by saying that had he first heard of the inverse square proportion from Hooke, he would still have some rights to it in view of his demonstrations of its accuracy. Hooke, without evidence in favor of the supposition, could only guess that the inverse square law was approximately valid at great distances from the center. According to Newton, while the 'Principia' was still at pre-publication stage, there were so many a priori reasons to doubt the accuracy of the inverse-square law (especially close to an attracting sphere) that ""without my (Newton's) Demonstrations, to which Mr Hooke is yet a stranger, it cannot believed by a judicious Philosopher to be any where accurate.""This remark refers among other things to Newton's finding, supported by mathematical demonstration, that if the inverse square law applies to tiny particles, then even a large spherically symmetrical mass also attracts masses external to its surface, even close up, exactly as if all its own mass were concentrated at its center. Thus Newton gave a justification, otherwise lacking, for applying the inverse square law to large spherical planetary masses as if they were tiny particles. In addition, Newton had formulated, in Propositions 43–45 of Book 1 and associated sections of Book 3, a sensitive test of the accuracy of the inverse square law, in which he showed that only where the law of force is calculated as the inverse square of the distance will the directions of orientation of the planets' orbital ellipses stay constant as they are observed to do apart from small effects attributable to inter-planetary perturbations.
 A. Newton learned about tangential motion and radially directed force or endeavour from Hooke's work.
 B. Newton's manuscripts did not show any evidence of combining tangential motion with the effects of radially directed force or endeavour.
 C. Newton combined tangential motion with the effects of radially directed force or endeavour and expressed the concept of linear inertia.
 D. Newton's manuscripts showed that he learned about the inverse square law from Hooke's private papers.
 E. Newton's manuscripts showed that he was indebted to Descartes' work, published in 1644, for the concept of linear inertia. "
What is the decay energy for the free neutron decay process?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the decay energy for the free neutron decay process?
 Context: -For the free neutron the decay energy for this process (based on the masses of the neutron, proton, and electron) is 0.782343 MeV. The maximal energy of the beta decay electron (in the process wherein the neutrino receives a vanishingly small amount of kinetic energy) has been measured at 0.782±0.013 MeV. The latter number is not well-enough measured to determine the comparatively tiny rest mass of the neutrino (which must in theory be subtracted from the maximal electron kinetic energy) as well as neutrino mass is constrained by many other methods.
-For the free neutron, the decay energy for this process (based on the rest masses of the neutron, proton and electron) is 0.782343 MeV. That is the difference between the rest mass of the neutron and the sum of the rest masses of the products. That difference has to be carried away as kinetic energy. The maximal energy of the beta decay electron (in the process wherein the neutrino receives a vanishingly small amount of kinetic energy) has been measured at 0.782±0.013 MeV. The latter number is not well-enough measured to determine the comparatively tiny rest mass of the neutrino (which must in theory be subtracted from the maximal electron kinetic energy); furthermore, neutrino mass is constrained by many other methods.
-Free neutron decay Outside the nucleus, free neutrons are unstable and have a mean lifetime of 879.6±0.8 s (about 14 minutes, 40 seconds); therefore the half-life for this process (which differs from the mean lifetime by a factor of ln(2) = 0.693) is 610.1±0.7 s (about 10 minutes, 10 seconds). This decay is only possible because the mass of the proton is less than that of the neutron. By the mass-energy equivalence, when a neutron decays to a proton this way, a lower energy state is attained.
-In a typical nuclear fission reaction, 187 MeV of energy are released instantaneously in the form of kinetic energy from the fission products, kinetic energy from the fission neutrons, instantaneous gamma rays, or gamma rays from the capture of neutrons. An additional 23 MeV of energy are released at some time after fission from the beta decay of fission products. About 10 MeV of the energy released from the beta decay of fission products is in the form of neutrinos, and since neutrinos are very weakly interacting, this 10 MeV of energy will not be deposited in the reactor core. This results in 13 MeV (6.5% of the total fission energy) being deposited in the reactor core from delayed beta decay of fission products, at some time after any given fission reaction has occurred. In a steady state, this heat from delayed fission product beta decay contributes 6.5% of the normal reactor heat output.
-Outside the nucleus, free neutrons are unstable and have a mean lifetime of 14 minutes, 42 seconds. Free neutrons decay by emission of an electron and an electron antineutrino to become a proton, a process known as beta decay:In the adjacent diagram, a neutron collides with a proton of the target material, and then becomes a fast recoil proton that ionizes in turn. At the end of its path, the neutron is captured by a nucleus in an (n,γ)-reaction that leads to the emission of a neutron capture photon. Such photons always have enough energy to qualify as ionizing radiation.
 A. 0.013343 MeV
 B. 0.013 MeV
 C. 1,000 MeV
 D. 0.782 MeV
 E. 0.782343 MeV "
What is Hesse's principle of transfer in geometry?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is Hesse's principle of transfer in geometry?
 Context: -In geometry, Hesse's principle of transfer (German: Übertragungsprinzip) states that if the points of the projective line P1 are depicted by a rational normal curve in Pn, then the group of the projective transformations of Pn that preserve the curve is isomorphic to the group of the projective transformations of P1 (this is a generalization of the original Hesse's principle, in a form suggested by Wilhelm Franz Meyer). It was originally introduced by Otto Hesse in 1866, in a more restricted form. It influenced Felix Klein in the development of the Erlangen program. Since its original conception, it was generalized by many mathematicians, including Klein, Fano, and Cartan.
-In model theory, a transfer principle states that all statements of some language that are true for some structure are true for another structure. One of the first examples was the Lefschetz principle, which states that any sentence in the first-order language of fields that is true for the complex numbers is also true for any algebraically closed field of characteristic 0.
-In chemistry, transfer hydrogenation is a chemical reaction involving the addition of hydrogen to a compound from a source other than molecular H2. It is applied in laboratory and industrial organic synthesis to saturate organic compounds and reduce ketones to alcohols, and imines to amines. It avoids the need for high-pressure molecular H2 used in conventional hydrogenation. Transfer hydrogenation usually occurs at mild temperature and pressure conditions using organic or organometallic catalysts, many of which are chiral, allowing efficient asymmetric synthesis. It uses hydrogen donor compounds such as formic acid, isopropanol or dihydroanthracene, dehydrogenating them to CO2, acetone, or anthracene respectively. Often, the donor molecules also function as solvents for the reaction. A large scale application of transfer hydrogenation is coal liquefaction using ""donor solvents"" such as tetralin.
-The transfer principle concerns the logical relation between the properties of the real numbers R, and the properties of a larger field denoted *R called the hyperreal numbers. The field *R includes, in particular, infinitesimal (""infinitely small"") numbers, providing a rigorous mathematical realisation of a project initiated by Leibniz.
-Mass transfer is the net movement of mass from one location (usually meaning stream, phase, fraction or component) to another. Mass transfer occurs in many processes, such as absorption, evaporation, drying, precipitation, membrane filtration, and distillation. Mass transfer is used by different scientific disciplines for different processes and mechanisms. The phrase is commonly used in engineering for physical processes that involve diffusive and convective transport of chemical species within physical systems.
 A. Hesse's principle of transfer is a concept in biology that explains the transfer of genetic information from one generation to another.
 B. Hesse's principle of transfer is a concept in chemistry that explains the transfer of electrons between atoms in a chemical reaction.
 C. Hesse's principle of transfer is a concept in physics that explains the transfer of energy from one object to another.
 D. Hesse's principle of transfer is a concept in economics that explains the transfer of wealth from one individual to another.
 E. Hesse's principle of transfer is a concept in geometry that states that if the points of the projective line P1 are depicted by a rational normal curve in Pn, then the group of the projective transformations of Pn that preserve the curve is isomorphic to the group of the projective transformations of P1. "
What is the relationship between the Cauchy momentum equation and the Navier-Stokes equation?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the relationship between the Cauchy momentum equation and the Navier-Stokes equation?
 Context: -All non-relativistic momentum conservation equations, such as the Navier–Stokes equation, can be derived by beginning with the Cauchy momentum equation and specifying the stress tensor through a constitutive relation. By expressing the shear tensor in terms of viscosity and fluid velocity, and assuming constant density and viscosity, the Cauchy momentum equation will lead to the Navier–Stokes equations. By assuming inviscid flow, the Navier–Stokes equations can further simplify to the Euler equations.
-All non-relativistic balance equations, such as the Navier–Stokes equations, can be derived by beginning with the Cauchy equations and specifying the stress tensor through a constitutive relation. By expressing the deviatoric (shear) stress tensor in terms of viscosity and the fluid velocity gradient, and assuming constant viscosity, the above Cauchy equations will lead to the Navier–Stokes equations below.
-The classic Navier-Stokes equation is the balance equation for momentum density for an isotropic, compressional and viscous fluid that is used in fluid mechanics in general and fluid dynamics in particular: ρ[∂u∂t+u⋅∇u]=−∇P+∇[ζ(∇⋅u)]+∇⋅[η(∇u+(∇u)T−23(∇⋅u)I)]+ρg On the right hand side is (the divergence of) the total stress tensor  σ which consists of a pressure tensor  (−PI) and a dissipative (or viscous or deviatoric) stress tensor  τd . The dissipative stress consists of a compression stress tensor  τc (term no. 2) and a shear stress tensor  τs (term no. 3). The rightmost term  ρg is the gravitational force which is the body force contribution, and  ρ is the mass density, and  u is the fluid velocity.
-The Navier–Stokes momentum equation can be derived as a particular form of the Cauchy momentum equation, whose general convective form is By setting the Cauchy stress tensor  σ {\textstyle {\boldsymbol {\sigma }}} to be the sum of a viscosity term  τ {\textstyle {\boldsymbol {\tau }}} (the deviatoric stress) and a pressure term  − p I {\textstyle -p\mathbf {I} } (volumetric stress), we arrive at where D D t {\textstyle {\frac {\mathrm {D} }{\mathrm {D} t}}} is the material derivative, defined as  ∂ ∂ t + u ⋅ ∇ {\textstyle {\frac {\partial }{\partial t}}+\mathbf {u} \cdot \nabla } , ρ {\textstyle \rho } is the (mass) density, u {\textstyle \mathbf {u} } is the flow velocity, ∇ ⋅ {\textstyle \nabla \cdot \,} is the divergence, p {\textstyle p} is the pressure, t {\textstyle t} is time, τ {\textstyle {\boldsymbol {\tau }}} is the deviatoric stress tensor, which has order 2, g {\textstyle \mathbf {g} } represents body accelerations acting on the continuum, for example gravity, inertial accelerations, electrostatic accelerations, and so on.In this form, it is apparent that in the assumption of an inviscid fluid – no deviatoric stress – Cauchy equations reduce to the Euler equations.
-It is closely related to the Navier–Stokes equations, because the flow of momentum in a fluid is mathematically similar to the flow of mass or energy. The correspondence is clearest in the case of an incompressible Newtonian fluid, in which case the Navier–Stokes equation is: where M is the momentum of the fluid (per unit volume) at each point (equal to the density ρ multiplied by the velocity v), μ is viscosity, P is fluid pressure, and f is any other body force such as gravity. In this equation, the term on the left-hand side describes the change in momentum at a given point; the first term on the right describes the diffusion of momentum by viscosity; the second term on the right describes the advective flow of momentum; and the last two terms on the right describes the external and internal forces which can act as sources or sinks of momentum.
 A. The Navier-Stokes equation can be derived from the Cauchy momentum equation by specifying the stress tensor through a constitutive relation, expressing the shear tensor in terms of viscosity and fluid velocity, and assuming constant density and viscosity.
 B. The Navier-Stokes equation is a simplified version of the Cauchy momentum equation that only applies to situations with constant density and viscosity.
 C. The Navier-Stokes equation is a special case of the Cauchy momentum equation, which is a more general equation that applies to all non-relativistic momentum conservation situations.
 D. The Cauchy momentum equation and the Navier-Stokes equation are completely unrelated and cannot be used interchangeably in any situation.
 E. The Cauchy momentum equation is a special case of the Navier-Stokes equation, which is a more general equation that applies to all non-relativistic momentum conservation situations. "
What is X-ray pulsar-based navigation (XNAV)?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is X-ray pulsar-based navigation (XNAV)?
 Context: -X-ray pulsar-based navigation and timing (XNAV) or simply pulsar navigation is a navigation technique whereby the periodic X-ray signals emitted from pulsars are used to determine the location of a vehicle, such as a spacecraft in deep space. A vehicle using XNAV would compare received X-ray signals with a database of known pulsar frequencies and locations. Similar to GPS, this comparison would allow the vehicle to calculate its position accurately (±5 km). The advantage of using X-ray signals over radio waves is that X-ray telescopes can be made smaller and lighter. Experimental demonstrations have been reported in 2018.
-Pulsar navigation X-ray pulsar-based navigation and timing (XNAV) or simply pulsar navigation is a navigation technique whereby the periodic X-ray signals emitted from pulsars are used to determine the location of a vehicle, such as a spacecraft in deep space. A vehicle using XNAV would compare received X-ray signals with a database of known pulsar frequencies and locations. Similar to GPS, this comparison would allow the vehicle to calculate its position accurately (±5 km). The advantage of using X-ray signals over radio waves is that X-ray telescopes can be made smaller and lighter. Experimental demonstrations have been reported in 2018.
-X-ray pulsar-based navigation and timing (XNAV) is an experimental navigation technique whereby the periodic X-ray signals emitted from pulsars are used to determine the location of a vehicle, such as a spacecraft in deep space. A vehicle using XNAV would compare received X-ray signals with a database of known pulsar frequencies and locations. Similar to GNSS, this comparison would allow the vehicle to triangulate its position accurately (±5 km). The advantage of using X-ray signals over radio waves is that X-ray telescopes can be made smaller and lighter. On 9 November 2016 the Chinese Academy of Sciences launched an experimental pulsar navigation satellite called XPNAV 1. SEXTANT (Station Explorer for X-ray Timing and Navigation Technology) is a NASA-funded project developed at the Goddard Space Flight Center that is testing XNAV on-orbit on board the International Space Station in connection with the NICER project, launched on 3 June 2017 on the SpaceX CRS-11 ISS resupply mission.
-Experiments XPNAV 1 On 9 November 2016, the Chinese Academy of Sciences launched an experimental pulsar navigation satellite called XPNAV 1. XPNAV-1 has a mass of 240 kg, and is in a 493 km × 512 km, 97.41° orbit. XPNAV-1 will characterize 26 nearby pulsars for their pulse frequency and intensity to create a navigation database that could be used by future operational missions. The satellite is expected to operate for five to ten years. XPNAV-1 is the first pulsar navigation mission launched into orbit.SEXTANT SEXTANT (Station Explorer for X-ray Timing and Navigation Technology) is a NASA-funded project developed at the Goddard Space Flight Center that is testing XNAV on-orbit on board the International Space Station in connection with the NICER project, launched on 3 June 2017 on the SpaceX CRS-11 ISS resupply mission. If this is successful, XNAV may be used as secondary navigation technology for the planned Orion missions. In January 2018, X-ray navigation feasibility was demonstrated using NICER/SEXTANT on ISS. It reported a 7 km accuracy (in 2 days).
-Studies The Advanced Concepts Team of ESA studied in 2003 the feasibility of x-ray pulsar navigation in collaboration with the Universitat Politecnica de Catalunya in Spain. After the study, the interest in the XNAV technology within the European Space Agency was consolidated leading, in 2012, to two different and more detailed studies performed by GMV AEROSPACE AND DEFENCE (ES) and the National Physical Laboratory (UK).
 A. X-ray pulsar-based navigation (XNAV) is a navigation technique that uses the periodic X-ray signals emitted from pulsars to determine the location of a vehicle in the Earth's atmosphere.
 B. X-ray pulsar-based navigation (XNAV) is a navigation technique that uses the periodic radio signals emitted from pulsars to determine the location of a vehicle in deep space, such as a spacecraft.
 C. X-ray pulsar-based navigation (XNAV) is a navigation technique that uses the periodic X-ray signals emitted from satellites to determine the location of a vehicle in deep space, such as a spacecraft.
 D. X-ray pulsar-based navigation (XNAV) is a navigation technique that uses the periodic X-ray signals emitted from pulsars to determine the location of a vehicle in deep space, such as a spacecraft.
 E. X-ray pulsar-based navigation (XNAV) is a navigation technique that uses the periodic radio signals emitted from satellites to determine the location of a vehicle in deep space, such as a spacecraft. "
What is the evidence for the existence of a supermassive black hole at the center of the Milky Way galaxy?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the evidence for the existence of a supermassive black hole at the center of the Milky Way galaxy?
 Context: -In the Milky Way Evidence indicates that the Milky Way galaxy has a supermassive black hole at its center, 26,000 light-years from the Solar System, in a region called Sagittarius A* because: The star S2 follows an elliptical orbit with a period of 15.2 years and a pericenter (closest distance) of 17 light-hours (1.8×1013 m or 120 AU) from the center of the central object.
-It is thought that supermassive black holes like these do not form immediately from the singular collapse of a cluster of stars. Instead they may begin life as smaller, stellar-sized black holes and grow larger by the accretion of matter, or even of other black holes.The Schwarzschild radius of the supermassive black hole at the Galactic Center of the Milky Way is approximately 12 million kilometres. Its mass is about 4.1 million M☉.
-Outside the Milky Way Unambiguous dynamical evidence for supermassive black holes exists only for a handful of galaxies; these include the Milky Way, the Local Group galaxies M31 and M32, and a few galaxies beyond the Local Group, such as NGC 4395. In these galaxies, the root mean square (or rms) velocities of the stars or gas rises proportionally to 1/r near the center, indicating a central point mass. In all other galaxies observed to date, the rms velocities are flat, or even falling, toward the center, making it impossible to state with certainty that a supermassive black hole is present. Nevertheless, it is commonly accepted that the center of nearly every galaxy contains a supermassive black hole. The reason for this assumption is the M–sigma relation, a tight (low scatter) relation between the mass of the hole in the 10 or so galaxies with secure detections, and the velocity dispersion of the stars in the bulges of those galaxies. This correlation, although based on just a handful of galaxies, suggests to many astronomers a strong connection between the formation of the black hole and the galaxy itself.On March 28, 2011, a supermassive black hole was seen tearing a mid-size star apart. That is the only likely explanation of the observations that day of sudden X-ray radiation and the follow-up broad-band observations. The source was previously an inactive galactic nucleus, and from study of the outburst the galactic nucleus is estimated to be a SMBH with mass of the order of a million M☉. This rare event is assumed to be a relativistic outflow (material being emitted in a jet at a significant fraction of the speed of light) from a star tidally disrupted by the SMBH. A significant fraction of a solar mass of material is expected to have accreted onto the SMBH. Subsequent long-term observation will allow this assumption to be confirmed if the emission from the jet decays at the expected rate for mass accretion onto a SMBH.
-Many bulges are thought to host a supermassive black hole at their centers. In our own galaxy, for instance, the object called Sagittarius A* is believed to be a supermassive black hole. There are many lines of evidence for the existence of black holes in spiral galaxy centers, including the presence of active nuclei in some spiral galaxies, and dynamical measurements that find large compact central masses in galaxies such as Messier 106.
-Proper motions of stars orbiting Sagittarius A* The proper motions of stars near the centre of our own Milky Way provide strong observational evidence that these stars are orbiting a supermassive black hole. Since 1995, astronomers have tracked the motions of 90 stars orbiting an invisible object coincident with the radio source Sagittarius A*. By fitting their motions to Keplerian orbits, the astronomers were able to infer, in 1998, that a 2.6×106 M☉ object must be contained in a volume with a radius of 0.02 light-years to cause the motions of those stars. Since then, one of the stars—called S2—has completed a full orbit. From the orbital data, astronomers were able to refine the calculations of the mass to 4.3×106 M☉ and a radius of less than 0.002 light-years for the object causing the orbital motion of those stars. The upper limit on the object's size is still too large to test whether it is smaller than its Schwarzschild radius; nevertheless, these observations strongly suggest that the central object is a supermassive black hole as there are no other plausible scenarios for confining so much invisible mass into such a small volume. Additionally, there is some observational evidence that this object might possess an event horizon, a feature unique to black holes.
 A. The Milky Way galaxy has a supermassive black hole at its center because of the bright flare activity observed near Sagittarius A*. The radius of the central object must be less than 17 light-hours, because otherwise S2 would collide with it. Observations of the star S14 indicate that the radius is no more than 6.25 light-hours, about the diameter of Uranus' orbit. No known astronomical object other than a black hole can contain 4.0 million M☉ in this volume of space.
 B. The Milky Way galaxy has a supermassive black hole at its center because the star S14 follows an elliptical orbit with a period of 15.2 years and a pericenter of 17 light-hours from the center of the central object. From the motion of star S14, the object's mass can be estimated as 4.0 million M☉, or about 7.96×1036 kg. The radius of the central object must be less than 17 light-hours, because otherwise S14 would collide with it. Observations of the star S2 indicate that the radius is no more than 6.25 light-hours, about the diameter of Uranus' orbit. No known astronomical object other than a black hole can contain 4.0 million M☉ in this volume of space.
 C. The Milky Way galaxy has a supermassive black hole at its center because of the bright flare activity observed near Sagittarius A*. The radius of the central object must be less than 6.25 light-hours, about the diameter of Uranus' orbit. Observations of the star S2 indicate that the radius is no more than 17 light-hours, because otherwise S2 would collide with it. No known astronomical object other than a black hole can contain 4.0 million M☉ in this volume of space.
 D. The Milky Way galaxy has a supermassive black hole at its center because it is the only explanation for the bright flare activity observed near Sagittarius A* at a separation of six to ten times the gravitational radius of the candidate SMBH.
 E. The star S2 follows an elliptical orbit with a period of 15.2 years and a pericenter of 17 light-hours from the center of the central object. From the motion of star S2, the object's mass can be estimated as 4.0 million M☉, or about 7.96×1036 kg. The radius of the central object must be less than 17 light-hours, because otherwise S2 would collide with it. Observations of the star S14 indicate that the radius is no more than 6.25 light-hours, about the diameter of Uranus' orbit. No known astronomical object other than a black hole can contain 4.0 million M☉ in this volume of space. "
What is the function of the fibrous cardiac skeleton?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the function of the fibrous cardiac skeleton?
 Context: -In cardiology, the cardiac skeleton, also known as the fibrous skeleton of the heart, is a high-density homogeneous structure of connective tissue that forms and anchors the valves of the heart, and influences the forces exerted by and through them. The cardiac skeleton separates and partitions the atria (the smaller, upper two chambers) from the ventricles (the larger, lower two chambers).The heart's cardiac skeleton comprises four dense connective tissue rings that encircle the mitral and tricuspid atrioventricular (AV) canals and extend to the origins of the pulmonary trunk and aorta. This provides crucial support and structure to the heart while also serving to electrically isolate the atria from the ventricles.The unique matrix of connective tissue within the cardiac skeleton isolates electrical influence within these defined chambers. In normal anatomy, there is only one conduit for electrical conduction from the upper chambers to the lower chambers, known as the atrioventricular node. The physiologic cardiac skeleton forms a firewall governing autonomic/electrical influence until bordering the bundle of His which further governs autonomic flow to the bundle branches of the ventricles. Understood as such, the cardiac skeleton efficiently centers and robustly funnels electrical energy from the atria to the ventricles.
-Cardiac The collagenous cardiac skeleton which includes the four heart valve rings, is histologically, elastically and uniquely bound to cardiac muscle. The cardiac skeleton also includes the separating septa of the heart chambers – the interventricular septum and the atrioventricular septum. Collagen contribution to the measure of cardiac performance summarily represents a continuous torsional force opposed to the fluid mechanics of blood pressure emitted from the heart. The collagenous structure that divides the upper chambers of the heart from the lower chambers is an impermeable membrane that excludes both blood and electrical impulses through typical physiological means. With support from collagen, atrial fibrillation never deteriorates to ventricular fibrillation. Collagen is layered in variable densities with smooth muscle mass. The mass, distribution, age, and density of collagen all contribute to the compliance required to move blood back and forth. Individual cardiac valvular leaflets are folded into shape by specialized collagen under variable pressure. Gradual calcium deposition within collagen occurs as a natural function of aging. Calcified points within collagen matrices show contrast in a moving display of blood and muscle, enabling methods of cardiac imaging technology to arrive at ratios essentially stating blood in (cardiac input) and blood out (cardiac output). Pathology of the collagen underpinning of the heart is understood within the category of connective tissue disease.
-The four cardiac valves are kept in their place partly because of the fibrous skeleton of the heart, which is a collection of connective tissue. It consists of the right fibrous trigone (which along with the membranous septum forms the central fibrous body), the left right fibrous trigone, and the conus tendon. The right fibrous trigone is the strongest part of the skeleton. It lies to the right of the aortic valve and connects it with the mitral and tricuspid valves. It is pierced by the Bundle of His. Lastly, the aortomitral curtain is also a part of the fibrous skeleton; it is formed by fibrous tissue connecting two of three of the aortic valve leaflets (the right and non-coronary leaflet) with anterior leaflet of the mitral valve.
-In humans, the axial skeleton serves to protect the brain, spinal cord, heart, and lungs. It also serves as the attachment site for muscles that move the head, neck, and back, and for muscles that act across the shoulder and hip joints to move their corresponding limbs.
-The structure of the components of the heart has become an area of increasing interest. The cardiac skeleton binds several bands of dense connective tissue, as collagen, that encircle the bases of the pulmonary trunk, aorta, and all four heart valves. While not a traditionally or ""true"" or rigid skeleton, it does provide structure and support for the heart, as well as isolate the atria from the ventricles. This is why atrial fibrillation almost never degrades to ventricular fibrillation. In youth, this collagen structure is free of calcium adhesions and is quite flexible. With aging, calcium and other mineral accumulation occur within this skeleton. Distensibility of the ventricles is tied to variable accumulation of minerals which also contributes to the delay of the depolarization wave in geriatric patients that can take place from the AV node and the bundle of His.
 A. The fibrous cardiac skeleton is a system of blood vessels that supplies oxygen and nutrients to the heart muscle.
 B. The fibrous cardiac skeleton is responsible for the pumping action of the heart, regulating the flow of blood through the atria and ventricles.
 C. The fibrous cardiac skeleton provides structure to the heart, forming the atrioventricular septum that separates the atria from the ventricles, and the fibrous rings that serve as bases for the four heart valves.
 D. The fibrous cardiac skeleton is a network of nerves that controls the heartbeat and rhythm of the heart.
 E. The fibrous cardiac skeleton is a protective layer that surrounds the heart, shielding it from external damage. "
What is the Carnot engine?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the Carnot engine?
 Context: -Carnot's principle The historical origin of the second law of thermodynamics was in Sadi Carnot's theoretical analysis of the flow of heat in steam engines (1824). The centerpiece of that analysis, now known as a Carnot engine, is an ideal heat engine fictively operated in the limiting mode of extreme slowness known as quasi-static, so that the heat and work transfers are between subsystems that are always in their own internal states of thermodynamic equilibrium. It represents the theoretical maximum efficiency of a heat engine operating between any two given thermal or heat reservoirs at different temperatures. Carnot's principle was recognized by Carnot at a time when the caloric theory represented the dominant understanding of the nature of heat, before the recognition of the first law of thermodynamics, and before the mathematical expression of the concept of entropy. Interpreted in the light of the first law, Carnot's analysis is physically equivalent to the second law of thermodynamics, and remains valid today. Some samples from his book are: ...wherever there exists a difference of temperature, motive power can be produced.The production of motive power is then due in steam engines not to an actual consumption of caloric, but to its transportation from a warm body to a cold body ...The motive power of heat is independent of the agents employed to realize it; its quantity is fixed solely by the temperatures of the bodies between which is effected, finally, the transfer of caloric.In modern terms, Carnot's principle may be stated more precisely: The efficiency of a quasi-static or reversible Carnot cycle depends only on the temperatures of the two heat reservoirs, and is the same, whatever the working substance. A Carnot engine operated in this way is the most efficient possible heat engine using those two temperatures.
-Carnot efficiency The second law of thermodynamics puts a fundamental limit on the thermal efficiency of all heat engines. Even an ideal, frictionless engine can't convert anywhere near 100% of its input heat into work. The limiting factors are the temperature at which the heat enters the engine,  TH , and the temperature of the environment into which the engine exhausts its waste heat,  TC , measured in an absolute scale, such as the Kelvin or Rankine scale. From Carnot's theorem, for any engine working between these two temperatures: ηth≤1−TCTH This limiting value is called the Carnot cycle efficiency because it is the efficiency of an unattainable, ideal, reversible engine cycle called the Carnot cycle. No device converting heat into mechanical energy, regardless of its construction, can exceed this efficiency.
-Due to the other causes detailed below, practical engines have efficiencies far below the Carnot limit. For example, the average automobile engine is less than 35% efficient.
Carnot's theorem applies to thermodynamic cycles, where thermal energy is converted to mechanical work. Devices that convert a fuel's chemical energy directly into electrical work, such as fuel cells, can exceed the Carnot efficiency.
-A Carnot heat engine is a heat engine performing a Carnot cycle, and its realization on a macroscopic scale is impractical. For example, for the isothermal expansion part of the Carnot cycle, the following conditions must be satisfied simultaneously at every step in the expansion: The hot reservoir temperature TH is infinitesimally higher than the system gas temperature T so heat flow (energy transfer) from the hot reservoir to the gas is made without increasing T (via infinitesimal work on the surroundings by the gas as another energy transfer); if TH is significantly higher than T, then T may be not uniform through the gas so the system would deviate from thermal equilibrium as well as not being a reversible process (i.e. not a Carnot cycle) or T might increase noticeably so it would not be an isothermal process.
-A quantum Carnot engine is one in which the atoms in the heat bath are given a small bit of quantum coherence. The phase of the atomic coherence provides a new control parameter.The deep physics behind the second law of thermodynamics is not violated; nevertheless, the quantum Carnot engine has certain features that are not possible in a classical engine.
 A. The Carnot engine is a theoretical engine that operates in the limiting mode of extreme speed known as dynamic. It represents the theoretical maximum efficiency of a heat engine operating between any two given thermal or heat reservoirs at different temperatures.
 B. The Carnot engine is an ideal heat engine that operates in the limiting mode of extreme slowness known as quasi-static. It represents the theoretical maximum efficiency of a heat engine operating between any two given thermal or heat reservoirs at different temperatures.
 C. The Carnot engine is a real heat engine that operates in the limiting mode of extreme speed known as dynamic. It represents the theoretical minimum efficiency of a heat engine operating between any two given thermal or heat reservoirs at different temperatures.
 D. The Carnot engine is a theoretical engine that operates in the limiting mode of extreme slowness known as quasi-static. It represents the theoretical minimum efficiency of a heat engine operating between any two given thermal or heat reservoirs at different temperatures.
 E. The Carnot engine is a real engine that operates in the limiting mode of extreme slowness known as quasi-static. It represents the theoretical maximum efficiency of a heat engine operating between any two given thermal or heat reservoirs at different temperatures. "
Which mathematical function is commonly used to characterize linear time-invariant systems?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: Which mathematical function is commonly used to characterize linear time-invariant systems?
 Context: -A continuous time-invariant linear state-space model is observable if and only if rank ⁡[CCA⋮CAn−1]=n.
Transfer function The ""transfer function"" of a continuous time-invariant linear state-space model can be derived in the following way: First, taking the Laplace transform of  x˙(t)=Ax(t)+Bu(t) yields sX(s)−x(0)=AX(s)+BU(s).
Next, we simplify for  X(s) , giving (sI−A)X(s)=x(0)+BU(s) and thus X(s)=(sI−A)−1x(0)+(sI−A)−1BU(s).
Substituting for  X(s) in the output equation Y(s)=CX(s)+DU(s), giving Y(s)=C((sI−A)−1x(0)+(sI−A)−1BU(s))+DU(s).
-Mason introduced both nonlinear and linear flow graphs. To clarify this point, Mason wrote : ""A linear flow graph is one whose associated equations are linear."" Examples of nonlinear branch functions It we denote by xj the signal at node j, the following are examples of node functions that do not pertain to a linear time-invariant system: log ,where  represents time Examples of nonlinear signal-flow graph models Although they generally can't be transformed between time domain and frequency domain representations for classical control theory analysis, nonlinear signal-flow graphs can be found in electrical engineering literature.
-Generalizing resonance and antiresonance for linear systems Next consider an arbitrary linear system with multiple inputs and outputs. For example, in state-space representation a third order linear time-invariant system with three inputs and two outputs might be written as where ui(t) are the inputs, xi(t) are the state variables, yi(t) are the outputs, and A, B, C, and D are matrices describing the dynamics between the variables.
-If we assume the controller C, the plant P, and the sensor F are linear and time-invariant (i.e., elements of their transfer function C(s), P(s), and F(s) do not depend on time), the systems above can be analysed using the Laplace transform on the variables. This gives the following relations: Y(s)=P(s)U(s) U(s)=C(s)E(s) E(s)=R(s)−F(s)Y(s).
Solving for Y(s) in terms of R(s) gives Y(s)=(P(s)C(s)1+F(s)P(s)C(s))R(s)=H(s)R(s).
-So  zn is an eigenfunction of an LTI system because the system response is the same as the input times the constant  H(z) Z and discrete-time Fourier transforms The eigenfunction property of exponentials is very useful for both analysis and insight into LTI systems. The Z transform is exactly the way to get the eigenvalues from the impulse response. Of particular interest are pure sinusoids; i.e. exponentials of the form  ejωn , where  ω∈R . These can also be written as  zn with  z=ejω . The discrete-time Fourier transform (DTFT)  H(ejω)=F{h[n]} gives the eigenvalues of pure sinusoids. Both of  H(z) and  H(ejω) are called the system function, system response, or transfer function.
 A. Trigonometric function
 B. Quadratic function
 C. Exponential function
 D. Logarithmic function
 E. Transfer function "
What is the second law of thermodynamics?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the second law of thermodynamics?
 Context: -The second law of thermodynamics is a physical law based on universal experience concerning heat and energy interconversions. One simple statement of the law is that heat always moves from hotter objects to colder objects (or ""downhill""), unless energy in some form is supplied to reverse the direction of heat flow. Another definition is: ""Not all heat energy can be converted into work in a cyclic process.""The second law of thermodynamics in other versions establishes the concept of entropy as a physical property of a thermodynamic system. It can be used to predict whether processes are forbidden despite obeying the requirement of conservation of energy as expressed in the first law of thermodynamics and provides necessary criteria for spontaneous processes. The second law may be formulated by the observation that the entropy of isolated systems left to spontaneous evolution cannot decrease, as they always arrive at a state of thermodynamic equilibrium where the entropy is highest at the given internal energy. An increase in the combined entropy of system and surroundings accounts for the irreversibility of natural processes, often referred to in the concept of the arrow of time.Historically, the second law was an empirical finding that was accepted as an axiom of thermodynamic theory. Statistical mechanics provides a microscopic explanation of the law in terms of probability distributions of the states of large assemblies of atoms or molecules. The second law has been expressed in many ways. Its first formulation, which preceded the proper definition of entropy and was based on caloric theory, is Carnot's theorem, formulated by the French scientist Sadi Carnot, who in 1824 showed that the efficiency of conversion of heat to work in a heat engine has an upper limit. The first rigorous definition of the second law based on the concept of entropy came from German scientist Rudolf Clausius in the 1850s and included his statement that heat can never pass from a colder to a warmer body without some other change, connected therewith, occurring at the same time.
-The first law of thermodynamics states that, when energy passes into or out of a system (as work, heat, or matter), the system's internal energy changes in accordance with the law of conservation of energy.
The second law of thermodynamics states that in a natural thermodynamic process, the sum of the entropies of the interacting thermodynamic systems never decreases. A common corollary of the statement is that heat does not spontaneously pass from a colder body to a warmer body.
-Second law of thermodynamics As an alternative to considering or defining the zeroth law of thermodynamics, it was the historical development in thermodynamics to define temperature in terms of the second law of thermodynamics which deals with entropy. The second law states that any process will result in either no change or a net increase in the entropy of the universe. This can be understood in terms of probability.
-Second law of thermodynamics The second law of thermodynamics conceptualizes that the entropy of a closed system can never decrease. As the law relates to power plants, it dictates that heat is to flow from a body at high temperature to a body at low temperature (the device in which electricity is being generated). This law is particularly pertinent to thermal power plants which derive their energy from the combustion of a fuel source.
-Second law A traditional version of the second law of thermodynamics states: Heat does not spontaneously flow from a colder body to a hotter body.
 A. The second law of thermodynamics is a physical law based on universal experience concerning heat and energy interconversions. It states that heat always moves from colder objects to hotter objects unless energy in some form is supplied to reverse the direction of heat flow.
 B. The second law of thermodynamics is a physical law based on universal experience concerning heat and energy interconversions. It establishes that the internal energy of a thermodynamic system is a physical property that can be used to predict whether processes are forbidden despite obeying the requirement of conservation of energy as expressed in the first law of thermodynamics.
 C. The second law of thermodynamics is a physical law based on universal experience concerning heat and energy interconversions. It establishes that all heat energy can be converted into work in a cyclic process.
 D. The second law of thermodynamics is a physical law based on universal experience concerning heat and energy interconversions. It states that the entropy of isolated systems left to spontaneous evolution can decrease, as they always arrive at a state of thermodynamic equilibrium where the entropy is highest at the given internal energy.
 E. The second law of thermodynamics is a physical law based on universal experience concerning heat and energy interconversions. It establishes the concept of entropy as a physical property of a thermodynamic system and can be used to predict whether processes are forbidden despite obeying the requirement of conservation of energy as expressed in the first law of thermodynamics. "
"What are amorphous ferromagnetic metallic alloys, and what are their advantages?","Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What are amorphous ferromagnetic metallic alloys, and what are their advantages?
 Context: -Amorphous (non-crystalline) ferromagnetic metallic alloys can be made by very rapid quenching (cooling) of an alloy. These have the advantage that their properties are nearly isotropic (not aligned along a crystal axis); this results in low coercivity, low hysteresis loss, high permeability, and high electrical resistivity. One such typical material is a transition metal-metalloid alloy, made from about 80% transition metal (usually Fe, Co, or Ni) and a metalloid component (B, C, Si, P, or Al) that lowers the melting point.
-Amorphous metal is usually an alloy rather than a pure metal. The alloys contain atoms of significantly different sizes, leading to low free volume (and therefore up to orders of magnitude higher viscosity than other metals and alloys) in molten state. The viscosity prevents the atoms moving enough to form an ordered lattice. The material structure also results in low shrinkage during cooling, and resistance to plastic deformation. The absence of grain boundaries, the weak spots of crystalline materials, leads to better resistance to wear and corrosion. Amorphous metals, while technically glasses, are also much tougher and less brittle than oxide glasses and ceramics. Amorphous metals can be grouped in two categories, as either non-ferromagnetic, if they are composed of Ln, Mg, Zr, Ti, Pd, Ca, Cu, Pt and Au, or ferromagnetic alloys, if they are composed of Fe, Co, and Ni.Thermal conductivity of amorphous materials is lower than that of crystalline metal. As formation of amorphous structure relies on fast cooling, this limits the maximum achievable thickness of amorphous structures. To achieve formation of amorphous structure even during slower cooling, the alloy has to be made of three or more components, leading to complex crystal units with higher potential energy and lower chance of formation. The atomic radius of the components has to be significantly different (over 12%), to achieve high packing density and low free volume. The combination of components should have negative heat of mixing, inhibiting crystal nucleation and prolonging the time the molten metal stays in supercooled state.
-Electric and Magnetic Properties The amorphous material produced by melt spinning is considered a soft magnet. That is to say that their natural coercivity is less than 1000 Am-1, which means that the metal's magnetism is more responsive to outside influences and as a result can be easily switched on and off. This makes amorphous metals particularly useful in applications requiring the repeated magnetization and demagnetization of a material in order to function. Certain amorphous alloys also provide the ability to enhance and or channel flux created by electrical currents, making them useful for magnetic shielding and insulation.
-Amorphous steel This material is a metallic glass prepared by pouring molten alloy onto a rotating cooled wheel, which cools the metal at a rate of about one megakelvin per second, so fast that crystals do not form. Amorphous steel is limited to foils of about 50 µm thickness. The mechanical properties of amorphous steel make stamping laminations for electric motors difficult. Since amorphous ribbon can be cast to any specific width under roughly 13 inches and can be sheared with relative ease, it is a suitable material for wound electrical transformer cores. In 2019 the price of amorphous steel outside the US is approximately $.95/pound compared to HiB grain-oriented steel which costs approximately $.86/pound. Transformers with amorphous steel cores can have core losses of one-third that of conventional electrical steels.
-Vitreous metal Amorphous metal is a variety of alloys (e.g. Metglas) that are non-crystalline or glassy. These are being used to create high-efficiency transformers. The materials can be highly responsive to magnetic fields for low hysteresis losses, and they can also have lower conductivity to reduce eddy current losses. Power utilities are currently making widespread use of these transformers for new installations. High mechanical strength and corrosion resistance are also common properties of metallic glasses which are positive for this application.
 A. Amorphous ferromagnetic metallic alloys are crystalline alloys that can be made by rapidly cooling a liquid alloy. Their properties are nearly anisotropic, resulting in low coercivity, low hysteresis loss, high permeability, and high electrical resistivity.
 B. Amorphous ferromagnetic metallic alloys are non-crystalline alloys that can be made by slowly heating a solid alloy. Their properties are nearly isotropic, resulting in low coercivity, low hysteresis loss, high permeability, and high electrical resistivity.
 C. Amorphous ferromagnetic metallic alloys are crystalline alloys that can be made by slowly cooling a liquid alloy. Their properties are nearly anisotropic, resulting in high coercivity, high hysteresis loss, low permeability, and low electrical resistivity.
 D. Amorphous ferromagnetic metallic alloys are non-crystalline alloys that can be made by rapidly cooling a liquid alloy. Their properties are nearly isotropic, resulting in low coercivity, low hysteresis loss, high permeability, and high electrical resistivity.
 E. Amorphous ferromagnetic metallic alloys are non-crystalline alloys that can be made by rapidly heating a solid alloy. Their properties are nearly isotropic, resulting in high coercivity, high hysteresis loss, low permeability, and low electrical resistivity. "
What is the Penrose process?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the Penrose process?
 Context: -The Penrose process (also called Penrose mechanism) is theorised by Sir Roger Penrose as a means whereby energy can be extracted from a rotating black hole. The process takes advantage of the ergosphere – a region of spacetime around the black hole dragged by its rotation faster than the speed of light, meaning that from the point of an outside observer any matter inside is forced to move in the direction of the rotation of the black hole.
-This process of removing energy from a rotating black hole was proposed by the mathematician Roger Penrose in 1969 and is called the Penrose process. The maximal amount of energy gain possible for a single particle via this process is 20.7% in terms of its mass equivalence, and if this process is repeated by the same mass, the theoretical maximal energy gain approaches 29% of its original mass-energy equivalent. As this energy is removed, the black hole loses angular momentum, and thus the limit of zero rotation is approached as spacetime dragging is reduced. In the limit, the ergosphere no longer exists. This process is considered a possible explanation for a source of energy of such energetic phenomena as gamma-ray bursts. Results from computer models show that the Penrose process is capable of producing the high-energy particles that are observed being emitted from quasars and other active galactic nuclei.
-The region outside the event horizon but inside the surface where the rotational velocity is the speed of light, is called the ergosphere (from Greek ergon meaning work). Particles falling within the ergosphere are forced to rotate faster and thereby gain energy. Because they are still outside the event horizon, they may escape the black hole. The net process is that the rotating black hole emits energetic particles at the cost of its own total energy. The possibility of extracting spin energy from a rotating black hole was first proposed by the mathematician Roger Penrose in 1969 and is thus called the Penrose process. Rotating black holes in astrophysics are a potential source of large amounts of energy and are used to explain energetic phenomena, such as gamma-ray bursts.
-Ergosphere Rotating black holes are surrounded by a region of spacetime in which it is impossible to stand still, called the ergosphere. This is the result of a process known as frame-dragging; general relativity predicts that any rotating mass will tend to slightly ""drag"" along the spacetime immediately surrounding it. Any object near the rotating mass will tend to start moving in the direction of rotation. For a rotating black hole, this effect is so strong near the event horizon that an object would have to move faster than the speed of light in the opposite direction to just stand still.The ergosphere of a black hole is a volume bounded by the black hole's event horizon and the ergosurface, which coincides with the event horizon at the poles but is at a much greater distance around the equator.Objects and radiation can escape normally from the ergosphere. Through the Penrose process, objects can emerge from the ergosphere with more energy than they entered with. The extra energy is taken from the rotational energy of the black hole. Thereby the rotation of the black hole slows down. A variation of the Penrose process in the presence of strong magnetic fields, the Blandford–Znajek process is considered a likely mechanism for the enormous luminosity and relativistic jets of quasars and other active galactic nuclei.
-A rotating black hole can produce large amounts of energy at the expense of its rotational energy. This happens through the Penrose process in the black hole's ergosphere, an area just outside its event horizon. In that case a rotating black hole gradually reduces to a Schwarzschild black hole, the minimum configuration from which no further energy can be extracted, although the Kerr black hole's rotation velocity will never quite reach zero.
 A. The Penrose process is a mechanism through which objects can emerge from the ergosphere with less energy than they entered with, taking energy from the rotational energy of the black hole and speeding up its rotation.
 B. The Penrose process is a mechanism through which objects can emerge from the ergosphere with the same energy as they entered with, taking energy from the rotational energy of the black hole and maintaining its rotation.
 C. The Penrose process is a mechanism through which objects can emerge from the ergosphere with more energy than they entered with, taking extra energy from the rotational energy of the black hole and slowing down its rotation.
 D. The Penrose process is a mechanism through which objects can emerge from the event horizon with less energy than they entered with, taking energy from the rotational energy of the black hole and speeding up its rotation.
 E. The Penrose process is a mechanism through which objects can emerge from the event horizon with more energy than they entered with, taking extra energy from the rotational energy of the black hole and slowing down its rotation. "
What was the aim of the Gravity Probe B (GP-B) mission?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What was the aim of the Gravity Probe B (GP-B) mission?
 Context: -Gravitomagnetism The existence of gravitomagnetism was proven by Gravity Probe B (GP-B), a satellite-based mission which launched on 20 April 2004. The spaceflight phase lasted until 2005. The mission aim was to measure spacetime curvature near Earth, with particular emphasis on gravitomagnetism.
-The geodetic effect (also known as geodetic precession, de Sitter precession or de Sitter effect) represents the effect of the curvature of spacetime, predicted by general relativity, on a vector carried along with an orbiting body. For example, the vector could be the angular momentum of a gyroscope orbiting the Earth, as carried out by the Gravity Probe B experiment. The geodetic effect was first predicted by Willem de Sitter in 1916, who provided relativistic corrections to the Earth–Moon system's motion. De Sitter's work was extended in 1918 by Jan Schouten and in 1920 by Adriaan Fokker. It can also be applied to a particular secular precession of astronomical orbits, equivalent to the rotation of the Laplace–Runge–Lenz vector.The term geodetic effect has two slightly different meanings as the moving body may be spinning or non-spinning. Non-spinning bodies move in geodesics, whereas spinning bodies move in slightly different orbits.The difference between de Sitter precession and Lense–Thirring precession (frame dragging) is that the de Sitter effect is due simply to the presence of a central mass, whereas Lense–Thirring precession is due to the rotation of the central mass. The total precession is calculated by combining the de Sitter precession with the Lense–Thirring precession.
-General relativityGeneral relativity is governed by the Einstein field equations, which describe the curvature of space-time due to mass–energy equivalent to the gravitational field. Solving the equation for the geometry of space warped due to the mass distribution gives the metric tensor. Using the geodesic equation, the motion of masses falling along the geodesics can be calculated.
GravitomagnetismIn a relatively flat spacetime due to weak gravitational fields, gravitational analogues of Maxwell's equations can be found; the GEM equations, to describe an analogous gravitomagnetic field. They are well established by the theory, and experimental tests form ongoing research.
Classical laws Kepler's Laws, though originally discovered from planetary observations (also due to Tycho Brahe), are true for any central forces.
-Also with implications for the physical exploration of the Earth's interior is the gravitational field, which is the net effect of gravitation (due to mass attraction) and centrifugal force (due to rotation). It can be measured very accurately at the surface and remotely by satellites. True vertical generally does not correspond to theoretical vertical (deflection ranges up to 50"") because topography and all geological masses disturb the gravitational field. Therefore, the gross structure of the Earth's crust and mantle can be determined by geodetic-geophysical models of the subsurface.
-The simplest (and historically the first) way of defining an asymptotically flat spacetime assumes that we have a coordinate chart, with coordinates  t,x,y,z , which far from the origin behaves much like a Cartesian chart on Minkowski spacetime, in the following sense. Write the metric tensor as the sum of a (physically unobservable) Minkowski background plus a perturbation tensor,  gab=ηab+hab , and set  r2=x2+y2+z2 . Then we require: lim r→∞hab=O(1/r) lim r→∞hab,p=O(1/r2) lim r→∞hab,pq=O(1/r3) One reason why we require the partial derivatives of the perturbation to decay so quickly is that these conditions turn out to imply that the gravitational field energy density (to the extent that this somewhat nebulous notion makes sense in a metric theory of gravitation) decays like  O(1/r4) , which would be physically sensible. (In classical electromagnetism, the energy of the electromagnetic field of a point charge decays like  O(1/r4) .) 
 A. To prove that pressure contributes equally to spacetime curvature as does mass-energy.
 B. To measure spacetime curvature near Earth, with particular emphasis on gravitomagnetism.
 C. To measure the distribution of Fe and Al on the Moon's surface.
 D. To confirm the relatively large geodetic effect due to simple spacetime curvature, and is also known as de Sitter precession.
 E. To measure the discrepancy between active and passive mass to about 10−12. "
What was Pierre de Fermat's solution to the problem of refraction?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What was Pierre de Fermat's solution to the problem of refraction?
 Context: -Fermat vs. the Cartesians In 1657, Pierre de Fermat received from Marin Cureau de la Chambre a copy of newly published treatise, in which La Chambre noted Hero's principle and complained that it did not work for refraction.Fermat replied that refraction might be brought into the same framework by supposing that light took the path of least resistance, and that different media offered different resistances. His eventual solution, described in a letter to La Chambre dated 1 January 1662, construed ""resistance"" as inversely proportional to speed, so that light took the path of least time. That premise yielded the ordinary law of refraction, provided that light traveled more slowly in the optically denser medium.Fermat's solution was a landmark in that it unified the then-known laws of geometrical optics under a variational principle or action principle, setting the precedent for the principle of least action in classical mechanics and the corresponding principles in other fields (see History of variational principles in physics). It was the more notable because it used the method of adequality, which may be understood in retrospect as finding the point where the slope of an infinitesimally short chord is zero, without the intermediate step of finding a general expression for the slope (the derivative).
-It was also immediately controversial. The ordinary law of refraction was at that time attributed to René Descartes (d. 1650), who had tried to explain it by supposing that light was a force that propagated instantaneously, or that light was analogous to a tennis ball that traveled faster in the denser medium, either premise being inconsistent with Fermat's. Descartes' most prominent defender, Claude Clerselier, criticized Fermat for apparently ascribing knowledge and intent to nature, and for failing to explain why nature should prefer to economize on time rather than distance. Clerselier wrote in part: 1. The principle that you take as the basis of your demonstration, namely that nature always acts in the shortest and simplest ways, is merely a moral principle and not a physical one; it is not, and cannot be, the cause of any effect in nature.... For otherwise we would attribute knowledge to nature; but here, by ""nature"", we understand only this order and this law established in the world as it is, which acts without foresight, without choice, and by a necessary determination.
-Optics The earlier ideas of variational principles in optics were generalized to refraction by Pierre de Fermat, who, in the 17th century, refined the principle to ""light travels between two given points along the path of shortest time""; now known as the principle of least time or Fermat's principle.
-If a ray follows a straight line, it obviously takes the path of least length. Hero of Alexandria, in his Catoptrics (1st century CE), showed that the ordinary law of reflection off a plane surface follows from the premise that the total length of the ray path is a minimum. Ibn al-Haytham, an 11th century polymaths later extended this principle to refraction, hence giving an early version of the Fermat's principle.
-The laws of reflection and refraction can be derived from Fermat's principle which states that the path taken between two points by a ray of light is the path that can be traversed in the least time.
 A. Fermat supposed that light took the path of least resistance, and that different media offered the same resistance. His eventual solution, described in a letter to La Chambre dated 1 January 1662, construed ""resistance"" as inversely proportional to speed, so that light took the path of least time. That premise yielded the ordinary law of refraction, provided that light traveled more slowly in the optically denser medium.
 B. Fermat supposed that light took the path of least resistance, and that different media offered different resistances. His eventual solution, described in a letter to La Chambre dated 1 January 1662, construed ""resistance"" as directly proportional to speed, so that light took the path of least time. That premise yielded the ordinary law of refraction, provided that light traveled more quickly in the optically denser medium.
 C. Fermat supposed that light took the path of least resistance, and that different media offered the same resistance. His eventual solution, described in a letter to La Chambre dated 1 January 1662, construed ""resistance"" as directly proportional to speed, so that light took the path of least time. That premise yielded the ordinary law of refraction, provided that light traveled more slowly in the optically denser medium.
 D. Fermat supposed that light took the path of least resistance, and that different media offered the same resistance. His eventual solution, described in a letter to La Chambre dated 1 January 1662, construed ""resistance"" as inversely proportional to speed, so that light took the path of least time. That premise yielded the ordinary law of refraction, provided that light traveled more quickly in the optically denser medium.
 E. Fermat supposed that light took the path of least resistance, and that different media offered different resistances. His eventual solution, described in a letter to La Chambre dated 1 January 1662, construed ""resistance"" as inversely proportional to speed, so that light took the path of least time. That premise yielded the ordinary law of refraction, provided that light traveled more slowly in the optically denser medium. "
What is the reason behind the adoption of a logarithmic scale of 5√100 ≈ 2.512 between magnitudes in astronomy?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the reason behind the adoption of a logarithmic scale of 5√100 ≈ 2.512 between magnitudes in astronomy?
 Context: -Thus in 1856 Norman Pogson of Oxford proposed that a logarithmic scale of 5√100 ≈ 2.512 be adopted between magnitudes, so five magnitude steps corresponded precisely to a factor of 100 in brightness. Every interval of one magnitude equates to a variation in brightness of 5√100 or roughly 2.512 times. Consequently, a magnitude 1 star is about 2.5 times brighter than a magnitude 2 star, about 2.52 times brighter than a magnitude 3 star, about 2.53 times brighter than a magnitude 4 star, and so on.
-Both the apparent and absolute magnitude scales are logarithmic units: one whole number difference in magnitude is equal to a brightness variation of about 2.5 times (the 5th root of 100 or approximately 2.512). This means that a first magnitude star (+1.00) is about 2.5 times brighter than a second magnitude (+2.00) star, and about 100 times brighter than a sixth magnitude star (+6.00). The faintest stars visible to the naked eye under good seeing conditions are about magnitude +6.On both apparent and absolute magnitude scales, the smaller the magnitude number, the brighter the star; the larger the magnitude number, the fainter the star. The brightest stars, on either scale, have negative magnitude numbers. The variation in brightness (ΔL) between two stars is calculated by subtracting the magnitude number of the brighter star (mb) from the magnitude number of the fainter star (mf), then using the difference as an exponent for the base number 2.512; that is to say: Δm=mf−mb 2.512 Δm=ΔL Relative to both luminosity and distance from Earth, a star's absolute magnitude (M) and apparent magnitude (m) are not equivalent; for example, the bright star Sirius has an apparent magnitude of −1.44, but it has an absolute magnitude of +1.41.
-Magnitude values do not have a unit. The scale is logarithmic and defined such that a magnitude 1 star is exactly 100 times brighter than a magnitude 6 star. Thus each step of one magnitude is  100 2.512 times brighter than the magnitude 1 higher. The brighter an object appears, the lower the value of its magnitude, with the brightest objects reaching negative values.
-The scale is reverse logarithmic: the brighter an object is, the lower its magnitude number. A difference of 1.0 in magnitude corresponds to a brightness ratio of  100 5 , or about 2.512. For example, a star of magnitude 2.0 is 2.512 times as bright as a star of magnitude 3.0, 6.31 times as bright as a star of magnitude 4.0, and 100 times as bright as one of magnitude 7.0.
-The magnitude scale is a reverse logarithmic scale. A common misconception is that the logarithmic nature of the scale is because the human eye itself has a logarithmic response. In Pogson's time this was thought to be true (see Weber–Fechner law), but it is now believed that the response is a power law (see Stevens' power law).Magnitude is complicated by the fact that light is not monochromatic. The sensitivity of a light detector varies according to the wavelength of the light, and the way it varies depends on the type of light detector. For this reason, it is necessary to specify how the magnitude is measured for the value to be meaningful. For this purpose the UBV system is widely used, in which the magnitude is measured in three different wavelength bands: U (centred at about 350 nm, in the near ultraviolet), B (about 435 nm, in the blue region) and V (about 555 nm, in the middle of the human visual range in daylight). The V band was chosen for spectral purposes and gives magnitudes closely corresponding to those seen by the human eye. When an apparent magnitude is discussed without further qualification, the V magnitude is generally understood.Because cooler stars, such as red giants and red dwarfs, emit little energy in the blue and UV regions of the spectrum, their power is often under-represented by the UBV scale. Indeed, some L and T class stars have an estimated magnitude of well over 100, because they emit extremely little visible light, but are strongest in infrared.Measures of magnitude need cautious treatment and it is extremely important to measure like with like. On early 20th century and older orthochromatic (blue-sensitive) photographic film, the relative brightnesses of the blue supergiant Rigel and the red supergiant Betelgeuse irregular variable star (at maximum) are reversed compared to what human eyes perceive, because this archaic film is more sensitive to blue light than it is to red light. Magnitudes obtained from this method are known as photographic magnitudes, and are now considered obsolete.For objects within the Milky Way with a given absolute magnitude, 5 is added to the apparent magnitude for every tenfold increase in the distance to the object. For objects at very great distances (far beyond the Milky Way), this relationship must be adjusted for redshifts and for non-Euclidean distance measures due to general relativity.For planets and other Solar System bodies, the apparent magnitude is derived from its phase curve and the distances to the Sun and observer.
 A. The logarithmic scale was adopted to ensure that five magnitude steps corresponded precisely to a factor of 100 in brightness.
 B. The logarithmic scale was adopted to measure the size of stars.
 C. The logarithmic scale was adopted to measure the intensity of light coming from a star.
 D. The logarithmic scale was adopted to ensure that the apparent sizes of stars were not spurious.
 E. The logarithmic scale was adopted to measure the distance between stars. "
What is the spin quantum number?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the spin quantum number?
 Context: -In physics, the spin quantum number is a quantum number (designated s) that describes the intrinsic angular momentum (or spin angular momentum, or simply spin) of an electron or other particle. It has the same value for all particles of the same type, such as s = 1/2 for all electrons. It is an integer for all bosons, such as photons, and a half-odd-integer for all fermions, such as electrons and protons. The component of the spin along a specified axis is given by the spin magnetic quantum number, conventionally written ms. The value of ms is the component of spin angular momentum, in units of the reduced Planck constant ħ, parallel to a given direction (conventionally labelled the z–axis). It can take values ranging from +s to −s in integer increments. For an electron, ms can be either ++1/2 or −+1/2 .
-Spin quantum number The spin quantum number describes the intrinsic spin angular momentum of the electron within each orbital and gives the projection of the spin angular momentum S along the specified axis: Sz = ms ħ.In general, the values of ms range from −s to s, where s is the spin quantum number, associated with the particle's intrinsic spin angular momentum: ms = −s, −s + 1, −s + 2, ..., s − 2, s − 1, s.An electron has spin number s = 1/2, consequently ms will be ±1/2, referring to ""spin up"" and ""spin down"" states. Each electron in any individual orbital must have different quantum numbers because of the Pauli exclusion principle, therefore an orbital never contains more than two electrons.
-The phrase spin quantum number was originally used to describe the fourth of a set of quantum numbers (the principal quantum number n, the azimuthal quantum number ℓ, the magnetic quantum number m, and the spin magnetic quantum number ms), which completely describe the quantum state of an electron in an atom.  Some introductory chemistry textbooks describe ms as the spin quantum number, and s is not mentioned since its value 1/2 is a fixed property of the electron, sometimes using the variable s in place of ms. Some authors discourage this usage as it causes confusion. At a more advanced level where quantum mechanical operators or coupled spins are introduced, s is referred to as the spin quantum number, and ms is described as the spin magnetic quantum number or as the z-component of spin sz.Spin quantum numbers apply also to systems of coupled spins, such as atoms that may contain more than one electron. Capitalized symbols are used: S for the total electronic spin, and mS or MS for the z-axis component. A pair of electrons in a spin singlet state has S = 0, and a pair in the triplet state has S = 1, with mS = −1, 0, or +1. Nuclear-spin quantum numbers are conventionally written I for spin, and mI or MI for the z-axis component.
-In atomic physics, a magnetic quantum number is a quantum number used to distinguish quantum states of an electron or other particle according to its angular momentum along a given axis in space. The orbital magnetic quantum number (ml or m) distinguishes the orbitals available within a given subshell of an atom. It specifies the component of the orbital angular momentum that lies along a given axis, conventionally called the z-axis, so it describes the orientation of the orbital in space. The spin magnetic quantum number ms specifies the z-axis component of the spin angular momentum for a particle having spin quantum number s. For an electron, s is 1⁄2, and ms is either +1⁄2 or −1⁄2, often called ""spin-up"" and ""spin-down"", or α and β. The term magnetic in the name refers to the magnetic dipole moment associated with each type of angular momentum, so states having different magnetic quantum numbers shift in energy in a magnetic field according to the Zeeman effect.The four quantum numbers conventionally used to describe the quantum state of an electron in an atom are the principal quantum number n, the azimuthal (orbital) quantum number  ℓ , and the magnetic quantum numbers ml and ms. Electrons in a given subshell of an atom (such as s, p, d, or f) are defined by values of  ℓ (0, 1, 2, or 3). The orbital magnetic quantum number takes integer values in the range from  −ℓ to  +ℓ , including zero. Thus the s, p, d, and f subshells contain 1, 3, 5, and 7 orbitals each, with values of ml within the ranges 0, ±1, ±2, ±3 respectively. Each of these orbitals can accommodate up to two electrons (with opposite spins), forming the basis of the periodic table.
-In quantum mechanics, spin is an intrinsic property of all elementary particles. All known fermions, the particles that constitute ordinary matter, have a spin of 1/2. The spin number describes how many symmetrical facets a particle has in one full rotation; a spin of 1/2 means that the particle must be rotated by two full turns (through 720°) before it has the same configuration as when it started.
 A. The spin quantum number is a measure of the distance between an elementary particle and the nucleus of an atom.
 B. The spin quantum number is a measure of the size of an elementary particle.
 C. The spin quantum number is a measure of the charge of an elementary particle.
 D. The spin quantum number is a measure of the speed of an elementary particle's rotation around some axis.
 E. The spin quantum number is a dimensionless quantity obtained by dividing the spin angular momentum by the reduced Planck constant ħ, which has the same dimensions as angular momentum. "
What is the synapstor or synapse transistor?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the synapstor or synapse transistor?
 Context: -In July 2008, Erokhin and Fontana claimed to have developed a polymeric memristor before the more recently announced titanium dioxide memristor.In 2010, Alibart, Gamrat, Vuillaume et al. introduced a new hybrid organic/nanoparticle device (the NOMFET : Nanoparticle Organic Memory Field Effect Transistor), which behaves as a memristor and which exhibits the main behavior of a biological spiking synapse. This device, also called a synapstor (synapse transistor), was used to demonstrate a neuro-inspired circuit (associative memory showing a pavlovian learning).In 2012, Crupi, Pradhan and Tozer described a proof of concept design to create neural synaptic memory circuits using organic ion-based memristors. The synapse circuit demonstrated long-term potentiation for learning as well as inactivity based forgetting. Using a grid of circuits, a pattern of light was stored and later recalled. This mimics the behavior of the V1 neurons in the primary visual cortex that act as spatiotemporal filters that process visual signals such as edges and moving lines.
-The model for gated synapses was originally derived from the model electronic circuit, in which the gatekeeper serves as a transistor in a circuit. In a circuit, a transistor can act as a switch that turns an electrical signal on or off. In addition, a transistor can serve to amplify an existing current in a circuit. In effect, the gatekeeper neuron acts as the transistor of a gated synapse by modulating the transmission of the signal between the pre-synaptic and post-synaptic neurons.
-A synaptic transistor has a traditional immediate response whose amount of current that passes between the source and drain contacts varies with voltage applied to the gate electrode. It also produces a much slower learned response such that the conductivity of the SNO layer varies in response to the transistor's STDP history, essentially by shuttling oxygen ions between the SNO and the ionic liquid.The analog of strengthening a synapse is to increase the SNO's conductivity, which essentially increases gain. Similarly, weakening a synapse is analogous to decreasing the SNO's conductivity, lowering the gain.The input and output of the synaptic transistor are continuous analog values, rather than digital on-off signals. While the physical structure of the device has the potential to learn from history, it contains no way to bias the transistor to control the memory effect. An external supervisory circuit converts the time delay between input and output into a voltage applied to the ionic liquid that either drives ions into the SNO or removes them.A network of such devices can learn particular responses to ""sensory inputs"", with those responses being learned through experience rather than explicitly programmed.
-These types of devices would allow for a synapse model that could realise a learning rule, by which the synaptic efficacy is altered by voltages applied to the terminals of the device. An example of such a learning rule is spike-timing-dependant-plasticty by which the weight of the synapse, in this case the conductivity, could be modulated based on the timing of pre and post synaptic spikes arriving at each terminal. The advantage of this approach over two terminal memristive devices is that read and write protocols have the possibility to occur simultaneously and distinctly.
-Research in 2004 has shown that synapses do not strengthen or weaken on a sliding scale. There are discrete states that synapses move between. These states are active, silent, recently silent, potentiated, and depressed. The states which they can move to are dependent on the state that they are in at the moment. Thus, the future state is determined by the state gained by previous activity. For instance, silent (but not recently silent) synapses can be converted to active via the insertion of AMPARs in the postsynaptic membrane. Active synapses can move to either potentiated or depressed via LTP or LTD respectively. Prolonged low-frequency stimulation (5 Hz, the method used to induce LTD) can move an active synapse to depressed and then silent. However, synapses that have just become active cannot be depressed or silenced. Thus there is state-machine-like behavior at the synapse when it comes to transitions. However, the states themselves can have varying degrees of intensity. One active-state synapse can be stronger than another active-state synapse. This is, in theory, how you can have a strong memory vs. a weak memory. The strong memories are the ones with very heavily populated active synapses, while weak memories may still be active but poorly populated with AMPARs. The same research has shown that NMDA receptors themselves, once thought to be the control mechanism behind AMPA receptor organization, can be regulated by synaptic activity. This regulation of the regulation mechanism itself adds another layer of complexity to the biology of the brain.
 A. A device used to demonstrate a neuro-inspired circuit that shows short-term potentiation for learning and inactivity-based forgetting.
 B. A device used to demonstrate a neuro-inspired circuit that shows long-term potentiation for learning and activity-based forgetting.
 C. A device used to demonstrate a neuro-inspired circuit that shows short-term depression for learning and inactivity-based forgetting.
 D. A device used to demonstrate a neuro-inspired circuit that shows short-term potentiation for learning and activity-based forgetting.
 E. A device used to demonstrate a neuro-inspired circuit that shows long-term potentiation for learning and inactivity-based forgetting. "
What is spontaneous symmetry breaking?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is spontaneous symmetry breaking?
 Context: -Showing that a system admits spontaneous symmetry breaking requires introducing a weak external source field that breaks the symmetry and gives rise to a preferred ground state. The system is then taken to the thermodynamic limit after which the external source field is switched off. If the vacuum expectation value of symmetry non-invariant operators is nonzero in this limit then there is spontaneous symmetry breaking. Physically it means that the system never leaves the original ground state into which it was placed by the external field. For global symmetries this occurs because the energy barrier between the various ground states is proportional to the volume, so in the thermodynamic limit this diverges, locking the system into the ground state. Local symmetries get around this construction because the energy barrier between two ground states depends only on local features so transitions to different gauge related ground states can occur locally and does not require the field to change everywhere at the same time as it does for global symmetries.
-Spontaneous symmetry breaking is a spontaneous process of symmetry breaking, by which a physical system in a symmetric state spontaneously ends up in an asymmetric state. In particular, it can describe systems where the equations of motion or the Lagrangian obey symmetries, but the lowest-energy vacuum solutions do not exhibit that same symmetry. When the system goes to one of those vacuum solutions, the symmetry is broken for perturbations around that vacuum even though the entire Lagrangian retains that symmetry.
-A field theory admits numerous types of symmetries, with the two most common ones being global and local symmetries. Global symmetries are fields transformations acting the same way everywhere while local symmetries act on fields in a position dependent way. The latter correspond to redundancies in the description of the system. This is a consequence of Noether's second theorem which shows that each gauge symmetry degree of freedom corresponds to a relation among the Euler–Lagrange equations, making the system underdetermined. Underdeterminacy requires gauge fixing of the non-propagating components so that the equations of motion admits a unique solution.Spontaneous symmetry breaking occurs when the action of a theory has a symmetry but the vacuum state violates this symmetry. In that case there will exist a local operator that is non-invariant under the symmetry giving it a nonzero vacuum expectation value. Such non-invariant local operators always have vanishing vacuum expectation values for finite size systems prohibiting spontaneous symmetry breaking. This occurs because over large timescales, finite systems always transition between all its possible ground states, averaging away the expectation value to zero.While spontaneous symmetry breaking can occur for global symmetries, Elitzur's theorem states that the same is not the case for gauge symmetries; all vacuum expectation values of gauge non-invariant operators are vanishing, even in systems of infinite size. On the lattice this follows from the fact that integrating gauge non-invariant observables over a group measure always yields zero for compact gauge groups. Positivity of the measure and gauge invariance are sufficient to prove the theorem. This is also an explanation for why gauge symmetries are mere redundancies in lattice field theories, where the equations of motion need not define a well-posed problem as they do not need to be solved. Instead, Elitzur's theorem shows that any observable that is not invariant under the symmetry has a vanishing expectation value making it unobservable and thus redundant.
-Spontaneous symmetry breaking When the Hamiltonian of a system (or the Lagrangian) has a certain symmetry, but the vacuum does not, then one says that spontaneous symmetry breaking (SSB) has taken place.
-An interesting feature can occur if m2 turns negative, but with λ still positive. In this case, the vacuum consists of two lowest-energy states, each of which spontaneously breaks the Z2 global symmetry of the original theory. This leads to the appearance of interesting collective states like domain walls. In the O(2) theory, the vacua would lie on a circle, and the choice of one would spontaneously break the O(2) symmetry. A continuous broken symmetry leads to a Goldstone boson. This type of spontaneous symmetry breaking is the essential component of the Higgs mechanism.
 A. Spontaneous symmetry breaking occurs when the action of a theory has no symmetry, but the vacuum state has a symmetry. In that case, there will exist a local operator that is non-invariant under the symmetry, giving it a nonzero vacuum expectation value.
 B. Spontaneous symmetry breaking occurs when the action of a theory has a symmetry, and the vacuum state also has the same symmetry. In that case, there will exist a local operator that is invariant under the symmetry, giving it a zero vacuum expectation value.
 C. Spontaneous symmetry breaking occurs when the action of a theory has no symmetry, and the vacuum state also has no symmetry. In that case, there will exist a local operator that is invariant under the symmetry, giving it a zero vacuum expectation value.
 D. Spontaneous symmetry breaking occurs when the action of a theory has a symmetry, but the vacuum state violates this symmetry. In that case, there will exist a local operator that is invariant under the symmetry, giving it a zero vacuum expectation value.
 E. Spontaneous symmetry breaking occurs when the action of a theory has a symmetry, but the vacuum state violates this symmetry. In that case, there will exist a local operator that is non-invariant under the symmetry, giving it a nonzero vacuum expectation value. "
What is the proper distance for a redshift of 8.2?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the proper distance for a redshift of 8.2?
 Context: -The most distant astronomical object identified (as of 2022) is a galaxy classified as HD1, with a redshift of 13.27, corresponding to a distance of about 33.4 billion light years. In 2009, a gamma ray burst, GRB 090423, was found to have a redshift of 8.2, which indicates that the collapsing star that caused it exploded when the universe was only 630 million years old. The burst happened approximately 13 billion years ago, so a distance of about 13 billion light-years was widely quoted in the media (or sometimes a more precise figure of 13.035 billion light-years) - however, this would be the ""light travel distance"" (see Distance measures (cosmology)) rather than the ""proper distance"" used in both Hubble's law and in defining the size of the observable universe, and cosmologist Ned Wright argues against using this measure. The proper distance for a redshift of 8.2 would be about 9.2 Gpc, or about 30 billion light-years.
-The decoupling, or the last scattering, is thought to have occurred about 300,000 years after the Big Bang, or at a redshift of about  1100 . We can determine both the approximate angular diameter of the universe and the physical size of the particle horizon that had existed at this time.  The angular diameter distance, in terms of redshift  z , is described by  dA(z)=r(z)/(1+z) . If we assume a flat cosmology then,  r(z)=∫temt0dta(t)=∫aem1daa2H(a)=∫0zdzH(z).
-There are a few different definitions of ""distance"" in cosmology which are all asymptotic one to another for small redshifts. The expressions for these distances are most practical when written as functions of redshift  z , since redshift is always the observable. They can also be written as functions of scale factor  a=1/(1+z).
In the remainder of this article, the peculiar velocity is assumed to be negligible unless specified otherwise.
We first give formulas for several distance measures, and then describe them in more detail further down. Defining the ""Hubble distance"" as 3000 Mpc 9.26 10 25 h−1m where  c is the speed of light,  H0 is the Hubble parameter today, and h is the dimensionless Hubble constant, all the distances are asymptotic to  z⋅dH for small z.
-The proper distance—the distance as would be measured at a specific time, including the present—between Earth and the edge of the observable universe is 46 billion light-years (14 billion parsecs), making the diameter of the observable universe about 93 billion light-years (28 billion parsecs). The distance the light from the edge of the observable universe has travelled is very close to the age of the universe times the speed of light, 13.8 billion light-years (4.2×10^9 pc), but this does not represent the distance at any given time because the edge of the observable universe and the Earth have since moved further apart. For comparison, the diameter of a typical galaxy is 30,000 light-years (9,198 parsecs), and the typical distance between two neighboring galaxies is 3 million light-years (919.8 kiloparsecs). As an example, the Milky Way is roughly 100,000–180,000 light-years in diameter, and the nearest sister galaxy to the Milky Way, the Andromeda Galaxy, is located roughly 2.5 million light-years away.Because humans cannot observe space beyond the edge of the observable universe, it is unknown whether the size of the universe in its totality is finite or infinite. Estimates suggest that the whole universe, if finite, must be more than 250 times larger than a Hubble sphere. Some disputed estimates for the total size of the universe, if finite, reach as high as  10 10 10 122 megaparsecs, as implied by a suggested resolution of the No-Boundary Proposal.
-The ""distance"" of a far away galaxy depends on how it is measured. With a redshift of 1, light from this galaxy is estimated to have taken around 7.7 billion years to reach Earth. However, since this galaxy is receding from Earth, the present comoving distance is estimated to be around 10 billion light-years away. In context, Hubble is observing this galaxy as it appeared when the Universe was around 5.9 billion years old.
 A. The proper distance for a redshift of 8.2 is about 6.2 Gpc, or about 24 billion light-years.
 B. The proper distance for a redshift of 8.2 is about 7.2 Gpc, or about 26 billion light-years.
 C. The proper distance for a redshift of 8.2 is about 9.2 Gpc, or about 30 billion light-years.
 D. The proper distance for a redshift of 8.2 is about 8.2 Gpc, or about 28 billion light-years.
 E. The proper distance for a redshift of 8.2 is about 10.2 Gpc, or about 32 billion light-years. "
Who was the first to determine the velocity of a star moving away from the Earth using the Doppler effect?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: Who was the first to determine the velocity of a star moving away from the Earth using the Doppler effect?
 Context: -The history of the subject began with the development in the 19th century of classical wave mechanics and the exploration of phenomena associated with the Doppler effect. The effect is named after Christian Doppler, who offered the first known physical explanation for the phenomenon in 1842. The hypothesis was tested and confirmed for sound waves by the Dutch scientist Christophorus Buys Ballot in 1845. Doppler correctly predicted that the phenomenon should apply to all waves, and in particular suggested that the varying colors of stars could be attributed to their motion with respect to the Earth. Before this was verified, however, it was found that stellar colors were primarily due to a star's temperature, not motion. Only later was Doppler vindicated by verified redshift observations.The first Doppler redshift was described by French physicist Hippolyte Fizeau in 1848, who pointed to the shift in spectral lines seen in stars as being due to the Doppler effect. The effect is sometimes called the ""Doppler–Fizeau effect"". In 1868, British astronomer William Huggins was the first to determine the velocity of a star moving away from the Earth by this method. In 1871, optical redshift was confirmed when the phenomenon was observed in Fraunhofer lines using solar rotation, about 0.1 Å in the red. In 1887, Vogel and Scheiner discovered the annual Doppler effect, the yearly change in the Doppler shift of stars located near the ecliptic due to the orbital velocity of the Earth. In 1901, Aristarkh Belopolsky verified optical redshift in the laboratory using a system of rotating mirrors.Arthur Eddington used the term red shift as early as 1923. The word does not appear unhyphenated until about 1934 by Willem de Sitter.Beginning with observations in 1912, Vesto Slipher discovered that most spiral galaxies, then mostly thought to be spiral nebulae, had considerable redshifts. Slipher first reports on his measurement in the inaugural volume of the Lowell Observatory Bulletin. Three years later, he wrote a review in the journal Popular Astronomy. In it he states that ""the early discovery that the great Andromeda spiral had the quite exceptional velocity of –300 km(/s) showed the means then available, capable of investigating not only the spectra of the spirals but their velocities as well."" Slipher reported the velocities for 15 spiral nebulae spread across the entire celestial sphere, all but three having observable ""positive"" (that is recessional) velocities. Subsequently, Edwin Hubble discovered an approximate relationship between the redshifts of such ""nebulae"" and the distances to them with the formulation of his eponymous Hubble's law. These observations corroborated Alexander Friedmann's 1922 work, in which he derived the Friedmann–Lemaître equations. In the present day they are considered strong evidence for an expanding universe and the Big Bang theory.
-Doppler first proposed this effect in 1842 in his treatise ""Über das farbige Licht der Doppelsterne und einiger anderer Gestirne des Himmels"" (On the coloured light of the binary stars and some other stars of the heavens). The hypothesis was tested for sound waves by Buys Ballot in 1845. He confirmed that the sound's pitch was higher than the emitted frequency when the sound source approached him, and lower than the emitted frequency when the sound source receded from him. Hippolyte Fizeau discovered independently the same phenomenon on electromagnetic waves in 1848 (in France, the effect is sometimes called ""effet Doppler-Fizeau"" but that name was not adopted by the rest of the world as Fizeau's discovery was six years after Doppler's proposal). In Britain, John Scott Russell made an experimental study of the Doppler effect (1848).
-The first measurements of the speed of light using completely terrestrial apparatus were published in 1849 by Hippolyte Fizeau (1819–96). Compared to values accepted today, Fizeau's result (about 313,000 kilometres per second) was too high, and less accurate than those obtained by Rømer's method. It would be another thirty years before A. A. Michelson in the United States published his more precise results (299,910±50 km/s) and Simon Newcomb confirmed the agreement with astronomical measurements, almost exactly two centuries after Rømer's announcement.
-1832–1838 – Following over 100 years of unsuccessful attempts, Thomas Henderson, Friedrich Bessel, and Otto Struve measure the parallax of a few nearby stars; these are the first measurements of any distances outside the Solar System.1842 – Christian Doppler proposes the redshift and blueshift effects, based on an analog effect found in sound. Hippolyte Fizeau discovered independently the same phenomenon on electromagnetic waves in 1848.
-A typical method to determine proper motion is to measure the position of a star relative to a limited, selected set of very distant objects that exhibit no mutual movement, and that, because of their distance, are assumed to have very small proper motion. Another approach is to compare photographs of a star at different times against a large background of more distant objects. The star with the largest known proper motion is Barnard's Star.Radial velocity of stars, and other deep-space objects, can be revealed spectroscopically thru the Doppler-Fizeau effect, by which the frequency of the received light decreases for objects that were receding (redshift) and increases for objects that were approaching (blueshift), when compared to the light emitted by a stationary object. William Huggins ventured in 1868 to estimate the radial velocity of Sirius with respect to the Sun, based on observed redshift of the star's light.The phrase ""fixed star"" is technically incorrect, but nonetheless it is used in an historical context, and in classical mechanics. When used as a visual reference for observations, they usually are called background stars or simply distant stars, still retaining the intuitive meaning of they being ""fixed"" in some practical sense.
 A. Fraunhofer
 B. William Huggins
 C. Hippolyte Fizeau
 D. Vogel and Scheiner
 E. None of the above "
What is the information loss paradox in black holes?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the information loss paradox in black holes?
 Context: -Information loss paradox Because a black hole has only a few internal parameters, most of the information about the matter that went into forming the black hole is lost. Regardless of the type of matter which goes into a black hole, it appears that only information concerning the total mass, charge, and angular momentum are conserved. As long as black holes were thought to persist forever this information loss is not that problematic, as the information can be thought of as existing inside the black hole, inaccessible from the outside, but represented on the event horizon in accordance with the holographic principle. However, black holes slowly evaporate by emitting Hawking radiation. This radiation does not appear to carry any additional information about the matter that formed the black hole, meaning that this information appears to be gone forever.The question whether information is truly lost in black holes (the black hole information paradox) has divided the theoretical physics community. In quantum mechanics, loss of information corresponds to the violation of a property called unitarity, and it has been argued that loss of unitarity would also imply violation of conservation of energy, though this has also been disputed. Over recent years evidence has been building that indeed information and unitarity are preserved in a full quantum gravitational treatment of the problem.One attempt to resolve the black hole information paradox is known as black hole complementarity. In 2012, the ""firewall paradox"" was introduced with the goal of demonstrating that black hole complementarity fails to solve the information paradox. According to quantum field theory in curved spacetime, a single emission of Hawking radiation involves two mutually entangled particles. The outgoing particle escapes and is emitted as a quantum of Hawking radiation; the infalling particle is swallowed by the black hole. Assume a black hole formed a finite time in the past and will fully evaporate away in some finite time in the future. Then, it will emit only a finite amount of information encoded within its Hawking radiation. According to research by physicists like Don Page and Leonard Susskind, there will eventually be a time by which an outgoing particle must be entangled with all the Hawking radiation the black hole has previously emitted. This seemingly creates a paradox: a principle called ""monogamy of entanglement"" requires that, like any quantum system, the outgoing particle cannot be fully entangled with two other systems at the same time; yet here the outgoing particle appears to be entangled both with the infalling particle and, independently, with past Hawking radiation. In order to resolve this contradiction, physicists may eventually be forced to give up one of three time-tested principles: Einstein's equivalence principle, unitarity, or local quantum field theory. One possible solution, which violates the equivalence principle, is that a ""firewall"" destroys incoming particles at the event horizon. In general, which—if any—of these assumptions should be abandoned remains a topic of debate.
-The simplest models of black hole evaporation lead to the black hole information paradox. The information content of a black hole appears to be lost when it dissipates, as under these models the Hawking radiation is random (it has no relation to the original information). A number of solutions to this problem have been proposed, including suggestions that Hawking radiation is perturbed to contain the missing information, that the Hawking evaporation leaves some form of remnant particle containing the missing information, and that information is allowed to be lost under these conditions.
-The information paradox appears when one considers a process in which a black hole is formed through a physical process and then evaporates away entirely through Hawking radiation. Hawking's calculation suggests that the final state of radiation would retain information only about the total mass, electric charge and angular momentum of the initial state. Since many different states can have the same mass, charge and angular momentum, this suggests that many initial physical states could evolve into the same final state. Therefore, information about the details of the initial state would be permanently lost; however, this violates a core precept of both classical and quantum physics—that, in principle, the state of a system at one point in time should determine its value at any other time. Specifically, in quantum mechanics the state of the system is encoded by its wave function. The evolution of the wave function is determined by a unitary operator, and unitarity implies that the wave function at any instant of time can be used to determine the wave function either in the past or the future. In 1993, Don Page argued that if a black hole starts in a pure quantum state and evaporates completely by a unitary process, the von Neumann entropy of the Hawking radiation initially increases and then decreases back to zero when the black hole has disappeared. This is called the Page curve.It is now generally believed that information is preserved in black-hole evaporation. For many researchers, deriving the Page curve is synonymous with solving the black hole information puzzle.: 291  However, views differ as to how, precisely, Hawking's original semi-classical calculation should be corrected. In recent years, several extensions of the original paradox have been explored. Taken together these puzzles about black hole evaporation have implications for how gravity and quantum mechanics must be combined, leading to the information paradox remaining an active field of research within quantum gravity.
-Information is stored in a large remnantThis idea suggests that Hawking radiation stops before the black hole reaches the Planck size. Since the black hole never evaporates, information about its initial state can remain inside the black hole and the paradox disappears. However, there is no accepted mechanism that would allow Hawking radiation to stop while the black hole remains macroscopic.
-Information is irretrievably lost A minority view within the theoretical physics community is that information is genuinely lost when black holes form and evaporate.
This conclusion follows if one assumes that the predictions of semiclassical gravity and the causal structure of the black-hole spacetime are exact.
 A. Black holes have an infinite number of internal parameters, so all the information about the matter that went into forming the black hole is preserved. Regardless of the type of matter which goes into a black hole, it appears that all the information is conserved. As black holes evaporate by emitting Hawking radiation, the information is lost forever.
 B. Black holes have an infinite number of internal parameters, so all the information about the matter that went into forming the black hole is preserved. Regardless of the type of matter which goes into a black hole, it appears that all the information is conserved. As black holes evaporate by emitting Hawking radiation, the information is lost temporarily but reappears once the black hole has fully evaporated.
 C. Black holes have only a few internal parameters, so most of the information about the matter that went into forming the black hole is lost. Regardless of the type of matter which goes into a black hole, it appears that only information concerning the total mass, charge, and angular momentum are conserved. As black holes evaporate by emitting Hawking radiation, the information is lost forever.
 D. Black holes have only a few internal parameters, so most of the information about the matter that went into forming the black hole is lost. Regardless of the type of matter which goes into a black hole, it appears that only information concerning the total mass, charge, and angular momentum are conserved. As black holes evaporate by emitting Hawking radiation, the information is preserved and reappears once the black hole has fully evaporated.
 E. Black holes have only a few internal parameters, so most of the information about the matter that went into forming the black hole is preserved. Regardless of the type of matter which goes into a black hole, it appears that all the information is conserved. As black holes evaporate by emitting Hawking radiation, the information is preserved and reappears once the black hole has fully evaporated. "
What is the Kutta condition?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the Kutta condition?
 Context: -Kutta condition – is a principle in steady-flow fluid dynamics, especially aerodynamics, that is applicable to solid bodies with sharp corners, such as the trailing edges of airfoils. It is named for German mathematician and aerodynamicist Martin Kutta.Kuethe and Schetzer state the Kutta condition as follows:: § 4.11 A body with a sharp trailing edge which is moving through a fluid will create about itself a circulation of sufficient strength to hold the rear stagnation point at the trailing edge.In fluid flow around a body with a sharp corner, the Kutta condition refers to the flow pattern in which fluid approaches the corner from above and below, meets at the corner, and then flows away from the body. None of the fluid flows around the sharp corner.The Kutta condition is significant when using the Kutta–Joukowski theorem to calculate the lift created by an airfoil with a sharp trailing edge. The value of circulation of the flow around the airfoil must be that value that would cause the Kutta condition to exist.Kutta–Joukowski theorem – is a fundamental theorem in aerodynamics used for the calculation of lift of an airfoil and any two-dimensional bodies including circular cylinders translating into a uniform fluid at a constant speed large enough so that the flow seen in the body-fixed frame is steady and unseparated. The theorem relates the lift generated by an airfoil to the speed of the airfoil through the fluid, the density of the fluid and the circulation around the airfoil. The circulation is defined as the line integral around a closed-loop enclosing the airfoil of the component of the velocity of the fluid tangent to the loop. It is named after Martin Kutta and Nikolai Zhukovsky (or Joukowski) who first developed its key ideas in the early 20th century. Kutta–Joukowski theorem is an inviscid theory, but it is a good approximation for real viscous flow in typical aerodynamic applications.
-The Kutta condition is a principle in steady-flow fluid dynamics, especially aerodynamics, that is applicable to solid bodies with sharp corners, such as the trailing edges of airfoils. It is named for German mathematician and aerodynamicist Martin Kutta.
Kuethe and Schetzer state the Kutta condition as follows:: § 4.11 A body with a sharp trailing edge which is moving through a fluid will create about itself a circulation of sufficient strength to hold the rear stagnation point at the trailing edge.
In fluid flow around a body with a sharp corner, the Kutta condition refers to the flow pattern in which fluid approaches the corner from above and below, meets at the corner, and then flows away from the body. None of the fluid flows around the sharp corner.
The Kutta condition is significant when using the Kutta–Joukowski theorem to calculate the lift created by an airfoil with a sharp trailing edge. The value of circulation of the flow around the airfoil must be that value which would cause the Kutta condition to exist.
-The Kutta condition allows an aerodynamicist to incorporate a significant effect of viscosity while neglecting viscous effects in the underlying conservation of momentum equation. It is important in the practical calculation of lift on a wing.
-Any real fluid is viscous, which implies that the fluid velocity vanishes on the airfoil. Prandtl showed that for large Reynolds number, defined as  Re =ρV∞cAμ , and small angle of attack, the flow around a thin airfoil is composed of a narrow viscous region called the boundary layer near the body and an inviscid flow region outside. In applying the Kutta-Joukowski theorem, the loop must be chosen outside this boundary layer. (For example, the circulation calculated using the loop corresponding to the surface of the airfoil would be zero for a viscous fluid.) The sharp trailing edge requirement corresponds physically to a flow in which the fluid moving along the lower and upper surfaces of the airfoil meet smoothly, with no fluid moving around the trailing edge of the airfoil. This is known as the Kutta condition.
-In irrotational, inviscid, incompressible flow (potential flow) over an airfoil, the Kutta condition can be implemented by calculating the stream function over the airfoil surface.
The same Kutta condition implementation method is also used for solving two dimensional subsonic (subcritical) inviscid steady compressible flows over isolated airfoils.
The viscous correction for the Kutta condition can be found in some of the recent studies.
 A. The Kutta condition is a physical requirement that the fluid moving along the lower and upper surfaces of an airfoil meet smoothly, with no fluid moving around the trailing edge of the airfoil.
 B. The Kutta condition is a physical requirement that the fluid moving along the lower and upper surfaces of an airfoil meet smoothly, with no fluid moving around the leading edge of the airfoil.
 C. The Kutta condition is a mathematical requirement that the loop used in applying the Kutta-Joukowski theorem must be chosen outside the boundary layer of the airfoil.
 D. The Kutta condition is a mathematical requirement that the flow can be assumed inviscid in the entire region outside the airfoil provided the Reynolds number is large and the angle of attack is small.
 E. The Kutta condition is a physical requirement that the circulation calculated using the loop corresponding to the surface of the airfoil must be zero for a viscous fluid. "
What is classical mechanics?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is classical mechanics?
 Context: -Classical mechanics is a physical theory describing the motion of macroscopic objects, from projectiles to parts of machinery and astronomical objects, such as spacecraft, planets, stars, and galaxies. For objects governed by classical mechanics, if the present state is known, it is possible to predict how it will move in the future (determinism), and how it has moved in the past (reversibility).
-Classical Classical physics includes the traditional branches and topics that were recognized and well-developed before the beginning of the 20th century—classical mechanics, acoustics, optics, thermodynamics, and electromagnetism. Classical mechanics is concerned with bodies acted on by forces and bodies in motion and may be divided into statics (study of the forces on a body or bodies not subject to an acceleration), kinematics (study of motion without regard to its causes), and dynamics (study of motion and the forces that affect it); mechanics may also be divided into solid mechanics and fluid mechanics (known together as continuum mechanics), the latter include such branches as hydrostatics, hydrodynamics, aerodynamics, and pneumatics. Acoustics is the study of how sound is produced, controlled, transmitted and received. Important modern branches of acoustics include ultrasonics, the study of sound waves of very high frequency beyond the range of human hearing; bioacoustics, the physics of animal calls and hearing, and electroacoustics, the manipulation of audible sound waves using electronics.Optics, the study of light, is concerned not only with visible light but also with infrared and ultraviolet radiation, which exhibit all of the phenomena of visible light except visibility, e.g., reflection, refraction, interference, diffraction, dispersion, and polarization of light. Heat is a form of energy, the internal energy possessed by the particles of which a substance is composed; thermodynamics deals with the relationships between heat and other forms of energy. Electricity and magnetism have been studied as a single branch of physics since the intimate connection between them was discovered in the early 19th century; an electric current gives rise to a magnetic field, and a changing magnetic field induces an electric current. Electrostatics deals with electric charges at rest, electrodynamics with moving charges, and magnetostatics with magnetic poles at rest.
-There are many branches of classical mechanics, such as: statics, dynamics, kinematics, continuum mechanics (which includes fluid mechanics), statistical mechanics, etc.
Mechanics: A branch of physics in which we study the object and properties of an object in form of a motion under the action of the force.
-Mechanics (from Ancient Greek: μηχανική, mēkhanikḗ, lit. ""of machines"") is the area of mathematics and physics concerned with the relationships between force, matter, and motion among physical objects. Forces applied to objects result in displacements or changes of an object's position relative to its environment.
Theoretical expositions of this branch of physics has its origins in Ancient Greece, for instance, in the writings of Aristotle and Archimedes (see History of classical mechanics and Timeline of classical mechanics). During the early modern period, scientists such as Galileo, Kepler, Huygens, and Newton laid the foundation for what is now known as classical mechanics.
As a branch of classical physics, mechanics deals with bodies that are either at rest or are moving with velocities significantly less than the speed of light. It can also be defined as the physical science that deals with the motion of and forces on bodies not in the quantum realm.
-Classical mechanics – describes the motion of macroscopic objects, from projectiles to parts of machinery, and astronomical objects, such as spacecraft, planets, stars and galaxies.
 A. Classical mechanics is the branch of physics that describes the motion of macroscopic objects using concepts such as mass, acceleration, and force. It is based on a three-dimensional Euclidean space with fixed axes, and utilises many equations and mathematical concepts to relate physical quantities to one another.
 B. Classical mechanics is the branch of physics that describes the motion of microscopic objects using concepts such as energy, momentum, and wave-particle duality. It is based on a four-dimensional space-time continuum and utilises many equations and mathematical concepts to relate physical quantities to one another.
 C. Classical mechanics is the branch of physics that studies the behaviour of subatomic particles such as electrons and protons. It is based on the principles of quantum mechanics and utilises many equations and mathematical concepts to describe the properties of these particles.
 D. Classical mechanics is the branch of physics that studies the behaviour of light and electromagnetic radiation. It is based on the principles of wave-particle duality and utilises many equations and mathematical concepts to describe the properties of light.
 E. Classical mechanics is the branch of physics that studies the behaviour of fluids and gases. It is based on the principles of thermodynamics and utilises many equations and mathematical concepts to describe the properties of these substances. "
Who shared the other half of the Nobel Prize with Yoichiro Nambu for discovering the origin of the explicit breaking of CP symmetry in the weak interactions?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: Who shared the other half of the Nobel Prize with Yoichiro Nambu for discovering the origin of the explicit breaking of CP symmetry in the weak interactions?
 Context: -On October 7, 2008, the Royal Swedish Academy of Sciences awarded the 2008 Nobel Prize in Physics to three scientists for their work in subatomic physics symmetry breaking. Yoichiro Nambu, of the University of Chicago, won half of the prize for the discovery of the mechanism of spontaneous broken symmetry in the context of the strong interactions, specifically chiral symmetry breaking. Physicists Makoto Kobayashi and Toshihide Maskawa, of Kyoto University, shared the other half of the prize for discovering the origin of the explicit breaking of CP symmetry in the weak interactions. This origin is ultimately reliant on the Higgs mechanism, but, so far understood as a ""just so"" feature of Higgs couplings, not a spontaneously broken symmetry phenomenon.
-Modern progress In 1963, American physicist Sheldon Glashow proposed that the weak nuclear force, electricity, and magnetism could arise from a partially unified electroweak theory. In 1967, Pakistani Abdus Salam and American Steven Weinberg independently revised Glashow's theory by having the masses for the W particle and Z particle arise through spontaneous symmetry breaking with the Higgs mechanism. This unified theory modeled the electroweak interaction as a force mediated by four particles: the photon for the electromagnetic aspect, and a neutral Z particle, and two charged W particles for the weak aspect. As a result of the spontaneous symmetry breaking, the weak force becomes short-range and the W and Z bosons acquire masses of 80.4 and 91.2 GeV/c2, respectively. Their theory was first given experimental support by the discovery of weak neutral currents in 1973. In 1983, the Z and W bosons were first produced at CERN by Carlo Rubbia's team. For their insights, Glashow, Salam, and Weinberg were awarded the Nobel Prize in Physics in 1979. Carlo Rubbia and Simon van der Meer received the Prize in 1984.
-However, this theory allowed a compound symmetry CP to be conserved. CP combines parity P (switching left to right) with charge conjugation C (switching particles with antiparticles). Physicists were again surprised when in 1964, James Cronin and Val Fitch provided clear evidence in kaon decays that CP symmetry could be broken too, winning them the 1980 Nobel Prize in Physics. In 1973, Makoto Kobayashi and Toshihide Maskawa showed that CP violation in the weak interaction required more than two generations of particles, effectively predicting the existence of a then unknown third generation. This discovery earned them half of the 2008 Nobel Prize in Physics.Unlike parity violation, CP violation occurs only in rare circumstances. Despite its limited occurrence under present conditions, it is widely believed to be the reason that there is much more matter than antimatter in the universe, and thus forms one of Andrei Sakharov's three conditions for baryogenesis.
-Charge violation was confirmed in the Wu experiment and in experiments performed by Valentine Telegdi and Jerome Friedman and Garwin and Lederman who observed parity non-conservation in pion and muon decay and found that C is also violated. Charge violation was more explicitly shown in experiments done by John Riley Holt at the University of Liverpool.Oehme then wrote a paper with Lee and Yang in which they discussed the interplay of non-invariance under P, C and T. The same result was also independently obtained by Ioffe, Okun and Rudik. Both groups also discussed possible CP violations in neutral kaon decays.Lev Landau proposed in 1957 CP-symmetry, often called just CP as the true symmetry between matter and antimatter. CP-symmetry is the product of two transformations: C for charge conjugation and P for parity. In other words, a process in which all particles are exchanged with their antiparticles was assumed to be equivalent to the mirror image of the original process and so the combined CP-symmetry would be conserved in the weak interaction.
-Following the success of quantum electrodynamics in the 1950s, attempts were undertaken to formulate a similar theory of the weak nuclear force. This culminated around 1968 in a unified theory of electromagnetism and weak interactions by Sheldon Glashow, Steven Weinberg, and Abdus Salam, for which they shared the 1979 Nobel Prize in Physics. Their electroweak theory postulated not only the W bosons necessary to explain beta decay, but also a new Z boson that had never been observed.
 A. Richard Feynman and Julian Schwinger
 B. Makoto Kobayashi and Toshihide Maskawa
 C. Steven Weinberg and Sheldon Glashow
 D. Peter Higgs and Francois Englert
 E. Murray Gell-Mann and George Zweig "
What are some models that attempt to account for all observations without invoking supplemental non-baryonic matter?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What are some models that attempt to account for all observations without invoking supplemental non-baryonic matter?
 Context: -Photons, W bosons, and Z bosons, excitations of the electroweak gauge fields.
Higgs bosons, excitations of one component of the Higgs field, which gives mass to fundamental particles.In addition, composite particles such as mesons, as well as quasiparticles, can be described as excitations of an effective field.
Gravity is not a part of the Standard Model, but it is thought that there may be particles called gravitons which are the excitations of gravitational waves. The status of this particle is still tentative, because the theory is incomplete and because the interactions of single gravitons may be too weak to be detected.
-The Higgs boson plays a unique role in the Standard Model, by explaining why the other elementary particles, except the photon and gluon, are massive. In particular, the Higgs boson explains why the photon has no mass, while the W and Z bosons are very heavy. Elementary-particle masses and the differences between electromagnetism (mediated by the photon) and the weak force (mediated by the W and Z bosons) are critical to many aspects of the structure of microscopic (and hence macroscopic) matter. In electroweak theory, the Higgs boson generates the masses of the leptons (electron, muon, and tau) and quarks. As the Higgs boson is massive, it must interact with itself.
-Their high masses limit the range of the weak interaction. By way of contrast, the photon is the force carrier of the electromagnetic force and has zero mass, consistent with the infinite range of electromagnetism; the hypothetical graviton is also expected to have zero mass. (Although gluons are also presumed to have zero mass, the range of the color force is limited for different reasons; see color confinement.) All three bosons have particle spin s = 1. The emission of a W+ or W− boson either lowers or raises the electric charge of the emitting particle by one unit, and also alters the spin by one unit. At the same time, the emission or absorption of a W± boson can change the type of the particle – for example changing a strange quark into an up quark. The neutral Z boson cannot change the electric charge of any particle, nor can it change any other of the so-called ""charges"" (such as strangeness, baryon number, charm, etc.). The emission or absorption of a Z0 boson can only change the spin, momentum, and energy of the other particle. (See also Weak neutral current.) 
-For the derivation of Einstein's equations from an entropic gravity perspective, Tower Wang shows that the inclusion of energy-momentum conservation and cosmological homogeneity and isotropy requirements severely restricts a wide class of potential modifications of entropic gravity, some of which have been used to generalize entropic gravity beyond the singular case of an entropic model of Einstein's equations. Wang asserts that: As indicated by our results, the modified entropic gravity models of form (2), if not killed, should live in a very narrow room to assure the energy-momentum conservation and to accommodate a homogeneous isotropic universe.
-Higgs boson Although the weak and electromagnetic forces appear quite different to us at everyday energies, the two forces are theorized to unify as a single electroweak force at high energies. This prediction was clearly confirmed by measurements of cross-sections for high-energy electron-proton scattering at the HERA collider at DESY. The differences at low energies is a consequence of the high masses of the W and Z bosons, which in turn are a consequence of the Higgs mechanism. Through the process of spontaneous symmetry breaking, the Higgs selects a special direction in electroweak space that causes three electroweak particles to become very heavy (the weak bosons) and one to remain with an undefined rest mass as it is always in motion (the photon). On 4 July 2012, after many years of experimentally searching for evidence of its existence, the Higgs boson was announced to have been observed at CERN's Large Hadron Collider. Peter Higgs who first posited the existence of the Higgs boson was present at the announcement. The Higgs boson is believed to have a mass of approximately 125 GeV. The statistical significance of this discovery was reported as 5 sigma, which implies a certainty of roughly 99.99994%. In particle physics, this is the level of significance required to officially label experimental observations as a discovery. Research into the properties of the newly discovered particle continues.
 A. The Doppler effect, the photoelectric effect, or the Compton effect.
 B. The Higgs boson, the W boson, or the Z boson.
 C. Modified Newtonian dynamics, tensor–vector–scalar gravity, or entropic gravity.
 D. The strong nuclear force, the weak nuclear force, or the electromagnetic force.
 E. Kepler's laws, Newton's laws, or Einstein's theory of general relativity. "
What is the purpose of the proximity-focusing design in a RICH detector?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the purpose of the proximity-focusing design in a RICH detector?
 Context: -The most advanced type of a detector is the RICH, or ring-imaging Cherenkov detector, developed in the 1980s. In a RICH detector, a cone of Cherenkov light is produced when a high-speed charged particle traverses a suitable medium, often called radiator. This light cone is detected on a position sensitive planar photon detector, which allows reconstructing a ring or disc, whose radius is a measure for the Cherenkov emission angle. Both focusing and proximity-focusing detectors are in use. In a focusing RICH detector, the photons are collected by a spherical mirror and focused onto the photon detector placed at the focal plane. The result is a circle with a radius independent of the emission point along the particle track. This scheme is suitable for low refractive index radiators—i.e. gases—due to the larger radiator length needed to create enough photons. In the more compact proximity-focusing design, a thin radiator volume emits a cone of Cherenkov light which traverses a small distance—the proximity gap—and is detected on the photon detector plane. The image is a ring of light whose radius is defined by the Cherenkov emission angle and the proximity gap. The ring thickness is determined by the thickness of the radiator. An example of a proximity gap RICH detector is the High Momentum Particle Identification Detector (HMPID), a detector currently under construction for ALICE (A Large Ion Collider Experiment), one of the six experiments at the LHC (Large Hadron Collider) at CERN.
-In the more compact proximity-focusing design a thin radiator volume emits a cone of Cherenkov light which traverses a small distance, the proximity gap, and is detected on the photon detector plane. The image is a ring of light the radius of which is defined by the Cherenkov emission angle and the proximity gap. The ring thickness is mainly determined by the thickness of the radiator. An example of a proximity gap RICH detector is the High Momentum Particle Identification (HMPID), one of the detectors of ALICE (A Large Ion Collider Experiment), which is one of the five experiments at the LHC (Large Hadron Collider) at CERN.
-If a dense medium (large refractive index) is used, only a thin radiator layer of the order of a few centimetres is required to emit a sufficient number of Cherenkov photons. The photon detector is then located at some distance (usually about 10 cm) behind the radiator, allowing the cone of light to expand and form the characteristic ring-shaped image. Such a proximity-focusing RICH is installed in the ALICE experiment.
-In a RICH detector the photons within this light-cone pass through an optical system and impinge upon a position sensitive photon detector. With a suitably focusing optical system this allows reconstruction of a ring, similar to that above, the radius of which gives a measure of the Cherenkov emission angle  θc . The resolving power of this method is illustrated by comparing the Cherenkov angle per photon, see the first plot above, with the mean Cherenkov angle per particle (averaged over all photons emitted by that particle) obtained by ring-imaging, shown below; the greatly enhanced separation between particle types is very clear: This ability of a RICH system to successfully resolve different hypotheses for the particle type depends on two principal factors, which in turn depend upon the listed sub-factors; The effective angular resolution per photon,  σ Chromatic dispersion in the radiator ( n varies with photon frequency) Aberrations in the optical system Position resolution of the photon detector The maximum number of detected photons in the ring-image,  Nc The length of radiator through which the particle travels Photon transmission through the radiator material Photon transmission through the optical system Quantum efficiency of the photon detectors σ is a measure of the intrinsic optical precision of the RICH detector.  Nc is a measure of the optical response of the RICH; it can be thought of as the limiting case of the number of actually detected photons produced by a particle whose velocity approaches that of light, averaged over all relevant particle trajectories in the RICH detector. The average number of Cherenkov photons detected, for a slower particle, of charge  q (normally ±1), emitting photons at angle  θc is then sin 2⁡(θc)1−1n2 and the precision with which the mean Cherenkov angle can be determined with these photons is approximately σm=σN to which the angular precision of the emitting particle's measured direction must be added in quadrature, if it is not negligible compared to  σm Given the known momentum of the emitting particle and the refractive index of the radiator, the expected Cherenkov angle for each particle type can be predicted, and its difference from the observed mean Cherenkov angle calculated. Dividing this difference by  σm then gives a measure of the 'number of sigma' deviation of the hypothesis from the observation, which can be used in computing a probability or likelihood for each possible hypothesis. The following figure shows the 'number of sigma' deviation of the kaon hypothesis from a true pion ring image (π not k) and of the pion hypothesis from a true kaon ring image (k not π), as a function of momentum, for a RICH with  n = 1.0005,  Nc = 25,  σ = 0.64 milliradians; Also shown are the average number of detected photons from pions(Ngπ) or from kaons(Ngk). One can see that the RICH's ability to separate the two particle types exceeds 4-sigma everywhere between threshold and 80 GeV/c, finally dropping below 3-sigma at about 100 GeV. It is important to note that this result is for an 'ideal' detector, with homogeneous acceptance and efficiency, normal error distributions and zero background. No such detector exists, of course, and in a real experiment much more sophisticated procedures are actually used to account for those effects; position dependent acceptance and efficiency; non-Gaussian error distributions; non negligible and variable event-dependent backgrounds.In practice, for the multi-particle final states produced in a typical collider experiment, separation of kaons from other final state hadrons, mainly pions, is the most important purpose of the RICH. In that context the two most vital RICH functions, which maximise signal and minimise combinatorial backgrounds, are its ability to correctly identify a kaon as a kaon and its ability not to misidentify a pion as a kaon. The related probabilities, which are the usual measures of signal detection and background rejection in real data, are plotted below to show their variation with momentum (simulation with 10% random background); Note that the ~30% π → k misidentification rate at 100 GeV is, for the most part, due to the presence of 10% background hits (faking photons) in the simulated detector; the 3-sigma separation in the mean Cherenkov angle (shown in the 4th plot above) would, by itself, only account for about 6% misidentification. More detailed analyses of the above type, for operational RICH detectors, can be found in the published literature.
-RICH Types Both focusing and proximity-focusing detectors are in use. In a focusing RICH detector, the photons are collected by a spherical mirror with focal length  f and focused onto the photon detector placed at the focal plane. The result is a circle with a radius  r=fθc , independent of the emission point along the particle's track ( θc≪1 ). This scheme is suitable for low refractive index radiators (i.e., gases) with their larger radiator length needed to create enough photons.
 A. To emit a cone of Askaryan radiation that traverses a small distance and is detected on the photon detector plane, creating a ring of light whose radius is defined by the Cherenkov emission angle and the proximity gap.
 B. To emit a cone of Bremsstrahlung radiation that traverses a large distance and is detected on the photon detector plane, creating a ring of light whose radius is defined by the Cherenkov emission angle and the proximity gap.
 C. To emit a cone of Cherenkov light that traverses a large distance and is detected on the photon detector plane, creating a ring of light whose radius is defined by the Cherenkov emission angle and the proximity gap.
 D. To emit a cone of Cherenkov light that traverses a small distance and is detected on the photon detector plane, creating a ring of light whose radius is defined by the Cherenkov emission angle and the proximity gap.
 E. To emit a cone of Bremsstrahlung radiation that traverses a small distance and is detected on the photon detector plane, creating a ring of light whose radius is defined by the Cherenkov emission angle and the proximity gap. "
What is a light-year?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is a light-year?
 Context: -A light-year, alternatively spelled light year, is a unit of length used to express astronomical distances and is equivalent to about 9.46 trillion kilometers (9.46×1012 km), or 5.88 trillion miles (5.88×1012 mi). As defined by the International Astronomical Union (IAU), a light-year is the distance that light travels in a vacuum in one Julian year (365.25 days). Because it includes the word ""year"", the term is sometimes misinterpreted as a unit of time.The light-year is most often used when expressing distances to stars and other distances on a galactic scale, especially in non-specialist contexts and popular science publications. The unit most commonly used in professional astronomy is the parsec (symbol: pc, about 3.26 light-years) which derives from astrometry; it is the distance at which one astronomical unit (au) subtends an angle of one second of arc.
-light-year (ly) A unit of length used to express astronomical distances that is equivalent to the distance that an object moving at the speed of light in vacuum would travel in one Julian year: approximately 9.46 trillion kilometres (9.46×1012 km) or 5.88 trillion miles (5.88×1012 mi). Though the light-year is often used to measure galactic-scale distances in non-specialist publications, the unit of length most commonly used in professional astrometry is the parsec.
-Distances between objects within a star system tend to be small fractions of a light-year, and are usually expressed in astronomical units. However, smaller units of length can similarly be formed usefully by multiplying units of time by the speed of light. For example, the light-second, useful in astronomy, telecommunications and relativistic physics, is exactly 299792458 metres or 1⁄31557600 of a light-year. Units such as the light-minute, light-hour and light-day are sometimes used in popular science publications. The light-month, roughly one-twelfth of a light-year, is also used occasionally for approximate measures. The Hayden Planetarium specifies the light month more precisely as 30 days of light travel time.Light travels approximately one foot in a nanosecond; the term ""light-foot"" is sometimes used as an informal measure of time.
-Subsequent explorations of the Solar System by space probes made it possible to obtain precise measurements of the relative positions of the inner planets and other objects by means of radar and telemetry. As with all radar measurements, these rely on measuring the time taken for photons to be reflected from an object. Because all photons move at the speed of light in vacuum, a fundamental constant of the universe, the distance of an object from the probe is calculated as the product of the speed of light and the measured time. However, for precision the calculations require adjustment for things such as the motions of the probe and object while the photons are transiting. In addition, the measurement of the time itself must be translated to a standard scale that accounts for relativistic time dilation. Comparison of the ephemeris positions with time measurements expressed in Barycentric Dynamical Time (TDB) leads to a value for the speed of light in astronomical units per day (of 86400 s). By 2009, the IAU had updated its standard measures to reflect improvements, and calculated the speed of light at 173.1446326847(69) au/d (TDB).In 1983, the CIPM modified the International System of Units (SI) to make the metre defined as the distance travelled in a vacuum by light in 1 / 299792458 second. This replaced the previous definition, valid between 1960 and 1983, which was that the metre equalled a certain number of wavelengths of a certain emission line of krypton-86. (The reason for the change was an improved method of measuring the speed of light.) The speed of light could then be expressed exactly as c0 = 299792458 m/s, a standard also adopted by the IERS numerical standards. From this definition and the 2009 IAU standard, the time for light to traverse an astronomical unit is found to be τA = 499.0047838061±0.00000001 s, which is slightly more than 8 minutes 19 seconds. By multiplication, the best IAU 2009 estimate was A = c0τA = 149597870700±3 m, based on a comparison of Jet Propulsion Laboratory and IAA–RAS ephemerides.In 2006, the BIPM reported a value of the astronomical unit as 1.49597870691(6)×1011 m. In the 2014 revision of the SI Brochure, the BIPM recognised the IAU's 2012 redefinition of the astronomical unit as 149597870700 m.This estimate was still derived from observation and measurements subject to error, and based on techniques that did not yet standardize all relativistic effects, and thus were not constant for all observers. In 2012, finding that the equalization of relativity alone would make the definition overly complex, the IAU simply used the 2009 estimate to redefine the astronomical unit as a conventional unit of length directly tied to the metre (exactly 149597870700 m). The new definition also recognizes as a consequence that the astronomical unit is now to play a role of reduced importance, limited in its use to that of a convenience in some applications.
-An equivalent formulation of the old definition of the astronomical unit is the radius of an unperturbed circular Newtonian orbit about the Sun of a particle having infinitesimal mass, moving with a mean motion of 0.01720209895 radians per day.  The speed of light in IAU is the defined value c0 = 299792458 m/s of the SI units. In terms of this speed, the old definition of the astronomical unit of length had the accepted value: 1 au = c0τA = (149597870700±3) m, where τA is the transit time of light across the astronomical unit. The astronomical unit of length was determined by the condition that the measured data in the ephemeris match observations, and that in turn decides the transit time τA.
 A. A unit of time used to express astronomical distances that is equivalent to the time that an object moving at the speed of light in vacuum would take to travel in one Julian year: approximately 9.46 trillion seconds (9.46×1012 s) or 5.88 trillion minutes (5.88×1012 min).
 B. A unit of length used to express astronomical distances that is equivalent to the distance that an object moving at the speed of light in vacuum would travel in one Julian year: approximately 9.46 trillion kilometres (9.46×1012 km) or 5.88 trillion miles (5.88×1012 mi).
 C. A unit of temperature used to express astronomical distances that is equivalent to the temperature of an object moving at the speed of light in vacuum in one Julian year: approximately 9.46 trillion Kelvin (9.46×1012 K) or 5.88 trillion Celsius (5.88×1012 °.
 D. A unit of energy used to express astronomical distances that is equivalent to the energy of an object moving at the speed of light in vacuum in one Julian year: approximately 9.46 trillion joules (9.46×1012 J) or 5.88 trillion watt-hours (5.88×1012 Wh).
 E. A unit of mass used to express astronomical distances that is equivalent to the mass of an object moving at the speed of light in vacuum in one Julian year: approximately 9.46 trillion kilograms (9.46×1012 kg) or 5.88 trillion pounds (5.88×1012 lb). "
What is the main advantage of ferroelectric memristors?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the main advantage of ferroelectric memristors?
 Context: -Ferroelectric memristor The ferroelectric memristor is based on a thin ferroelectric barrier sandwiched between two metallic electrodes. Switching the polarization of the ferroelectric material by applying a positive or negative voltage across the junction can lead to a two order of magnitude resistance variation: ROFF ≫ RON (an effect called Tunnel Electro-Resistance). In general, the polarization does not switch abruptly. The reversal occurs gradually through the nucleation and growth of ferroelectric domains with opposite polarization. During this process, the resistance is neither RON or ROFF, but in between. When the voltage is cycled, the ferroelectric domain configuration evolves, allowing a fine tuning of the resistance value. The ferroelectric memristor's main advantages are that ferroelectric domain dynamics can be tuned, offering a way to engineer the memristor response, and that the resistance variations are due to purely electronic phenomena, aiding device reliability, as no deep change to the material structure is involved.
-Atomristor Atomristor is defined as the electrical devices showing memristive behavior in atomically thin nanomaterials or atomic sheets. In 2018, Ge and Wu et al. in the Akinwande group at the University of Texas, first reported a universal memristive effect in single-layer TMD (MX2, M = Mo, W; and X = S, Se) atomic sheets based on vertical metal-insulator-metal (MIM) device structure. The work was later extended to monolayer hexagonal boron nitride, which is the thinnest memory material of around 0.33 nm. These atomristors offer forming-free switching and both unipolar and bipolar operation. The switching behavior is found in single-crystalline and poly-crystalline films, with various conducting electrodes (gold, silver and graphene). Atomically thin TMD sheets are prepared via CVD/MOCVD, enabling low-cost fabrication. Afterwards, taking advantage of the low ""on"" resistance and large on/off ratio, a high-performance zero-power RF switch is proved based on MoS2 or h-BN atomristors, indicating a new application of memristors for 5G, 6G and THz communication and connectivity systems. In 2020, atomistic understanding of the conductive virtual point mechanism was elucidated in an article in nature nanotechnology.
-Memristive networks are a particular type of physical neural network that have very similar properties to (Little-)Hopfield networks, as they have a continuous dynamics, have a limited memory capacity and they natural relax via the minimization of a function which is asymptotic to the Ising model. In this sense, the dynamics of a memristive circuit has the advantage compared to a Resistor-Capacitor network to have a more interesting non-linear behavior. From this point of view, engineering an analog memristive networks accounts to a peculiar type of neuromorphic engineering in which the device behavior depends on the circuit wiring, or topology.
-Multiferroicity and magnetic-ferroelectric crossover. The PJTE theory of ferroelectricity in ABO3 crystals was expanded to show that, depending on the number of electrons in the dn shell of the transition metal ion B4+ and their low spin or high spin arrangement (which controls the symmetry and spin multiplicity of the ground and PJTE active excited states of the [BO6] center), the ferroelectricity may coexist with a magnetic moment (multiferroicity). Moreover, in combination with the temperature dependent spin crossover phenomenon (which changes the spin multiplicity), this kind of multiferroicity may lead to a novel effect known as a magnetic-ferroelectric crossover.Solid state magnetic-dielectric bistability. Similar to the above-mentioned molecular bistability induced by the hidden PJTE, a magnetic-dielectric bistability due to two coexisting equilibrium configurations with corresponding properties may take place also in crystals with transition metal centers, subject to the electronic configuration with half-filled e2 or t3 shells. As in molecular systems, the latter produce a hidden PJTE and local bistability which, distinguished from the molecular case, are enhanced by the cooperative interactions, thus acquiring larger lifetimes. This crystal bistability was proved by calculations for LiCuO2 and NaCuO2 crystals, in which the Cu3+ ion has the electronic e2(d8) configuration (similar to the CuF3 molecule).Giant enhancement of observable properties in interaction with external perturbations. In a recent development it was shown that in inorganic crystals with PJTE centers, in which the local distortions are not ordered (before the phase transition to the cooperative phase), the effect of interaction with external perturbations contains an orientational contribution which enhances the observable properties by several orders of magnitude. This was demonstrated on the properties of crystals like paraelectric BaTiO3 in interaction with electric fields (in permittivity and electrostriction), or under a strain gradient (flexoelectricity). These giant enhancement effects occur due to the dynamic nature of the PJTE local dipolar distortions (their tunneling between the equivalent minima); the independently rotating dipole moments on each center become oriented (frozen) along the external perturbation resulting in an orientational polarization which is not there in the absence of the PJTE 
-Layered memristor In 2014, Bessonov et al. reported a flexible memristive device comprising a MoOx/MoS2 heterostructure sandwiched between silver electrodes on a plastic foil. The fabrication method is entirely based on printing and solution-processing technologies using two-dimensional layered transition metal dichalcogenides (TMDs). The memristors are mechanically flexible, optically transparent and produced at low cost. The memristive behaviour of switches was found to be accompanied by a prominent memcapacitive effect. High switching performance, demonstrated synaptic plasticity and sustainability to mechanical deformations promise to emulate the appealing characteristics of biological neural systems in novel computing technologies.
 A. Ferroelectric memristors have a higher resistance than other types of memristors, making them more suitable for high-power applications.
 B. Ferroelectric domain dynamics can be tuned, allowing for the engineering of memristor response, and resistance variations are due to purely electronic phenomena, making the device more reliable.
 C. Ferroelectric memristors have a more complex structure than other types of memristors, allowing for a wider range of applications.
 D. Ferroelectric memristors have a unique piezoelectric field that allows for the creation of non-uniform elastic strain and a more stable structure.
 E. Ferroelectric memristors are based on vertically aligned carbon nanotubes, which offer a more efficient and faster switching mechanism than other materials. "
What is the term used to describe the conduction that occurs in non-crystalline semiconductors by charges quantum tunnelling from one localised site to another?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the term used to describe the conduction that occurs in non-crystalline semiconductors by charges quantum tunnelling from one localised site to another?
 Context: -Extrinsic (doped) semiconductors have a far more complicated temperature profile. As temperature increases starting from absolute zero they first decrease steeply in resistance as the carriers leave the donors or acceptors. After most of the donors or acceptors have lost their carriers, the resistance starts to increase again slightly due to the reducing mobility of carriers (much as in a metal). At higher temperatures, they behave like intrinsic semiconductors as the carriers from the donors/acceptors become insignificant compared to the thermally generated carriers.In non-crystalline semiconductors, conduction can occur by charges quantum tunnelling from one localised site to another. This is known as variable range hopping and has the characteristic form of where n = 2, 3, 4, depending on the dimensionality of the system.
-Unlike in metals, the atoms that make up the bulk semiconductor crystal do not provide the electrons which are responsible for conduction. In semiconductors, electrical conduction is due to the mobile charge carriers, electrons or holes which are provided by impurities or dopant atoms in the crystal. In an extrinsic semiconductor, the concentration of doping atoms in the crystal largely determines the density of charge carriers, which determines its electrical conductivity, as well as a great many other electrical properties. This is the key to semiconductors' versatility; their conductivity can be manipulated over many orders of magnitude by doping.
-The electrical conductivity of chemically pure semiconductors can still be affected by crystallographic defects of technological origin (like vacancies), some of which can behave similar to dopants. Their effect can often be neglected, though, and the number of electrons in the conduction band is then exactly equal to the number of holes in the valence band. The conduction of current of intrinsic semiconductor is enabled purely by electron excitation across the band-gap, which is usually small at room temperature except for narrow-bandgap semiconductors, like Hg0.8Cd0.2Te.
-In semiconductors, which are the materials used to make electronic components like transistors and integrated circuits, two types of charge carrier are possible. In p-type semiconductors, ""effective particles"" known as electron holes with positive charge move through the crystal lattice, producing an electric current. The ""holes"" are, in effect, electron vacancies in the valence-band electron population of the semiconductor and are treated as charge carriers because they are mobile, moving from atom site to atom site. In n-type semiconductors, electrons in the conduction band move through the crystal, resulting in an electric current.In some conductors, such as ionic solutions and plasmas, positive and negative charge carriers coexist, so in these cases an electric current consists of the two types of carrier moving in opposite directions. In other conductors, such as metals, there are only charge carriers of one polarity, so an electric current in them simply consists of charge carriers moving in one direction.
-Semiconductor materials are useful because their behavior can be easily manipulated by the deliberate addition of impurities, known as doping. Semiconductor conductivity can be controlled by the introduction of an electric or magnetic field, by exposure to light or heat, or by the mechanical deformation of a doped monocrystalline silicon grid; thus, semiconductors can make excellent sensors. Current conduction in a semiconductor occurs due to mobile or ""free"" electrons and electron holes, collectively known as charge carriers. Doping a semiconductor with a small proportion of an atomic impurity, such as phosphorus or boron, greatly increases the number of free electrons or holes within the semiconductor. When a doped semiconductor contains excess holes, it is called a p-type semiconductor (p for positive electric charge); when it contains excess free electrons, it is called an n-type semiconductor (n for a negative electric charge). A majority of mobile charge carriers have negative charges. The manufacture of semiconductors controls precisely the location and concentration of p- and n-type dopants. The connection of n-type and p-type semiconductors form p–n junctions.
 A. Intrinsic semiconductors
 B. Electrical impedance tomography
 C. Quantum conduction
 D. Carrier mobility
 E. Variable range hopping "
What is resistivity?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is resistivity?
 Context: -ResistivityElectrical resistivity (also called specific electrical resistance or volume resistivity) and its inverse, electrical conductivity, is a fundamental property of a material that quantifies how strongly it resists or conducts electric current. A low resistivity indicates a material that readily allows electric current. Resistivity is commonly represented by the Greek letter ρ (rho). The SI unit of electrical resistivity is the ohm-meter (Ω⋅m). For example, if a 1 m × 1 m × 1 m solid cube of material has sheet contacts on two opposite faces, and the resistance between these contacts is 1 Ω, then the resistivity of the material is 1 Ω⋅m.
-Electrical resistivity (also called volume resistivity or specific electrical resistance) is a fundamental specific property of a material that measures its electrical resistance or how strongly it resists electric current. A low resistivity indicates a material that readily allows electric current. Resistivity is commonly represented by the Greek letter ρ (rho). The SI unit of electrical resistivity is the ohm-metre (Ω⋅m). For example, if a 1 m3 solid cube of material has sheet contacts on two opposite faces, and the resistance between these contacts is 1 Ω, then the resistivity of the material is 1 Ω⋅m.
-Both resistance and resistivity describe how difficult it is to make electrical current flow through a material, but unlike resistance, resistivity is an intrinsic property and doesn't depend on geometric properties of a material. This means that all pure copper (Cu) wires (which have not been subjected to distortion of their crystalline structure etc.), irrespective of their shape and size, have the same resistivity, but a long, thin copper wire has a much larger resistance than a thick, short copper wire. Every material has its own characteristic resistivity. For example, rubber has a far larger resistivity than copper.
-Resistivity, which is a characteristic of particles in an electric field, is a measure of a particle's resistance to transferring charge (both accepting and giving up charges). Resistivity is a function of a particle's chemical composition as well as flue gas operating conditions such as temperature and moisture. Particles can have high, moderate (normal), or low resistivity.
-Bulk resistivity is defined using a more general version of Ohm’s Law, as given in Equation (1) below: Where: E is the Electric field strength.Unit:-(V/cm); j is the Current density.Unit:-(A/cm2); and ρ is the Resistivity.Unit:-(Ohm-cm) A better way of displaying this would be to solve for resistivity as a function of applied voltage and current, as given in Equation (2) below: Where: ρ = Resistivity.Unit:-(Ohm-cm) V = The applied DC potential.Unit:-(Volts); I = The measured current.Unit:-(Amperes); l = The ash layer thickness.Unit:-(cm); and A = The current measuring electrode face area.Unit:-(cm2).
 A. Resistivity is an extrinsic property of a material that describes how difficult it is to make electrical current flow through it. It is measured in ohms and is dependent on the material's shape and size.
 B. Resistivity is a measure of the resistance of a material to electrical current flow. It is measured in ohm-meters and is dependent on the material's shape and size.
 C. Resistivity is an intrinsic property of a material that describes how difficult it is to make electrical current flow through it. It is measured in ohm-meters and is independent of the material's shape and size.
 D. Resistivity is a measure of the electrical current that can flow through a material. It is measured in ohms and is dependent on the material's shape and size.
 E. Resistivity is a measure of the electrical current that can flow through a material. It is measured in ohm-meters and is independent of the material's shape and size. "
What did Newton adopt after his correspondence with Hooke in 1679-1680?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What did Newton adopt after his correspondence with Hooke in 1679-1680?
 Context: -In regard to evidence that still survives of the earlier history, manuscripts written by Newton in the 1660s show that Newton himself had, by 1669, arrived at proofs that in a circular case of planetary motion, ""endeavour to recede"" (what was later called centrifugal force) had an inverse-square relation with distance from the center. After his 1679–1680 correspondence with Hooke, Newton adopted the language of inward or centripetal force. According to Newton scholar J. Bruce Brackenridge, although much has been made of the change in language and difference of point of view, as between centrifugal or centripetal forces, the actual computations and proofs remained the same either way. They also involved the combination of tangential and radial displacements, which Newton was making in the 1660s. The lesson offered by Hooke to Newton here, although significant, was one of perspective and did not change the analysis. This background shows there was basis for Newton to deny deriving the inverse square law from Hooke.
-Newton's early work on motion In the 1660s Newton studied the motion of colliding bodies and deduced that the centre of mass of two colliding bodies remains in uniform motion. Surviving manuscripts of the 1660s also show Newton's interest in planetary motion and that by 1669 he had shown, for a circular case of planetary motion, that the force he called ""endeavour to recede"" (now called centrifugal force) had an inverse-square relation with distance from the center. After his 1679–1680 correspondence with Hooke, described below, Newton adopted the language of inward or centripetal force. According to Newton scholar J. Bruce Brackenridge, although much has been made of the change in language and difference of point of view, as between centrifugal or centripetal forces, the actual computations and proofs remained the same either way. They also involved the combination of tangential and radial displacements, which Newton was making in the 1660s. The difference between the centrifugal and centripetal points of view, though a significant change of perspective, did not change the analysis. Newton also clearly expressed the concept of linear inertia in the 1660s: for this Newton was indebted to Descartes' work published 1644.
-In physics, the history of centrifugal and centripetal forces illustrates a long and complex evolution of thought about the nature of forces, relativity, and the nature of physical laws.
-The principle of equivalence: There is no experiment observers can perform to distinguish whether an acceleration arises because of a gravitational force or because their reference frame is accelerating In short, centrifugal force played a key early role in establishing the set of inertial frames of reference and the significance of fictitious forces, even aiding in the development of general relativity.
-Around 1914, the analogy between centrifugal force (sometimes used to create artificial gravity) and gravitational forces led to the equivalence principle of general relativity.
 A. The language of inward or centripetal force.
 B. The language of gravitational force.
 C. The language of outward or centrifugal force.
 D. The language of tangential and radial displacements.
 E. The language of electromagnetic force. "
What is the metallicity of Kapteyn's star estimated to be?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the metallicity of Kapteyn's star estimated to be?
 Context: -In 2014, the first planets around a halo star were announced around Kapteyn's star, the nearest halo star to Earth, around 13 light years away. However, later research suggests that Kapteyn b is just an artefact of stellar activity and that Kapteyn c needs more study to be confirmed. The metallicity of Kapteyn's star is estimated to be about 8 times less than the Sun.Different types of galaxies have different histories of star formation and hence planet formation. Planet formation is affected by the ages, metallicities, and orbits of stellar populations within a galaxy. Distribution of stellar populations within a galaxy varies between the different types of galaxies.
-Metallicity of 89–112% (± 0.05 dex) of that of the Sun, meaning the star's proplyd would have had almost exactly the same amount of dust for planetary formation No stellar companion, because the Sun itself is a solitary star An age within 1 billion years from that of the Sun (3.6 to 5.6 Ga)The following are the known stars that come closest to satisfying the criteria for a solar twin. The Sun is listed for comparison. Highlighted boxes are out of range for a solar twin. The star may have been noted as solar twin in the past, but are more of a solar analog.
-The remainder of the elements are collectively referred to as ""metals"", and the metallicity – the mass fraction of elements heavier than helium – is calculated as Z=∑e>HemeM=1−X−Y.
For the surface of the Sun (symbol  ⊙ ), these parameters are measured to have the following values: Due to the effects of stellar evolution, neither the initial composition nor the present day bulk composition of the Sun is the same as its present-day surface composition.
Chemical abundance ratios The overall stellar metallicity is conventionally defined using the total hydrogen content, since its abundance is considered to be relatively constant in the Universe, or the iron content of the star, which has an abundance that is generally linearly increasing in time in the Universe.
-The star has a mass of 0.539 solar masses, a radius of 0.547 solar radii, and a temperature of about 3,652 K (3,379 °C; 6,114 °F). It is about 0.3-3 billion years old, with a metallicity of 0.2 Fe/H and a rotation period of 21.54 days. The star exhibits strong stellar activity, with three ultraviolet flares detected by 2021.
-The distance is uncertain, with estimates between 3.5 kiloparsecs (11,410 light-years) and 6.9 kiloparsecs (22,500 light-years). Assuming a distance of 4.8 kiloparsecs (15,600 light-years), this star is calculated to be 229,000 times brighter than the Sun, 13 times more massive, and 1.26 times larger with a surface temperature of 112,200 K. This makes it currently the second smallest known WN star in the galaxy, after WR 2.
 A. 8 times more than the Sun
 B. 8 times less than the Sun
 C. 13 light years away from Earth
 D. Unknown
 E. Equal to the Sun "
What is the SI base unit of time and how is it defined?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the SI base unit of time and how is it defined?
 Context: -A unit of time is any particular time interval, used as a standard way of measuring or expressing duration. The base unit of time in the International System of Units (SI), and by extension most of the Western world, is the second, defined as about 9 billion oscillations of the caesium atom. The exact modern SI definition is ""[The second] is defined by taking the fixed numerical value of the cesium frequency, ΔνCs, the unperturbed ground-state hyperfine transition frequency of the cesium 133 atom, to be 9 192 631 770 when expressed in the unit Hz, which is equal to s−1.""Historically, many units of time were defined by the movements of astronomical objects.
-In the International System of Units (SI), the unit of time is the second (symbol:  s ). It is a SI base unit, and has been defined since 1967 as ""the duration of 9,192,631,770 [cycles] of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the caesium 133 atom"". This definition is based on the operation of a caesium atomic clock. These clocks became practical for use as primary reference standards after about 1955, and have been in use ever since.
-International System of Units definition Since 1968, the SI has defined the second as the duration of 9192631770 cycles of radiation corresponding to the transition between two energy levels of the ground state of the caesium-133 atom. In 1997, the International Committee for Weights and Measures (CIPM) added that the preceding definition refers to a caesium atom at rest at a temperature of absolute zero.: 113 This definition makes the caesium oscillator the primary standard for time and frequency measurements, called the caesium standard. Following the 2019 redefinition of the SI base units, the definition of every base unit except the mole and almost every derived unit relies on the definition of the second.
-The SI base units are the standard units of measurement defined by the International System of Units (SI) for the seven base quantities of what is now known as the International System of Quantities: they are notably a basic set from which all other SI units can be derived. The units and their physical quantities are the second for time, the metre (sometimes spelled meter) for length or distance, the kilogram for mass, the ampere for electric current, the kelvin for thermodynamic temperature, the mole for amount of substance, and the candela for luminous intensity. The SI base units are a fundamental part of modern metrology, and thus part of the foundation of modern science and technology.
-Caesium-133 is the only stable isotope of caesium. The SI base unit of time, the second, is defined by a specific caesium-133 transition. Since 1967, the official definition of a second is: The second, symbol s, is defined by taking the fixed numerical value of the caesium frequency, ΔνCs, the unperturbed ground-state hyperfine transition frequency of the caesium-133 atom, to be 9192631770 when expressed in the unit Hz, which is equal to s−1.
 A. The SI base unit of time is the week, which is defined by measuring the electronic transition frequency of caesium atoms.
 B. The SI base unit of time is the second, which is defined by measuring the electronic transition frequency of caesium atoms.
 C. The SI base unit of time is the hour, which is defined by measuring the electronic transition frequency of caesium atoms.
 D. The SI base unit of time is the day, which is defined by measuring the electronic transition frequency of caesium atoms.
 E. The SI base unit of time is the minute, which is defined by measuring the electronic transition frequency of caesium atoms. "
What is a planetary system?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is a planetary system?
 Context: -A planetary system is a set of gravitationally bound non-stellar objects in or out of orbit around a star or star system. Generally speaking, systems with one or more planets constitute a planetary system, although such systems may also consist of bodies such as dwarf planets, asteroids, natural satellites, meteoroids, comets, planetesimals and circumstellar disks. The Sun together with the planetary system revolving around it, including Earth, forms the Solar System. The term exoplanetary system is sometimes used in reference to other planetary systems.
-A planet is a large, rounded astronomical body that is neither a star nor its remnant. The best available theory of planet formation is the nebular hypothesis, which posits that an interstellar cloud collapses out of a nebula to create a young protostar orbited by a protoplanetary disk. Planets grow in this disk by the gradual accumulation of material driven by gravity, a process called accretion. The Solar System has at least eight planets: the terrestrial planets Mercury, Venus, Earth and Mars, and the giant planets Jupiter, Saturn, Uranus and Neptune. These planets each rotate around an axis tilted with respect to its orbital pole. All planets of the Solar System other than Mercury possess a considerable atmosphere, and some share such features as ice caps, seasons, volcanism, hurricanes, tectonics, and even hydrology. Apart from Venus and Mars, the Solar System planets generate magnetic fields, and all except Venus and Mercury have natural satellites. The giant planets bear planetary rings, the most prominent being those of Saturn.
-photometric system photosphere plane of reference Also reference plane.
An arbitrarily chosen, imaginary plane from which to measure and define orbital elements such as inclination and longitude of the ascending node. The ecliptic plane, invariable plane, and equatorial plane are all commonly used as reference planes in various contexts.
planet A type of astronomical body orbiting a star or stellar remnant which is massive enough to be rounded by its own gravity (but not massive enough to achieve thermonuclear fusion) and has cleared its neighbouring region of all planetesimals.
planetary Of or relating to a planet or planets.
planetary body Also planetary object.
Any secondary body that is geologically differentiated or in hydrostatic equilibrium and therefore has a planet-like geology, such as a planet, dwarf planet, or other planetary-mass object, but excluding smaller objects such as planetesimals.
planetary differentiation The process of separating out different constituents of a planetary body, causing it to develop compositionally distinct layers (such as a metallic core).
planetary nebula A type of emission nebula formed from a glowing shell of expanding plasma that has been ejected from a red giant star late in its life. The name derives from their resemblance to a planet. An example is the Ring Nebula.
planetary science Also sometimes called planetology.
The scientific study of planets, moons, and planetary systems, with the aim of understanding their formation, composition, topography, dynamics, and interactions with other bodies.
planetary system Any set of gravitationally bound non-stellar objects in or out of orbit around a star or star system. In general, planetary systems include one or more planets, though such systems may also consist of dwarf planets, moons, asteroids, meteoroids, planetesimals, and debris discs, among other objects.
planetary-mass object (PMO) Also planemo or planetary body.
-Planetary science Planetary science is the study of the assemblage of planets, moons, dwarf planets, comets, asteroids, and other bodies orbiting the Sun, as well as extrasolar planets. The Solar System has been relatively well-studied, initially through telescopes and then later by spacecraft. This has provided a good overall understanding of the formation and evolution of the Sun's planetary system, although many new discoveries are still being made.The Solar System is divided into the inner Solar System (subdivided into the inner planets and the asteroid belt), the outer Solar System (subdivided into the outer planets and centaurs), comets, the trans-Neptunian region (subdivided into the Kuiper belt, and the scattered disc) and the farthest regions (e.g., boundaries of the heliosphere, and the Oort Cloud, which may extend as far as a light-year). The inner terrestrial planets consist of Mercury, Venus, Earth, and Mars. The outer giant planets are the gas giants (Jupiter and Saturn) and the ice giants (Uranus and Neptune).The planets were formed 4.6 billion years ago in the protoplanetary disk that surrounded the early Sun. Through a process that included gravitational attraction, collision, and accretion, the disk formed clumps of matter that, with time, became protoplanets. The radiation pressure of the solar wind then expelled most of the unaccreted matter, and only those planets with sufficient mass retained their gaseous atmosphere. The planets continued to sweep up, or eject, the remaining matter during a period of intense bombardment, evidenced by the many impact craters on the Moon. During this period, some of the protoplanets may have collided and one such collision may have formed the Moon.Once a planet reaches sufficient mass, the materials of different densities segregate within, during planetary differentiation. This process can form a stony or metallic core, surrounded by a mantle and an outer crust. The core may include solid and liquid regions, and some planetary cores generate their own magnetic field, which can protect their atmospheres from solar wind stripping.A planet or moon's interior heat is produced from the collisions that created the body, by the decay of radioactive materials (e.g. uranium, thorium, and 26Al), or tidal heating caused by interactions with other bodies. Some planets and moons accumulate enough heat to drive geologic processes such as volcanism and tectonics. Those that accumulate or retain an atmosphere can also undergo surface erosion from wind or water. Smaller bodies, without tidal heating, cool more quickly; and their geological activity ceases with the exception of impact cratering.
-The Solar System is the gravitationally bound system of the Sun and the objects that orbit it. The largest of such objects are the eight planets, in order from the Sun: four terrestrial planets named Mercury, Venus, Earth and Mars, two gas giants named Jupiter and Saturn, and two ice giants named Uranus and Neptune. The terrestrial planets have a definite surface and are mostly made of rock and metal. The gas giants are mostly made of hydrogen and helium, while the ice giants are mostly made of 'volatile' substances such as water, ammonia, and methane. In some texts, these terrestrial and giant planets are called the inner Solar System and outer Solar System planets respectively.
 A. A system of planets that are all located in the same solar system.
 B. A system of planets that are all the same size and shape.
 C. Any set of gravitationally bound non-stellar objects in or out of orbit around a star or star system.
 D. A system of planets that are all located in the same galaxy.
 E. A system of planets that are all made of gas. "
What is the result of the collapse of a cavitation bubble?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the result of the collapse of a cavitation bubble?
 Context: -Inertial cavitation Inertial cavitation was first observed in the late 19th century, considering the collapse of a spherical void within a liquid. When a volume of liquid is subjected to a sufficiently low pressure, it may rupture and form a cavity. This phenomenon is coined cavitation inception and may occur behind the blade of a rapidly rotating propeller or on any surface vibrating in the liquid with sufficient amplitude and acceleration. A fast-flowing river can cause cavitation on rock surfaces, particularly when there is a drop-off, such as on a waterfall.Other ways of generating cavitation voids involve the local deposition of energy, such as an intense focused laser pulse (optic cavitation) or with an electrical discharge through a spark. Vapor gases evaporate into the cavity from the surrounding medium; thus, the cavity is not a vacuum at all, but rather a low-pressure vapor (gas) bubble. Once the conditions which caused the bubble to form are no longer present, such as when the bubble moves downstream, the surrounding liquid begins to implode due its higher pressure, building up inertia as it moves inward. As the bubble finally collapses, the inward inertia of the surrounding liquid causes a sharp increase of pressure and temperature of the vapor within. The bubble eventually collapses to a minute fraction of its original size, at which point the gas within dissipates into the surrounding liquid via a rather violent mechanism which releases a significant amount of energy in the form of an acoustic shock wave and as visible light. At the point of total collapse, the temperature of the vapor within the bubble may be several thousand kelvin, and the pressure several hundred atmospheres.Inertial cavitation can also occur in the presence of an acoustic field. Microscopic gas bubbles that are generally present in a liquid will be forced to oscillate due to an applied acoustic field. If the acoustic intensity is sufficiently high, the bubbles will first grow in size and then rapidly collapse. Hence, inertial cavitation can occur even if the rarefaction in the liquid is insufficient for a Rayleigh-like void to occur. High-power ultrasonics usually utilize the inertial cavitation of microscopic vacuum bubbles for treatment of surfaces, liquids, and slurries.
-Cavitation is the formation of vapour bubbles in liquid caused by flow around an object. Bubbles form when water accelerates around sharp corners and the pressure drops below the vapour pressure. Pressure increases upon deceleration, and the water generally reabsorbs the vapour; however, vapour bubbles can implode and apply small concentrated impulses that may damage surfaces like ship propellers and pump impellers.
-Upon irradiation with high intensity sound or ultrasound, acoustic cavitation usually occurs. Cavitation – the formation, growth, and implosive collapse of bubbles irradiated with sound — is the impetus for sonochemistry and sonoluminescence. Bubble collapse in liquids produces enormous amounts of energy from the conversion of kinetic energy of the liquid motion into heating the contents of the bubble. The compression of the bubbles during cavitation is more rapid than thermal transport, which generates a short-lived localized hot-spot. Experimental results have shown that these bubbles have temperatures around 5000 K, pressures of roughly 1000 atm, and heating and cooling rates above 1010 K/s. These cavitations can create extreme physical and chemical conditions in otherwise cold liquids.
-Cavitation is a phenomenon in which the static pressure of a liquid reduces to below the liquid's vapour pressure, leading to the formation of small vapor-filled cavities in the liquid. When subjected to higher pressure, these cavities, called ""bubbles"" or ""voids"", collapse and can generate shock waves that may damage machinery. These shock waves are strong when they are very close to the imploded bubble, but rapidly weaken as they propagate away from the implosion. Cavitation is a significant cause of wear in some engineering contexts. Collapsing voids that implode near to a metal surface cause cyclic stress through repeated implosion. This results in surface fatigue of the metal, causing a type of wear also called ""cavitation"". The most common examples of this kind of wear are to pump impellers, and bends where a sudden change in the direction of liquid occurs. Cavitation is usually divided into two classes of behavior: inertial (or transient) cavitation and non-inertial cavitation.
-Sound waves propagating through a liquid at ultrasonic frequencies have wavelengths many times longer than the molecular dimensions or the bond length between atoms in the molecule. Therefore, the sound wave cannot directly affect the vibrational energy of the bond, and can therefore not directly increase the internal energy of a molecule. Instead, sonochemistry arises from acoustic cavitation: the formation, growth, and implosive collapse of bubbles in a liquid. The collapse of these bubbles is an almost adiabatic process, thereby resulting in the massive build-up of energy inside the bubble, resulting in extremely high temperatures and pressures in a microscopic region of the sonicated liquid. The high temperatures and pressures result in the chemical excitation of any matter within or very near the bubble as it rapidly implodes. A broad variety of outcomes can result from acoustic cavitation including sonoluminescence, increased chemical activity in the solution due to the formation of primary and secondary radical reactions, and increased chemical activity through the formation of new, relatively stable chemical species that can diffuse further into the solution to create chemical effects (for example, the formation of hydrogen peroxide from the combination of two hydroxyl radicals following the dissociation of water vapor within collapsing bubbles when water is exposed to ultrasound).
 A. The collapse of a cavitation bubble causes the surrounding liquid to expand, resulting in the formation of a low-pressure vapor bubble.
 B. The collapse of a cavitation bubble causes a decrease in pressure and temperature of the vapor within, releasing a small amount of energy in the form of an acoustic shock wave and visible light.
 C. The collapse of a cavitation bubble causes a sharp increase in pressure and temperature of the vapor within, releasing a significant amount of energy in the form of an acoustic shock wave and visible light.
 D. The collapse of a cavitation bubble causes the surrounding liquid to implode, resulting in the formation of a vacuum.
 E. The collapse of a cavitation bubble has no effect on the surrounding liquid or vapor. "
Who was Giordano Bruno?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: Who was Giordano Bruno?
 Context: -Early speculations This space we declare to be infinite... In it are an infinity of worlds of the same kind as our own.
In the sixteenth century, the Italian philosopher Giordano Bruno, an early supporter of the Copernican theory that Earth and other planets orbit the Sun (heliocentrism), put forward the view that the fixed stars are similar to the Sun and are likewise accompanied by planets.
-Speculation on extrasolar planetary systems In the 16th century the Italian philosopher Giordano Bruno, an early supporter of the Copernican theory that Earth and other planets orbit the Sun, put forward the view that the fixed stars are similar to the Sun and are likewise accompanied by planets. He was burned at the stake for his ideas by the Roman Inquisition.In the 18th century the same possibility was mentioned by Sir Isaac Newton in the ""General Scholium"" that concludes his Principia. Making a comparison to the Sun's planets, he wrote ""And if the fixed stars are the centres of similar systems, they will all be constructed according to a similar design and subject to the dominion of One.""His theories gained traction through the 19th and 20th centuries despite a lack of supporting evidence. Long before their confirmation by astronomers, conjecture on the nature of planetary systems had been a focus of the search for extraterrestrial intelligence and has been a prevalent theme in fiction, particularly science fiction.
-1584 – Giordano Bruno published two important philosophical dialogues (La Cena de le Ceneri and De l'infinito universo et mondi) in which he argued against the planetary spheres and affirmed the Copernican principle. Bruno's infinite universe was filled with a substance—a ""pure air"", aether, or spiritus—that offered no resistance to the heavenly bodies which, in Bruno's view, rather than being fixed, moved under their own impetus (momentum). Most dramatically, he completely abandoned the idea of a hierarchical universe. Bruno's cosmology distinguishes between ""suns"" which produce their own light and heat, and have other bodies moving around them; and ""earths"" which move around suns and receive light and heat from them. Bruno suggested that some, if not all, of the objects classically known as fixed stars are in fact suns, so he was arguably the first person to grasp that ""stars are other suns with their own planets."" Bruno wrote that other worlds ""have no less virtue nor a nature different from that of our Earth"" and, like Earth, ""contain animals and inhabitants"".
-Plato, Aristotle and other like Greek thinkers of antiquity, and later the Ptolemaic model of the cosmos showed an Earth-centered universe. Ptolemy was influential with his heavily mathematical work, the Almagest, which attempts to explain the peculiarity of stars that moved. These ""wandering stars"", planets, moved across the background of fixed stars which were spread along a sphere surrounding encompassing the universe. This geocentric view was held through the Middle Ages, and was later countered by subsequent astronomers and mathematicians alike, such as Nicolaus Copernicus and Johannes Kepler, who challenged the long-standing view of geocentrism and constructed a Sun-centered universe, this being known as the heliocentric system. The tradition of thought which appears in all of these systems of the universe, even with their divergent mechanisms, is the presence of the sphere of fixed stars.
-Early beliefs Prior to the Copernican Revolution, the Ptolemaic system, also known as the geocentric model, was widely accepted. This put the Earth at the center of the universe, with the Sun and other planets revolving around the Earth in an epicyclic orbit. Aristotle's geocentric model was also broadly acknowledged, along with his claim that the planets rotated but did not orbit. The reasoning behind this was due to the belief that all objects outside of the lunar sphere were celestial bodies, and therefore could not change, as they were made of quintessence.There were notable critiques of this model prior to Copernicus. In the Islamic world, Ibn al-Haytham doubted Ptolemy's notion of the planetary orbits, and Muhammad al-Battani recalculated the parameters. However, both still agreed with the geocentric model.One of the first known astronomers that supported the Heliocentric theory was Aristarchus of Samos. After observing a lunar eclipse, he came to the conclusion that the Sun was farther away from Earth than the Moon and that the Sun was much larger than Earth. He also claimed the Sun was a star. While Aristarchus was later an influence on Copernicus and his groundbreaking work, prior to the 17th century Aristarchus' findings were obstructed by the more established theories of Ptolemy and Aristotle.
 A. A German philosopher who supported the Keplerian theory that planets move in elliptical orbits around the Sun and believed that fixed stars are similar to the Sun but have different designs and are not subject to the dominion of One.
 B. An English philosopher who supported the Ptolemaic theory that Earth is the center of the universe and believed that fixed stars are not similar to the Sun and do not have planets orbiting them.
 C. A French philosopher who supported the Aristotelian theory that Earth is at the center of the universe and believed that fixed stars are similar to the Sun but do not have planets orbiting them.
 D. An Italian philosopher who supported the Copernican theory that Earth and other planets orbit the Sun and believed that fixed stars are similar to the Sun and have planets orbiting them.
 E. A Spanish philosopher who supported the Galilean theory that Earth and other planets orbit the Sun and believed that fixed stars are not similar to the Sun and do not have planets orbiting them. "
What are the Navier-Stokes equations?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What are the Navier-Stokes equations?
 Context: -The Navier–Stokes equations are a set of partial differential equations that describe the motion of fluids. They are given by: ∂v∂t+(v⋅∇)v=−1ρ∇p+ν∇2v+f ∇⋅v=0 where  v(x,t) is the velocity field of the fluid,  p(x,t) is the pressure,  ρ is the density,  ν is the kinematic viscosity, and  f(x,t) is an external force. The first equation is known as the momentum equation, and the second equation is known as the continuity equation.
-The Navier–Stokes equations (named after Claude-Louis Navier and George Gabriel Stokes) are differential equations that describe the force balance at a given point within a fluid. For an incompressible fluid with vector velocity field  u , the Navier–Stokes equations are ∂u∂t+(u⋅∇)u=−1ρ∇p+ν∇2u .These differential equations are the analogues for deformable materials to Newton's equations of motion for particles – the Navier–Stokes equations describe changes in momentum (force) in response to pressure  p and viscosity, parameterized by the kinematic viscosity  ν . Occasionally, body forces, such as the gravitational force or Lorentz force are added to the equations.
-In mathematics, the Navier–Stokes equations are a system of nonlinear partial differential equations for abstract vector fields of any size. In physics and engineering, they are a system of equations that model the motion of liquids or non-rarefied gases (in which the mean free path is short enough so that it can be thought of as a continuum mean instead of a collection of particles) using continuum mechanics. The equations are a statement of Newton's second law, with the forces modeled according to those in a viscous Newtonian fluid—as the sum of contributions by pressure, viscous stress and an external body force. Since the setting of the problem proposed by the Clay Mathematics Institute is in three dimensions, for an incompressible and homogeneous fluid, only that case is considered below.
-The Navier–Stokes equations are strictly a statement of the balance of momentum. To fully describe fluid flow, more information is needed, how much depending on the assumptions made. This additional information may include boundary data (no-slip, capillary surface, etc.), conservation of mass, balance of energy, and/or an equation of state.
-The Navier–Stokes equations mathematically express momentum balance and conservation of mass for Newtonian fluids. They are sometimes accompanied by an equation of state relating pressure, temperature and density. They arise from applying Isaac Newton's second law to fluid motion, together with the assumption that the stress in the fluid is the sum of a diffusing viscous term (proportional to the gradient of velocity) and a pressure term—hence describing viscous flow. The difference between them and the closely related Euler equations is that Navier–Stokes equations take viscosity into account while the Euler equations model only inviscid flow. As a result, the Navier–Stokes are a parabolic equation and therefore have better analytic properties, at the expense of having less mathematical structure (e.g. they are never completely integrable).
 A. The Navier-Stokes equations are partial differential equations that describe the motion of viscous fluid substances, expressing momentum balance and conservation of mass for Newtonian fluids.
 B. The Navier-Stokes equations are partial differential equations that describe the motion of viscous fluid substances, expressing momentum balance and conservation of mass for non-Newtonian fluids.
 C. The Navier-Stokes equations are partial differential equations that describe the motion of non-viscous fluid substances, expressing momentum balance and conservation of mass for Newtonian fluids.
 D. The Navier-Stokes equations are algebraic equations that describe the motion of non-viscous fluid substances, expressing momentum balance and conservation of mass for Newtonian fluids.
 E. The Navier-Stokes equations are algebraic equations that describe the motion of viscous fluid substances, expressing momentum balance and conservation of mass for Newtonian fluids. "
What is the revised view of the atmosphere's nature based on the time-varying multistability that is associated with the modulation of large-scale processes and aggregated feedback of small-scale processes?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the revised view of the atmosphere's nature based on the time-varying multistability that is associated with the modulation of large-scale processes and aggregated feedback of small-scale processes?
 Context: -Model for the nature of chaos and order in the atmosphere The scientific community accepts that the chaotic features found in low-dimensional Lorenz models could represent features of the Earth's atmosphere (), yielding the statement of “weather is chaotic.” By comparison, based on the concept of attractor coexistence within the generalized Lorenz model and the original Lorenz model (), Shen and his co-authors proposed a revised view that “weather possesses both chaos and order with distinct predictability”. The revised view, which is a build-up of the conventional view, is used to suggest that “the chaotic and regular features found in theoretical Lorenz models could better represent features of the Earth's atmosphere”.
-Atmospheric circulation is the large-scale movement of air and together with ocean circulation is the means by which thermal energy is redistributed on the surface of the Earth. The Earth's atmospheric circulation varies from year to year, but the large-scale structure of its circulation remains fairly constant. The smaller-scale weather systems – mid-latitude depressions, or tropical convective cells – occur chaotically, and long-range weather predictions of those cannot be made beyond ten days in practice, or a month in theory (see chaos theory and the butterfly effect).
-The dual nature with distinct predictability Over 50 years since Lorenz’s 1963 study and a follow-up presentation in 1972, the statement “weather is chaotic” has been well accepted. Such a view turns our attention from regularity associated with Laplace’s view of determinism to irregularity associated with chaos. In contrast to single-type chaotic solutions, recent studies using a generalized Lorenz model have focused on the coexistence of chaotic and regular solutions that appear within the same model using the same modeling configurations but different initial conditions. The results, with attractor coexistence, suggest that the entirety of weather possesses a dual nature of chaos and order with distinct predictability.Using a slowly varying, periodic heating parameter within a generalized Lorenz model, Shen and his co-authors suggested a revised view: “The atmosphere possesses chaos and order; it includes, as examples, emerging organized systems (such as tornadoes) and time varying forcing from recurrent seasons”.
-Climate networks enable insights into the dynamics of climate system over many spatial scales. The local degree centrality and related measures have been used to identify super-nodes and to associate them to known dynamical interrelations in the atmosphere, called teleconnection patterns. It was observed that climate networks possess “small world” properties owing to the long-range spatial connections.Steinhaeuser et al. applied complex networks to explore the multivariate and multi-scale dependence in climate data. Findings of the group suggested a close similarity of observed dependence patterns in multiple variables over multiple time and spatial scales.
-Atmosphere The atmospheric component of the CM2.X models employs a 24-level atmosphere with horizontal resolution of 2° in east–west and 2.5° in north–south directions. This resolution is sufficient to resolve the large mid-latitude cyclones responsible for weather variability. It is too coarse, however, to resolve processes such as hurricanes or intense thunderstorm outbreaks. The atmosphere includes a representation of radiative fluxes, mixing in the atmospheric boundary layer, representations of the impacts of stratus and cumulus clouds, a scheme for representing drag on upper level winds caused by gravity waves, changes in the spatial distribution of ozone and the ability to represent the impact of multiple greenhouse gases.
 A. The atmosphere is a system that is only influenced by large-scale processes and does not exhibit any small-scale feedback.
 B. The atmosphere possesses both chaos and order, including emerging organized systems and time-varying forcing from recurrent seasons.
 C. The atmosphere is a system that is only influenced by small-scale processes and does not exhibit any large-scale modulation.
 D. The atmosphere is a completely chaotic system with no order or organization.
 E. The atmosphere is a completely ordered system with no chaos or randomness. "
What is the reason that it is nearly impossible to see light emitted at the Lyman-alpha transition wavelength from a star farther than a few hundred light years from Earth?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the reason that it is nearly impossible to see light emitted at the Lyman-alpha transition wavelength from a star farther than a few hundred light years from Earth?
 Context: -Extinction provides one of the best ways of mapping the three-dimensional structure of the ISM, especially since the advent of accurate distances to millions of stars from the Gaia mission. The total amount of dust in front of each star is determined from its reddening, and the dust is then located along the line of sight by comparing the dust column density in front of stars projected close together on the sky, but at different distances. By 2022 it was possible to generate a map of ISM structures within 3 kpc (10,000 light years) of the Sun.Far ultraviolet light is absorbed effectively by the neutral hydrogen gas the ISM. Specifically, atomic hydrogen absorbs very strongly at about 121.5 nanometers, the Lyman-alpha transition, and also at the other Lyman series lines. Therefore, it is nearly impossible to see light emitted at those wavelengths from a star farther than a few hundred light years from Earth, because most of it is absorbed during the trip to Earth by intervening neutral hydrogen. All photons with wavelength < 91.6 nm, the Lyman limit, can ionize hydrogen and are also very strongly absorbed. The absorption gradually decreases with increasing photon energy, and the ISM begins to become transparent again in soft X-rays, with wavelengths shorter than about 1 nm.
-The ISM is generally very transparent to radio waves, allowing unimpeded observations right through the disk of the Galaxy. There are a few exceptions to this rule. The most intense spectral lines in the radio spectrum can become opaque, so that only the surface of the line-emitting cloud is visible. This mainly affects the carbon monoxide lines at millimetre wavelengths that are used to trace molecular clouds, but the 21-cm line from neutral hydrogen can become opaque in the cold neutral medium. Such absorption only affects photons at the line frequencies: the clouds are otherwise transparent. The other significant absorption process occurs in dense ionized regions. These emit photons, including radio waves, via thermal bremsstrahlung. At short wavelengths, typically microwaves, these are quite transparent, but their brightness approaches the black body limit as  2.1 , and at wavelengths long enough that this limit is reached, they become opaque. Thus metre-wavelength observations show H II regions as cool spots blocking the bright background emission from Galactic synchrotron radiation, while at decametres the entire galactic plane is absorbed, and the longest radio waves observed, 1 km, can only propagate 10-50 parsecs through the Local Bubble. The frequency at which a particular nebula becomes optically thick depends on its emission measure EM=∫ne2dl ,the column density of squared electron number density. Exceptionally dense nebulae can become optically thick at centimetre wavelengths: these are just-formed and so both rare and small ('Ultra-compact H II regions') The general transparency of the ISM to radio waves, especially microwaves, may seem surprising since radio waves at frequencies > 10 GHz are significantly attenuated by Earth's atmosphere (as seen in the figure). But the column density through the atmosphere is vastly larger than the column through the entire Galaxy, due to the extremely low density of the ISM.
-Since the hydrogen Lyman-alpha radiation is strongly absorbed by the air, its observation in laboratory requires use of vacuumed spectroscopic systems. For the same reason, Lyman-alpha astronomy is ordinarily carried out by satellite-borne instruments, except for observing extremely distant sources whose redshifts allow the line to penetrate the Earth atmosphere.
The line was also observed in antihydrogen. Within the experimental uncertainties, the measured frequency is equal to that of hydrogen, in agreement with predictions of quantum electrodynamics.
-The Hα line splitting is 44.5 nm. In similar white dwarfs an absorption line is expected to be seen instead, so that means the emission has sufficient energy to overpower any absorption. The emission was originally discovered by Jesse L. Greenstein. The original Hα line has a wavelength at 655.2 nm and is called the π component. The blue shifted component σ− has wavelength 633.4 nm and red shifted component line σ+ is at 678.2 nm.
-For a neutral hydrogen atom, spectral lines are formed when an electron transitions between energy levels. The Lyman series of spectral lines are produced by electrons transitioning between the ground state and higher energy levels (excited states). The Lyman-alpha transition corresponds to an electron transitioning between the ground state (n = 1) and the first excited state (n = 2). The Lyman-alpha spectral line has a laboratory wavelength (or rest wavelength) of 1216 Å, which is in the ultraviolet portion of the electromagnetic spectrum.The Lyman-alpha absorption lines in the quasar spectra result from intergalactic gas through which the galaxy or quasar's light has traveled. Since neutral hydrogen clouds in the intergalactic medium are at different degrees of redshift (due to their varying distance from Earth), their absorption lines are observed at a range of wavelengths. Each individual cloud leaves its fingerprint as an absorption line at a different position in the observed spectrum.
 A. Far ultraviolet light is absorbed effectively by the charged components of the ISM, including atomic helium, which has a typical absorption wavelength of about 121.5 nanometers, the Lyman-alpha transition.
 B. Far ultraviolet light is absorbed effectively by the neutral components of the ISM, including atomic hydrogen, which has a typical absorption wavelength of about 121.5 nanometers, the Lyman-alpha transition.
 C. Far ultraviolet light is absorbed effectively by the charged components of the ISM, including atomic hydrogen, which has a typical absorption wavelength of about 121.5 nanometers, the Lyman-alpha transition.
 D. Far ultraviolet light is absorbed effectively by the neutral components of the ISM, including atomic helium, which has a typical absorption wavelength of about 121.5 nanometers, the Lyman-alpha transition.
 E. Far ultraviolet light is absorbed effectively by the neutral components of the ISM, including atomic hydrogen, which has a typical absorption wavelength of about 212.5 nanometers, the Lyman-alpha transition. "
What is a Schwarzschild black hole?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is a Schwarzschild black hole?
 Context: -According to Birkhoff's theorem, the Schwarzschild metric is the most general spherically symmetric, vacuum solution of the Einstein field equations. A Schwarzschild black hole or static black hole is a black hole that has no charge or angular momentum. A Schwarzschild black hole is described by the Schwarzschild metric, and cannot be distinguished from any other Schwarzschild black hole except by its mass.
-Physical properties The simplest static black holes have mass but neither electric charge nor angular momentum. These black holes are often referred to as Schwarzschild black holes after Karl Schwarzschild who discovered this solution in 1916. According to Birkhoff's theorem, it is the only vacuum solution that is spherically symmetric. This means there is no observable difference at a distance between the gravitational field of such a black hole and that of any other spherical object of the same mass. The popular notion of a black hole ""sucking in everything"" in its surroundings is therefore correct only near a black hole's horizon; far away, the external gravitational field is identical to that of any other body of the same mass.Solutions describing more general black holes also exist. Non-rotating charged black holes are described by the Reissner–Nordström metric, while the Kerr metric describes a non-charged rotating black hole. The most general stationary black hole solution known is the Kerr–Newman metric, which describes a black hole with both charge and angular momentum.While the mass of a black hole can take any positive value, the charge and angular momentum are constrained by the mass. The total electric charge Q and the total angular momentum J are expected to satisfy the inequality Q24πϵ0+c2J2GM2≤GM2 for a black hole of mass M. Black holes with the minimum possible mass satisfying this inequality are called extremal. Solutions of Einstein's equations that violate this inequality exist, but they do not possess an event horizon. These solutions have so-called naked singularities that can be observed from the outside, and hence are deemed unphysical. The cosmic censorship hypothesis rules out the formation of such singularities, when they are created through the gravitational collapse of realistic matter. This is supported by numerical simulations.Due to the relatively large strength of the electromagnetic force, black holes forming from the collapse of stars are expected to retain the nearly neutral charge of the star. Rotation, however, is expected to be a universal feature of compact astrophysical objects. The black-hole candidate binary X-ray source GRS 1915+105 appears to have an angular momentum near the maximum allowed value. That uncharged limit is J≤GM2c, allowing definition of a dimensionless spin parameter such that 1.
-Rotating and charged black holes The Schwarzschild solution supposes an object that is not rotating in space and is not charged. To account for charge, the metric must satisfy the Einstein Field equations like before, as well as Maxwell's equations in a curved spacetime. A charged, non-rotating mass is described by the Reissner–Nordström metric.  Rotating black holes are described by the Kerr metric and the Kerr–Newman metric.
-In Einstein's theory of general relativity, the Schwarzschild metric (also known as the Schwarzschild solution) is an  exact solution to the Einstein field equations that describes the gravitational field outside a spherical mass, on the assumption that the electric charge of the mass, angular momentum of the mass, and universal cosmological constant are all zero. The solution is a useful approximation for describing slowly rotating astronomical objects such as many stars and planets, including Earth and the Sun. It was found by Karl Schwarzschild in 1916, and around the same time independently by Johannes Droste, who published his more complete and modern-looking discussion four months after Schwarzschild.According to Birkhoff's theorem, the Schwarzschild metric is the most general spherically symmetric vacuum solution of the Einstein field equations. A Schwarzschild black hole or static black hole is a black hole that has neither electric charge nor angular momentum. A Schwarzschild black hole is described by the Schwarzschild metric, and cannot be distinguished from any other Schwarzschild black hole except by its mass.
-In terms of these properties, the four types of black holes can be defined as follows: Note that astrophysical black holes are expected to have non-zero angular momentum, due to their formation via collapse of rotating stellar objects, but effectively zero charge, since any net charge will quickly attract the opposite charge and neutralize. For this reason the term ""astrophysical"" black hole is usually reserved for the Kerr black hole.
 A. A black hole that has mass but neither electric charge nor angular momentum, and is not spherically symmetric, according to Birkhoff's theorem.
 B. A black hole that has mass, electric charge, and angular momentum, and is spherically symmetric, according to Birkhoff's theorem.
 C. A black hole that has mass but neither electric charge nor angular momentum, and is spherically symmetric, according to Birkhoff's theorem.
 D. A black hole that has neither mass nor electric charge nor angular momentum, and is not spherically symmetric, according to Birkhoff's theorem.
 E. A black hole that has mass, electric charge, and angular momentum, and is not spherically symmetric, according to Birkhoff's theorem. "
What is the definition of Atomristor?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the definition of Atomristor?
 Context: -Atomristor Atomristor is defined as the electrical devices showing memristive behavior in atomically thin nanomaterials or atomic sheets. In 2018, Ge and Wu et al. in the Akinwande group at the University of Texas, first reported a universal memristive effect in single-layer TMD (MX2, M = Mo, W; and X = S, Se) atomic sheets based on vertical metal-insulator-metal (MIM) device structure. The work was later extended to monolayer hexagonal boron nitride, which is the thinnest memory material of around 0.33 nm. These atomristors offer forming-free switching and both unipolar and bipolar operation. The switching behavior is found in single-crystalline and poly-crystalline films, with various conducting electrodes (gold, silver and graphene). Atomically thin TMD sheets are prepared via CVD/MOCVD, enabling low-cost fabrication. Afterwards, taking advantage of the low ""on"" resistance and large on/off ratio, a high-performance zero-power RF switch is proved based on MoS2 or h-BN atomristors, indicating a new application of memristors for 5G, 6G and THz communication and connectivity systems. In 2020, atomistic understanding of the conductive virtual point mechanism was elucidated in an article in nature nanotechnology.
-Layered memristor In 2014, Bessonov et al. reported a flexible memristive device comprising a MoOx/MoS2 heterostructure sandwiched between silver electrodes on a plastic foil. The fabrication method is entirely based on printing and solution-processing technologies using two-dimensional layered transition metal dichalcogenides (TMDs). The memristors are mechanically flexible, optically transparent and produced at low cost. The memristive behaviour of switches was found to be accompanied by a prominent memcapacitive effect. High switching performance, demonstrated synaptic plasticity and sustainability to mechanical deformations promise to emulate the appealing characteristics of biological neural systems in novel computing technologies.
-A memristor (; a portmanteau of memory resistor) is a non-linear two-terminal electrical component relating electric charge and magnetic flux linkage. It was described and named in 1971 by Leon Chua, completing a theoretical quartet of fundamental electrical components which comprises also the resistor, capacitor and inductor.Chua and Kang later generalized the concept to memristive systems. Such a system comprises a circuit, of multiple conventional components, which mimics key properties of the ideal memristor component and is also commonly referred to as a memristor. Several such memristor system technologies have been developed, notably ReRAM.
-Carbon nanotube memristor In 2013, Ageev, Blinov et al. reported observing memristor effect in structure based on vertically aligned carbon nanotubes studying bundles of CNT by scanning tunneling microscope.
Later it was found that CNT memristive switching is observed when a nanotube has a non-uniform elastic strain ΔL0. It was shown that the memristive switching mechanism of strained СNT is based on the formation and subsequent redistribution of non-uniform elastic strain and piezoelectric field Edef in the nanotube under the influence of an external electric field E(x,t).
-According to the original 1971 definition, the memristor is the fourth fundamental circuit element, forming a non-linear relationship between electric charge and magnetic flux linkage. In 2011, Chua argued for a broader definition that includes all two-terminal non-volatile memory devices based on resistance switching. Williams argued that MRAM, phase-change memory and ReRAM are memristor technologies. Some researchers argued that biological structures such as blood and skin fit the definition. Others argued that the memory device under development by HP Labs and other forms of ReRAM are not memristors, but rather part of a broader class of variable-resistance systems, and that a broader definition of memristor is a scientifically unjustifiable land grab that favored HP's memristor patents.In 2011, Meuffels and Schroeder noted that one of the early memristor papers included a mistaken assumption regarding ionic conduction. In 2012, Meuffels and Soni discussed some fundamental issues and problems in the realization of memristors. They indicated inadequacies in the electrochemical modeling presented in the Nature article ""The missing memristor found"" because the impact of concentration polarization effects on the behavior of metal−TiO2−x−metal structures under voltage or current stress was not considered. This critique was referred to by Valov et al. in 2013.
 A. Atomristor is a flexible memristive device comprising a MoOx/MoS2 heterostructure sandwiched between silver electrodes on a plastic foil.
 B. Atomristor is a prominent memcapacitive effect observed in switches with memristive behavior.
 C. Atomristor is defined as the electrical devices showing memristive behavior in atomically thin nanomaterials or atomic sheets.
 D. Atomristor is a printing and solution-processing technology used to fabricate memristive devices.
 E. Atomristor is a type of two-dimensional layered transition metal dichalcogenides (TMDs) used in the fabrication of memristive devices. "
Who published the first theory that was able to encompass previously separate field theories to provide a unifying theory of electromagnetism?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: Who published the first theory that was able to encompass previously separate field theories to provide a unifying theory of electromagnetism?
 Context: -Classic theory The first successful classical unified field theory was developed by James Clerk Maxwell. In 1820, Hans Christian Ørsted discovered that electric currents exerted forces on magnets, while in 1831, Michael Faraday made the observation that time-varying magnetic fields could induce electric currents. Until then, electricity and magnetism had been thought of as unrelated phenomena. In 1864, Maxwell published his famous paper on a dynamical theory of the electromagnetic field. This was the first example of a theory that was able to encompass previously separate field theories (namely electricity and magnetism) to provide a unifying theory of electromagnetism. By 1905, Albert Einstein had used the constancy of the speed of light in Maxwell's theory to unify our notions of space and time into an entity we now call spacetime and in 1915 he expanded this theory of special relativity to a description of gravity, general relativity, using a field to describe the curving geometry of four-dimensional spacetime.
-Field theory had its origins in the 18th century in a mathematical formulation of Newtonian mechanics, but it was seen as deficient as it implied action at a distance. In 1852, Michael Faraday treated the magnetic field as a physical object, reasoning about lines of force. James Clerk Maxwell used Faraday's conceptualisation to help formulate his unification of electricity and magnetism in his electromagnetic theory.
-Maxwell and the theoretical prediction of electromagnetic waves Between 1861 and 1865, based on the earlier experimental work of Faraday and other scientists and on his own modification to Ampere's law, James Clerk Maxwell developed his theory of electromagnetism, which predicted the existence of electromagnetic waves. In 1873 Maxwell described the theoretical basis of the propagation of electromagnetic waves in his paper to the Royal Society, ""A Dynamical Theory of the Electromagnetic Field."" This theory united all previously unrelated observations, experiments and equations of electricity, magnetism, and optics into a consistent theory. His set of equations—Maxwell's equations—demonstrated that electricity, magnetism, and light are all manifestations of the same phenomenon, the electromagnetic field. Subsequently, all other classic laws or equations of these disciplines were special cases of Maxwell's equations. Maxwell's work in electromagnetism has been called the ""second great unification in physics"", after Newton's unification of gravity in the 17th century.Oliver Heaviside, later reformulated Maxwell's original equations into the set of four vector equations that are generally known today as Maxwell's equations. Neither Maxwell nor Heaviside transmitted or received radio waves; however, their equations for electromagnetic fields established principles for radio design, and remain the standard expression of classical electromagnetism.
-Electromagnetism is one of the fundamental forces of nature. Early on, electricity and magnetism were studied separately and regarded as separate phenomena. Hans Christian Ørsted discovered that the two were related – electric currents give rise to magnetism. Michael Faraday discovered the converse, that magnetism could induce electric currents, and James Clerk Maxwell put the whole thing together in a unified theory of electromagnetism. Maxwell's equations further indicated that electromagnetic waves existed, and the experiments of Heinrich Hertz confirmed this, making radio possible. Maxwell also postulated, correctly, that light was a form of electromagnetic wave, thus making all of optics a branch of electromagnetism. Radio waves differ from light only in that the wavelength of the former is much longer than the latter. Albert Einstein showed that the magnetic field arises through the relativistic motion of the electric field and thus magnetism is merely a side effect of electricity. The modern theoretical treatment of electromagnetism is as a quantum field in quantum electrodynamics.
-It was Hans Christian Ørsted who first proposed the connection between electricity and magnetism after observing the deflection of a compass needle by a nearby electric current. By the early 1830s Michael Faraday had demonstrated that magnetic fields and electricity could generate each other. In 1864 James Clerk Maxwell presented to the Royal Society a set of equations that described this relationship between electricity and magnetism. Maxwell's equations also predicted correctly that light is an electromagnetic wave. Starting with astronomy, the principles of natural philosophy crystallized into fundamental laws of physics which were enunciated and improved in the succeeding centuries. By the 19th century, the sciences had segmented into multiple fields with specialized researchers and the field of physics, although logically pre-eminent, no longer could claim sole ownership of the entire field of scientific research.
 A. Maxwell
 B. Einstein
 C. Galileo
 D. Faraday
 E. Newton "
What is the relevant type of coherence for the Young's double-slit interferometer?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the relevant type of coherence for the Young's double-slit interferometer?
 Context: -In some systems, such as water waves or optics, wave-like states can extend over one or two dimensions. Spatial coherence describes the ability for two spatial points x1 and x2 in the extent of a wave to interfere when averaged over time. More precisely, the spatial coherence is the cross-correlation between two points in a wave for all times. If a wave has only 1 value of amplitude over an infinite length, it is perfectly spatially coherent. The range of separation between the two points over which there is significant interference defines the diameter of the coherence area, Ac (Coherence length, often a feature of a source, is usually an industrial term related to the coherence time of the source, not the coherence area in the medium.) Ac is the relevant type of coherence for the Young's double-slit interferometer. It is also used in optical imaging systems and particularly in various types of astronomy telescopes. Sometimes people also use ""spatial coherence"" to refer to the visibility when a wave-like state is combined with a spatially shifted copy of itself.
-Young double slit experiment In Young's double slit experiment, light from a light source is allowed to pass through two pinholes separated by some distance, and a screen is placed some distance away from the pinholes where the interference between the light waves is observed (Figure. 1). Young's double slit experiment demonstrates the dependence of interference on coherence, specifically on the first-order correlation. This experiment is equivalent to the Mach–Zehnder interferometer with the caveat that Young's double slit experiment is concerned with spatial coherence, while the Mach–Zehnder interferometer relies on temporal coherence.The intensity measured at the position  r at time  t is ⟨I⟩=⟨|E+(r,t)|2⟩=⟨I⟩=I1+I2+2I1I2|γ(1)(x1,x2)|cosϕ(x1,x2) .Light field has highest degree of coherence when the corresponding interference pattern has the maximum contrast on the screen. The fringe contrast is defined as  V=Imax−IminImax+Imin Classically,  Iminmax=I1+I2±2I1I2|γ(1)(x1,x2)| and hence  V=2I1I2|γ(1)(x1,x2)|I1+I2 . As coherence is the ability to interfere visibility and coherence are linked: |γ(1)(x1,x2)|=1 means highest contrast, complete coherence 0<|γ(1)(x1,x2)|<1 means partial fringe visibility, partial coherence |γ(1)(x1,x2)|=0 means no contrast, complete incoherence.
-Coherence controls the visibility or contrast of interference patterns. For example visibility of the double slit experiment pattern requires that both slits be illuminated by a coherent wave as illustrated in the figure. Large sources without collimation or sources that mix many different frequencies will have lower visibility.: 264 Coherence contains several distinct concepts. Spatial coherence describes the correlation (or predictable relationship) between waves at different points in space, either lateral or longitudinal. Temporal coherence describes the correlation between waves observed at different moments in time. Both are observed in the Michelson–Morley experiment and Young's interference experiment. Once the fringes are obtained in the Michelson interferometer, when one of the mirrors is moved away gradually from the beam-splitter, the time for the beam to travel increases and the fringes become dull and finally disappear, showing temporal coherence. Similarly, in a double-slit experiment, if the space between the two slits is increased, the coherence dies gradually and finally the fringes disappear, showing spatial coherence. In both cases, the fringe amplitude slowly disappears, as the path difference increases past the coherence length.
-Interferometric visibility – which quantifies interference contrast in opticsPages displaying wikidata descriptions as a fallback Mutual coherence function Degree of coherence – Measurement in quantum optics Van Cittert–Zernike theorem Michelson stellar interferometer – astronomical instrument for measuring angular diameter of stars by means of interferometryPages displaying wikidata descriptions as a fallback Correlation interferometry – Astronomy devicePages displaying short descriptions of redirect targets Hanbury–Brown and Twiss effect – Quantum correlations related to wave-particle dualityPages displaying short descriptions of redirect targets Phase-contrast microscope – Optical microscopy techniquePages displaying short descriptions of redirect targets Englert–Greenberger duality relation – A relation of quantum opticsPages displaying short descriptions of redirect targets 
-The coherence length can also be measured using a Michelson interferometer and is the optical path length difference of a self-interfering laser beam which corresponds to  37 % fringe visibility, where the fringe visibility is defined as max min max min , where  I is the fringe intensity.
In long-distance transmission systems, the coherence length may be reduced by propagation factors such as dispersion, scattering, and diffraction.
 A. Visibility
 B. Coherence time
 C. Spatial coherence
 D. Coherence length
 E. Diameter of the coherence area (Ac) "
What is the Peierls bracket in canonical quantization?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the Peierls bracket in canonical quantization?
 Context: -In theoretical physics, the Peierls bracket is an equivalent description of the Poisson bracket. It can be defined directly from the action and does not require the canonical coordinates and their canonical momenta to be defined in advance.The bracket [A,B] is defined as DA(B)−DB(A) ,as the difference between some kind of action of one quantity on the other, minus the flipped term.
-The method does not apply to all possible actions (for instance, actions with a noncausal structure or actions with gauge ""flows""). It starts with the classical algebra of all (smooth) functionals over the configuration space. This algebra is quotiented over by the ideal generated by the Euler–Lagrange equations. Then, this quotient algebra is converted into a Poisson algebra by introducing a Poisson bracket derivable from the action, called the Peierls bracket. This Poisson algebra is then ℏ -deformed in the same way as in canonical quantization.
-In the Hamiltonian formulation of ordinary classical mechanics the Poisson bracket is an important concept. A ""canonical coordinate system"" consists of canonical position and momentum variables that satisfy canonical Poisson-bracket relations, where the Poisson bracket is given by for arbitrary phase space functions  f(qi,pj) and  g(qi,pj) . With the use of Poisson brackets, the Hamilton's equations can be rewritten as, These equations describe a ""flow"" or orbit in phase space generated by the Hamiltonian  H . Given any phase space function  F(q,p) , we have In canonical quantization the phase space variables are promoted to quantum operators on a Hilbert space and the Poisson bracket between phase space variables is replaced by the canonical commutation relation: In the so-called position representation this commutation relation is realized by the choice: and  The dynamics are described by Schrödinger equation: where  H^ is the operator formed from the Hamiltonian  H(q,p) with the replacement  q↦q and  p↦−iℏddq 
-In mathematics and classical mechanics, the Poisson bracket is an important binary operation in Hamiltonian mechanics, playing a central role in Hamilton's equations of motion, which govern the time evolution of a Hamiltonian dynamical system. The Poisson bracket also distinguishes a certain class of coordinate transformations, called canonical transformations, which map canonical coordinate systems into canonical coordinate systems. A ""canonical coordinate system"" consists of canonical position and momentum variables (below symbolized by  qi and  pi , respectively) that satisfy canonical Poisson bracket relations. The set of possible canonical transformations is always very rich. For instance, it is often possible to choose the Hamiltonian itself  H=H(q,p,t) as one of the new canonical momentum coordinates.
-In quantum mechanics, the Peierls bracket becomes a commutator i.e. a Lie bracket.
 A. The Peierls bracket is a mathematical symbol used to represent the Poisson algebra in the canonical quantization method.
 B. The Peierls bracket is a mathematical tool used to generate the Hamiltonian in the canonical quantization method.
 C. The Peierls bracket is a Poisson bracket derived from the action in the canonical quantization method that converts the quotient algebra into a Poisson algebra.
 D. The Peierls bracket is a mathematical symbol used to represent the quotient algebra in the canonical quantization method.
 E. The Peierls bracket is a mathematical tool used to generate the Euler-Lagrange equations in the canonical quantization method. "
What is the isophotal diameter used for in measuring a galaxy's size?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the isophotal diameter used for in measuring a galaxy's size?
 Context: -Isophotal diameter The isophotal diameter is introduced as a conventional way of measuring a galaxy's size based on its apparent surface brightness. Isophotes are curves in a diagram - such as a picture of a galaxy - that adjoins points of equal brightnesses, and are useful in defining the extent of the galaxy. The apparent brightness flux of a galaxy is measured in units of magnitudes per square arcsecond (mag/arcsec2; sometimes expressed as mag arcsec−2), which defines the brightness depth of the isophote. To illustrate how this unit works, a typical galaxy has a brightness flux of 18 mag/arcsec2 at its central region. This brightness is equivalent to the light of an 18th magnitude hypothetical point object (like a star) being spread out evenly in a one square arcsecond area of the sky. For the purposes of objectivity, the spectrum of light being used is sometimes also given in figures. As an example, the Milky Way has an average surface brightness of 22.1 B-mag/arcsec−2, where B-mag refers to the brightness at the B-band (445 nm wavelength of light, in the blue part of the visible spectrum).
-Examples of isophotal diameter measurements: Large Magellanic Cloud - 9.86 kiloparsecs (32,200 light-years) at the 25.0 B-mag/arcsec2 isophote.
Milky Way - has a diameter at the 25.0 B-mag/arcsec2 isophote of 26.8 ± 1.1 kiloparsecs (87,400 ± 3,590 light-years).
Messier 87 - has a has a diameter at the 25.0 B-mag/arcsec2 isophote of 40.55 kiloparsecs (132,000 light-years).
Andromeda Galaxy - has a has a diameter at the 25.0 B-mag/arcsec2 isophote of 46.56 kiloparsecs (152,000 light-years).
-This conventional standard, however, is not universally agreed upon. Erik Holmberg in 1958 measured the diameters of at least 300 galaxies at the isophote of about 26.5 mag/arcsec2 (originally defined as where the photographic brightness density with respect to plate background is 0.5%). Various other surveys such that of the ESO in 1989 use isophotes as faint as 27.0 mag/arcsec2. Nevertheless, corrections of these diameters were introduced by both the Second and Third Reference Catalogue of Galaxies (RC2 and RC3), at least to those galaxies being covered by the two catalogues.
-The extragalactic distance scale is a series of techniques used today by astronomers to determine the distance of cosmological bodies beyond our own galaxy, which are not easily obtained with traditional methods. Some procedures utilize properties of these objects, such as stars, globular clusters, nebulae, and galaxies as a whole. Other methods are based more on the statistics and probabilities of things such as entire galaxy clusters.
-In defining Re, it is necessary that the overall brightness flux galaxy should be captured, with a method employed by Bershady in 2000 suggesting to measure twice the size where the brightness flux of an arbitrarily chosen radius, defined as the local flux, divided by the overall average flux equals to 0.2. Using half-light radius allows a rough estimate of a galaxy's size, but is not particularly helpful in determining its morphology.Variations of this method exist. In particular, in the ESO-Uppsala Catalogue of Galaxies values of 50%, 70%, and 90% of the total blue light (the light detected through a B-band specific filter) had been used to calculate a galaxy's diameter.
 A. The isophotal diameter is a way of measuring a galaxy's distance from Earth.
 B. The isophotal diameter is a measure of a galaxy's age.
 C. The isophotal diameter is a measure of a galaxy's mass.
 D. The isophotal diameter is a measure of a galaxy's temperature.
 E. The isophotal diameter is a conventional way of measuring a galaxy's size based on its apparent surface brightness. "
What is the Maxwell's Demon thought experiment?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the Maxwell's Demon thought experiment?
 Context: -Maxwell's demon James Clerk Maxwell imagined one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other, separated by a wall. Observing the molecules on both sides, an imaginary demon guards a microscopic trapdoor in the wall. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.One response to this question was suggested in 1929 by Leó Szilárd and later by Léon Brillouin. Szilárd pointed out that a real-life Maxwell's demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Likewise, Brillouin demonstrated that the decrease in entropy caused by the demon would be less than the entropy produced by choosing molecules based on their speed.Maxwell's 'demon' repeatedly alters the permeability of the wall between A and B. It is therefore performing thermodynamic operations on a microscopic scale, not just observing ordinary spontaneous or natural macroscopic thermodynamic processes.
-Maxwell's demon is a thought experiment that would hypothetically violate the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a ""finite being"" or a ""being who can play a game of skill with the molecules"". Lord Kelvin would later call it a ""demon"".In the thought experiment, a demon controls a small massless door between two chambers of gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon's actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, without applying any work, thereby violating the second law of thermodynamics.
-Thought experiments In some cases a thought (or gedanken) experiment appears to suggest that perpetual motion may be possible through accepted and understood physical processes. However, in all cases, a flaw has been found when all of the relevant physics is considered. Examples include: Maxwell's demon: This was originally proposed to show that the Second Law of Thermodynamics applied in the statistical sense only, by postulating a ""demon"" that could select energetic molecules and extract their energy. Subsequent analysis (and experiment) have shown there is no way to physically implement such a system that does not result in an overall increase in entropy.
-This technique is widely described as a ""Maxwell's demon"" because it realizes Maxwell's process of creating a temperature difference by sorting high and low energy atoms into different containers. However, scientists have pointed out that it is not a true Maxwell's demon in the sense that it does not violate the second law of thermodynamics; it does not result in a net decrease in entropy and cannot be used to produce useful energy. This is because the process requires more energy from the laser beams than could be produced by the temperature difference generated. The atoms absorb low entropy photons from the laser beam and emit them in a random direction, thus increasing the entropy of the environment.In 2014, Pekola et al. demonstrated an experimental realization of a Szilárd engine. Only a year later and based on an earlier theoretical proposal, the same group presented the first experimental realization of an autonomous Maxwell's demon, which extracts microscopic information from a system and reduces its entropy by applying feedback. The demon is based on two capacitively coupled single-electron devices, both integrated on the same electronic circuit. The operation of the demon is directly observed as a temperature drop in the system, with a simultaneous temperature rise in the demon arising from the thermodynamic cost of generating the mutual information. In 2016, Pekola et al. demonstrated a proof-of-principle of an autonomous demon in coupled single-electron circuits, showing a way to cool critical elements in a circuit with information as a fuel. Pekola et al. have also proposed that a simple qubit circuit, e.g., made of a superconducting circuit, could provide a basis to study a quantum Szilard's engine.
-The thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell's 1872 book on thermodynamics titled Theory of Heat.In his letters and books, Maxwell described the agent opening the door between the chambers as a ""finite being"". William Thomson (Lord Kelvin) was the first to use the word ""demon"" for Maxwell's concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.
 A. A thought experiment in which a demon guards a microscopic trapdoor in a wall separating two parts of a container filled with different gases at equal temperatures. The demon selectively allows molecules to pass from one side to the other, causing an increase in temperature in one part and a decrease in temperature in the other, contrary to the second law of thermodynamics.
 B. A thought experiment in which a demon guards a macroscopic trapdoor in a wall separating two parts of a container filled with different gases at different temperatures. The demon selectively allows molecules to pass from one side to the other, causing a decrease in temperature in one part and an increase in temperature in the other, in accordance with the second law of thermodynamics.
 C. A thought experiment in which a demon guards a microscopic trapdoor in a wall separating two parts of a container filled with the same gas at equal temperatures. The demon selectively allows faster-than-average molecules to pass from one side to the other, causing a decrease in temperature in one part and an increase in temperature in the other, contrary to the second law of thermodynamics.
 D. A thought experiment in which a demon guards a macroscopic trapdoor in a wall separating two parts of a container filled with the same gas at equal temperatures. The demon selectively allows faster-than-average molecules to pass from one side to the other, causing an increase in temperature in one part and a decrease in temperature in the other, contrary to the second law of thermodynamics.
 E. A thought experiment in which a demon guards a microscopic trapdoor in a wall separating two parts of a container filled with the same gas at different temperatures. The demon selectively allows slower-than-average molecules to pass from one side to the other, causing a decrease in temperature in one part and an increase in temperature in the other, in accordance with the second law of thermodynamics. "
What is the application of Memristor?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the application of Memristor?
 Context: -Memristors remain a laboratory curiosity, as yet made in insufficient numbers to gain any commercial applications. Despite this lack of mass availability, according to Allied Market Research the memristor market was worth $3.2 million in 2015 and will be worth $79.0 million by 2022.A potential application of memristors is in analog memories for superconducting quantum computers.Memristors can potentially be fashioned into non-volatile solid-state memory, which could allow greater data density than hard drives with access times similar to DRAM, replacing both components. HP prototyped a crossbar latch memory that can fit 100 gigabits in a square centimeter, and proposed a scalable 3D design (consisting of up to 1000 layers or 1 petabit per cm3). In May 2008 HP reported that its device reaches currently about one-tenth the speed of DRAM. The devices' resistance would be read with alternating current so that the stored value would not be affected. In May 2012, it was reported that the access time had been improved to 90 nanoseconds, which is nearly one hundred times faster than the contemporaneous Flash memory. At the same time, the energy consumption was just one percent of that consumed by Flash memory.Memristor have applications in programmable logic signal processing, Super-resolution imaging physical neural networks, control systems, reconfigurable computing, in-memory computing, brain–computer interfaces and RFID. Memristive devices are potentially used for stateful logic implication, allowing a replacement for CMOS-based logic computation Several early works have been reported in this direction.In 2009, a simple electronic circuit consisting of an LC network and a memristor was used to model experiments on adaptive behavior of unicellular organisms. It was shown that subjected to a train of periodic pulses, the circuit learns and anticipates the next pulse similar to the behavior of slime molds Physarum polycephalum where the viscosity of channels in the cytoplasm responds to periodic environment changes. Applications of such circuits may include, e.g., pattern recognition. The DARPA SyNAPSE project funded HP Labs, in collaboration with the Boston University Neuromorphics Lab, has been developing neuromorphic architectures which may be based on memristive systems. In 2010, Versace and Chandler described the MoNETA (Modular Neural Exploring Traveling Agent) model. MoNETA is the first large-scale neural network model to implement whole-brain circuits to power a virtual and robotic agent using memristive hardware. Application of the memristor crossbar structure in the construction of an analog soft computing system was demonstrated by Merrikh-Bayat and Shouraki. In 2011, they showed how memristor crossbars can be combined with fuzzy logic to create an analog memristive neuro-fuzzy computing system with fuzzy input and output terminals. Learning is based on the creation of fuzzy relations inspired from Hebbian learning rule.
-Neuromemristive systems Neuromemristive systems is a subclass of neuromorphic computing systems that focuses on the use of memristors to implement neuroplasticity. While neuromorphic engineering focuses on mimicking biological behavior, neuromemristive systems focus on abstraction. For example, a neuromemristive system may replace the details of a cortical microcircuit's behavior with an abstract neural network model.There exist several neuron inspired threshold logic functions implemented with memristors that have applications in high level pattern recognition applications. Some of the applications reported recently include speech recognition, face recognition and object recognition. They also find applications in replacing conventional digital logic gates.For (quasi)ideal passive memristive circuits, the evolution of the memristive memories can be written in a closed form (Caravelli-Traversa-Di Ventra equation): ddtX→=−αX→+1β(I−χΩX)−1ΩS→ as a function of the properties of the physical memristive network and the external sources. The equation is valid for the case of the Williams-Strukov original toy model, as in the case of ideal memristors,  α=0 . However, the hypothesis of the existence of an ideal memristor is debatable. In the equation above,  α is the ""forgetting"" time scale constant, typically associated to memory volatility, while  off on off is the ratio of off and on values of the limit resistances of the memristors,  S→ is the vector of the sources of the circuit and  Ω is a projector on the fundamental loops of the circuit. The constant  β has the dimension of a voltage and is associated to the properties of the memristor; its physical origin is the charge mobility in the conductor. The diagonal matrix and vector  diag ⁡(X→) and  X→ respectively, are instead the internal value of the memristors, with values between 0 and 1. This equation thus requires adding extra constraints on the memory values in order to be reliable.
-Memory storage Electronic memory designs in the past have largely relied on the formation of transistors. However, research into crossbar switch based electronics have offered an alternative using reconfigurable interconnections between vertical and horizontal wiring arrays to create ultra high density memories. Two leaders in this area are Nantero which has developed a carbon nanotube based crossbar memory called Nano-RAM and Hewlett-Packard which has proposed the use of memristor material as a future replacement of Flash memory.An example of such novel devices is based on spintronics. The dependence of the resistance of a material (due to the spin of the electrons) on an external field is called magnetoresistance. This effect can be significantly amplified (GMR - Giant Magneto-Resistance) for nanosized objects, for example when two ferromagnetic layers are separated by a nonmagnetic layer, which is several nanometers thick (e.g. Co-Cu-Co). The GMR effect has led to a strong increase in the data storage density of hard disks and made the gigabyte range possible. The so-called tunneling magnetoresistance (TMR) is very similar to GMR and based on the spin dependent tunneling of electrons through adjacent ferromagnetic layers. Both GMR and TMR effects can be used to create a non-volatile main memory for computers, such as the so-called magnetic random access memory or MRAM.
-Memristive neural network Greg Snider of HP Labs describes a system of cortical computing with memristive nanodevices. The memristors (memory resistors) are implemented by thin film materials in which the resistance is electrically tuned via the transport of ions or oxygen vacancies within the film. DARPA's SyNAPSE project has funded IBM Research and HP Labs, in collaboration with the Boston University Department of Cognitive and Neural Systems (CNS), to develop neuromorphic architectures which may be based on memristive systems.
-Following this claim, Leon Chua has argued that the memristor definition could be generalized to cover all forms of two-terminal non-volatile memory devices based on resistance switching effects. Chua also argued that the memristor is the oldest known circuit element, with its effects predating the resistor, capacitor, and inductor. There are, however, some serious doubts as to whether a genuine memristor can actually exist in physical reality. Additionally, some experimental evidence contradicts Chua's generalization since a non-passive nanobattery effect is observable in resistance switching memory. A simple test has been proposed by Pershin and Di Ventra to analyze whether such an ideal or generic memristor does actually exist or is a purely mathematical concept. Up to now, there seems to be no experimental resistance switching device (ReRAM) which can pass the test.These devices are intended for applications in nanoelectronic memory devices, computer logic, and neuromorphic/neuromemristive computer architectures. In 2013, Hewlett-Packard CTO Martin Fink suggested that memristor memory may become commercially available as early as 2018. In March 2012, a team of researchers from HRL Laboratories and the University of Michigan announced the first functioning memristor array built on a CMOS chip.
 A. Memristor has applications in the production of electric cars, airplanes, and ships.
 B. Memristor has applications in the production of food, clothing, and shelter.
 C. Memristor has applications in the production of solar panels, wind turbines, and hydroelectric power plants.
 D. Memristor has applications in programmable logic signal processing, Super-resolution imaging, physical neural networks, control systems, reconfigurable computing, in-memory computing, brain–computer interfaces and RFID.
 E. Memristor has applications in optical fiber communication, satellite communication, and wireless communication. "
What is the effect generated by a spinning superconductor?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the effect generated by a spinning superconductor?
 Context: -London moment Conversely, a spinning superconductor generates a magnetic field, precisely aligned with the spin axis. The effect, the London moment, was put to good use in Gravity Probe B. This experiment measured the magnetic fields of four superconducting gyroscopes to determine their spin axes. This was critical to the experiment since it is one of the few ways to accurately determine the spin axis of an otherwise featureless sphere.
-Magnetization Although this magnetic flux distribution seems somewhat counter-intuitive to those familiar with simple bar magnets or solenoids, the reason for this flux distribution can be intuitively visualised using Mallinson's original diagram (note that it uses the negative y component, unlike the diagram in Mallinson's article). The diagram shows the field from a strip of ferromagnetic material with alternating magnetization in the y direction (top left) and in the x direction (top right). Note that the field above the plane is in the same direction for both structures, but the field below the plane is in opposite directions. The effect of superimposing both of these structures is shown in the figure.
-Superconductors can enhance central effects in spintronics such as magnetoresistance effects, spin lifetimes and dissipationless spin-currents.The simplest method of generating a spin-polarised current in a metal is to pass the current through a ferromagnetic material. The most common applications of this effect involve giant magnetoresistance (GMR) devices. A typical GMR device consists of at least two layers of ferromagnetic materials separated by a spacer layer. When the two magnetization vectors of the ferromagnetic layers are aligned, the electrical resistance will be lower (so a higher current flows at constant voltage) than if the ferromagnetic layers are anti-aligned. This constitutes a magnetic field sensor.
-The magnetic field strength associated with a rotating superconductor is given by: B=−2MQω, where M and Q are the mass and the charge of the superconducting charge carriers respectively. For the case of Cooper pairs of electrons, M = 2me and Q = 2e. Despite the electrons existing in a strongly interacting environment, me denotes here the mass of the bare electrons (as in vacuum), and not e.g. the effective mass of conducting electrons of the normal phase.
-In 1992 Evgeny Podkletnov published a heavily debated journal article claiming a specific type of rotating superconductor could shield gravitational force. Independently of this, from 1991 to 1993 Ning Li and Douglas Torr published a number of articles about gravitational effects in superconductors. One finding they derived is the source of gravitomagnetic flux in a type II superconductor material is due to spin alignment of the lattice ions. Quoting from their third paper: ""It is shown that the coherent alignment of lattice ion spins will generate a detectable gravitomagnetic field, and in the presence of a time-dependent applied magnetic vector potential field, a detectable gravitoelectric field."" The claimed size of the generated force has been disputed by some but defended by others. In 1997 Li published a paper attempting to replicate Podkletnov's results and showed the effect was very small, if it existed at all. Li is reported to have left the University of Alabama in 1999 to found the company AC Gravity LLC. AC Gravity was awarded a U.S. DOD grant for $448,970 in 2001 to continue anti-gravity research. The grant period ended in 2002 but no results from this research were ever made public.In 2002 Phantom Works, Boeing's advanced research and development facility in Seattle, approached Evgeny Podkletnov directly. Phantom Works was blocked by Russian technology transfer controls. At this time Lieutenant General George Muellner, the outgoing head of the Boeing Phantom Works, confirmed that attempts by Boeing to work with Podkletnov had been blocked by Moscow, also commenting that ""The physical principles – and Podkletnov's device is not the only one – appear to be valid... There is basic science there. They're not breaking the laws of physics. The issue is whether the science can be engineered into something workable""Froning and Roach (2002) put forward a paper that builds on the work of Puthoff, Haisch and Alcubierre. They used fluid dynamic simulations to model the interaction of a vehicle (like that proposed by Alcubierre) with the zero-point field. Vacuum field perturbations are simulated by fluid field perturbations and the aerodynamic resistance of viscous drag exerted on the interior of the vehicle is compared to the Lorentz force exerted by the zero-point field (a Casimir-like force is exerted on the exterior by unbalanced zero-point radiation pressures). They find that the optimized negative energy required for an Alcubierre drive is where it is a saucer-shaped vehicle with toroidal electromagnetic fields. The EM fields distort the vacuum field perturbations surrounding the craft sufficiently to affect the permeability and permittivity of space.
 A. An electric field, precisely aligned with the spin axis.
 B. A magnetic field, randomly aligned with the spin axis.
 C. A magnetic field, precisely aligned with the spin axis.
 D. A gravitational field, randomly aligned with the spin axis.
 E. A gravitational field, precisely aligned with the spin axis. "
What is the main focus of cryogenic and noble liquid detectors in dark matter experiments?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the main focus of cryogenic and noble liquid detectors in dark matter experiments?
 Context: -Noble gas scintillators Noble gas scintillators use the property of certain materials to scintillate, which is when a material absorbs energy from a particle and remits the same amount of energy as light. Of particular interest for dark matter detection is the use of noble gases, even more specifically liquid xenon.  The XENON series of experiments, also located at the Gran Sasso National Lab, is a forefront user of liquid xenon scintillators. Common across all generations of the experiment, the detector consists of a tank of liquid xenon with a gaseous layer on top. At the top and bottom of the detector is a layer of photomultiplier tubes (PMTs). When a dark matter particle collides with the liquid xenon, it rapidly releases a photon which is detected by the PMTs. To cross reference this data point an electric field is applied which is sufficiently large to prevent complete recombination of the electrons knocked loose by the interaction. These drift to the top of the detector and are also detected, creating two separate detections for each event. Measuring the time delay between these allows for a complete 3-D reconstruction of the interaction. The detector is also able to discriminate between electronic recoils and nuclear recoils, as both types of events would produce differing ratios of the photon energy and the released electron energy.  The most recently completed version of the XENON experiment is XENON1T, which used 3.2 tons of liquid xenon. This experiment produced a then record limit for the cross section of WIMP dark matter of 4.1×10−47 cm2 at a mass of 30 GeV/c2. The most recent iteration of the XENON succession is XENONnT, which is currently running with 8 tones of liquid xenon. This experiment is projected to be able to probe WIMP-nucleon cross sections of 1.4×10−48 cm2 for a 50 GeV/c2 WIMP mass. At this ultra-low cross section, interference from the background neutrino flux is predicted to be problematic.
-These experiments mostly use either cryogenic or noble liquid detector technologies. Cryogenic detectors operating at temperatures below 100 mK, detect the heat produced when a particle hits an atom in a crystal absorber such as germanium. Noble liquid detectors detect scintillation produced by a particle collision in liquid xenon or argon. Cryogenic detector experiments include: CDMS, CRESST, EDELWEISS, EURECA. Noble liquid experiments include LZ, XENON, DEAP, ArDM, WARP, DarkSide, PandaX, and LUX, the Large Underground Xenon experiment. Both of these techniques focus strongly on their ability to distinguish background particles (which predominantly scatter off electrons) from dark matter particles (that scatter off nuclei). Other experiments include SIMPLE and PICASSO.
-Cryogenic dark matter experiments use particle detectors operating at millikelvin temperatures to search for the elastic scattering of WIMPs of an atomic nuclei. A particle interaction inside an absorber crystal will create a large number of phonons, these thermalise inside a thermometer on the crystal surface, which records the rise in temperature. Such cryogenic detectors are used as they combine a high sensitivity with a low energy threshold and excellent resolution.
-Since liquid argon is a scintillating material a particle interacting with it produces light in proportion to the energy deposited from the incident particle, this is a linear effect for low energies before quenching becomes a major contributing factor. The interaction of a particle with the argon causes ionization and recoiling along the path of interaction. The recoiling argon nuclei undergo recombination or self-trapping, ultimately resulting in the emission of 128nm vacuum ultra-violet (VUV) photons. Additionally liquid argon has the unique property of being transparent to its own scintillation light, this allows for light yields of tens of thousands of photons produced for every MeV of energy deposited.  The elastic scattering of a WIMP dark matter particle with an argon nucleus is expected to cause the nucleus to recoil. This is expected to be a very low energy interaction (keV) and requires a low detection threshold in order to be sensitive. Due to the necessarily low detection threshold, the number of background events detected is very high. The faint signature of a dark matter particle such as a WIMP will be masked by the many different types of possible background events. A technique for identifying these non-dark matter events is pulse shape discrimination (PSD), which characterizes an event based on the timing signature of the scintillation light from liquid argon.  PSD is possible in a liquid argon detector because interactions due to different incident particles such as electrons, high energy photons, alphas, and neutrons create different proportions of excited states of the recoiling argon nuclei, these are known as singlet and triplet states and they decay with characteristic lifetimes of 6 ns and 1300 ns respectively. Interactions from gammas and electrons produce primarily triplet excited states through electronic recoils, while neutron and alpha interactions produce primarily singlet excited states through nuclear recoils. It is expected that WIMP-nucleon interactions also produce a nuclear recoil type signal due to the elastic scattering of the dark matter particle with the argon nucleus.  By using the arrival time distribution of light for an event, it is possible to identify its likely source. This is done quantitatively by measuring the ratio of the light measured by the photo-detectors in a ""prompt"" window (<60 ns) over the light measured in a ""late"" window (<10,000 ns). In DEAP this parameter is called Fprompt. Nuclear recoil type events have high Fprompt (~0.7) values while electronic recoil events have a low Fprompt value (~0.3). Due to this separation in Fprompt for WIMP-like (Nuclear Recoil) and background-like (Electronic Recoil) events, it is possible to uniquely identify the most dominant sources of background in the detector.The most abundant background in DEAP comes from the beta decay of Argon-39 which has an activity of approximately 1 Bq/kg in atmospheric argon. Discrimination of beta and gamma background events from nuclear recoils in the energy region of interest (near 20 keV of electron energy) is required to be better than 1 in 108 to sufficiently suppress these backgrounds for a dark matter search in liquid atmospheric argon.
-Condensed noble gases, most notably liquid xenon and liquid argon, are excellent radiation detection media. They can produce two signatures for each particle interaction: a fast flash of light (scintillation) and the local release of charge (ionisation). In two-phase xenon – so called since it involves liquid and gas phases in equilibrium – the scintillation light produced by an interaction in the liquid is detected directly with photomultiplier tubes; the ionisation electrons released at the interaction site are drifted up to the liquid surface under an external electric field, and subsequently emitted into a thin layer of xenon vapour. Once in the gas, they generate a second, larger pulse of light (electroluminescence or proportional scintillation), which is detected by the same array of photomultipliers. These systems are also known as xenon 'emission detectors'.This configuration is that of a time projection chamber (TPC); it allows three-dimensional reconstruction of the interaction site, since the depth coordinate (z) can be measured very accurately from the time separation between the two light pulses. The horizontal coordinates can be reconstructed from the hit pattern in the photomultiplier array(s). Critically for WIMP searches, the ratio between the two response channels (scintillation and ionisation) allows the rejection of the predominant backgrounds for WIMP searches: gamma and beta radiation from trace radioactivity in detector materials and the immediate surroundings. WIMP candidate events produce lower ionisation/scintillation ratios than the more prevalent background interactions.
 A. Distinguishing background particles from dark matter particles by detecting the heat produced when a particle hits an atom in a crystal absorber or the scintillation produced by a particle collision in liquid xenon or argon.
 B. Detecting the heat produced when a particle hits an atom in a crystal absorber or the scintillation produced by a particle collision in liquid xenon or argon to determine the mass and interaction cross section with electrons of dark matter particles.
 C. Detecting the heat produced when a particle hits an atom in a crystal absorber or the scintillation produced by a particle collision in liquid xenon or argon to distinguish between different types of background particles.
 D. Detecting the heat produced when a particle hits an atom in a crystal absorber or the scintillation produced by a particle collision in liquid xenon or argon to determine the mass and interaction cross section with nucleons of dark matter particles.
 E. Detecting the mass and interaction cross section with nucleons of dark matter particles by detecting the heat produced when a particle hits an atom in a crystal absorber or the scintillation produced by a particle collision in liquid xenon or argon. "
What is a pycnometer?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is a pycnometer?
 Context: -A gas pycnometer is a laboratory device used for measuring the density—or, more accurately, the volume—of solids, be they regularly shaped, porous or non-porous, monolithic, powdered, granular or in some way comminuted, employing some method of gas displacement and the volume:pressure relationship known as Boyle's Law. A gas pycnometer is also sometimes referred to as a helium pycnometer.
-Volume vs density While pycnometers (of any type) are recognized as density measuring devices they are in fact devices for measuring volume only. Density is merely calculated as the ratio of mass to volume; mass being invariably measured on a discrete device, usually by weighing. The volume measured in a gas pycnometer is that amount of three-dimensional space which is inaccessible to the gas used, i.e. that volume within the sample chamber from which the gas is excluded. Therefore, the volume measured considering the finest scale of surface roughness will depend on the atomic or molecular size of the gas. Helium therefore is most often prescribed as the measurement gas, not only is it of small size, it is also inert and the most ideal gas.
-Pycnometer A pycnometer (from Ancient Greek: πυκνός, romanized: puknos, lit. 'dense'), also called pyknometer or specific gravity bottle, is a device used to determine the density of a liquid. A pycnometer is usually made of glass, with a close-fitting ground glass stopper with a capillary tube through it, so that air bubbles may escape from the apparatus. This device enables a liquid's density to be measured accurately by reference to an appropriate working fluid, such as water or mercury, using an analytical balance.If the flask is weighed empty, full of water, and full of a liquid whose relative density is desired, the relative density of the liquid can easily be calculated. The particle density of a powder, to which the usual method of weighing cannot be applied, can also be determined with a pycnometer. The powder is added to the pycnometer, which is then weighed, giving the weight of the powder sample. The pycnometer is then filled with a liquid of known density, in which the powder is completely insoluble. The weight of the displaced liquid can then be determined, and hence the relative density of the powder.
-Volumetric measurement A gas pycnometer can be used to measure the volume of a powder sample. A sample of known mass is loaded into a chamber of known volume that is connected by a closed valve to a gas reservoir, also of known volume, at a higher pressure than the chamber. After the valve is opened, the final pressure in the system allows the total gas volume to be determined by application of Boyle's law.
-Pycnometer is the preferred spelling in modern American English usage. Pyknometer is to be found in older texts, and is used interchangeably with pycnometer in British English. The term has its origins in the Greek word πυκνός, meaning ""dense"".
The density calculated from a volume measured using a gas pycnometer is often referred to as skeletal density, true density or helium density.
For non-porous solids a pycnometer can be used to measure particle density.
An extreme example of the gas displacement principle for volume measurement is described in U.S. Patent 5,231,873 (Lindberg, 1993) wherein a chamber large enough to hold a flatbed truck is used to measure the volume of a load of timber.
 A. A device used to measure the density of a gas.
 B. A device used to measure the mass of a liquid.
 C. A device used to measure the volume of a gas.
 D. A device used to determine the density of a liquid.
 E. A device used to determine the volume of a liquid. "
"What is the estimated redshift of CEERS-93316, a candidate high-redshift galaxy observed by the James Webb Space Telescope?","Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the estimated redshift of CEERS-93316, a candidate high-redshift galaxy observed by the James Webb Space Telescope?
 Context: -MIRI low-resolution spectroscopy (LRS): a hot super-Earth planet L 168-9 b (TOI-134) around a bright M-dwarf starWithin two weeks of the first Webb images, several preprint papers described a wide range of high redshift and very luminous (presumably large) galaxies believed to date from 235 million years (z=16.7) to 280 million years after the Big Bang, far earlier than previously known. On 17 August 2022, NASA released a large mosaic image of 690 individual frames taken by the Near Infrared Camera (NIRCam) on Webb of numerous very early galaxies. Some early galaxies observed by Webb like CEERS-93316, which has an estimated redshift of approximately z=16.7 corresponding to 235.8 million years after the Big Bang, are high redshift galaxy candidates. In September 2022, primordial black holes were proposed as explaining these unexpectedly large and early galaxies.In June 2023 detection of organic molecules 12 billion light-years away in a galaxy called SPT0418-47 using the Webb telescope was announced.On 12 July 2023, NASA celebrated the first year of operations with the release of Webb’s image of a small star-forming region in the Rho Ophiuchi cloud complex, 390 light years away.
-Redshift (z) can be expressed by the following equations: In these equations, frequency is denoted by  f and wavelength by  λ . The larger the value of z, the more redshifted the light and the farther away the object is from the Earth. As of January 2013, the largest galaxy redshift of z~12 was found using the Hubble Ultra-Deep Field, corresponding to an age of over 13 billion years (the universe is approximately 13.82 billion years old).The Doppler effect and Hubble's law can be combined to form the equation Hubble c where c is the speed of light.
-The decoupling, or the last scattering, is thought to have occurred about 300,000 years after the Big Bang, or at a redshift of about  1100 . We can determine both the approximate angular diameter of the universe and the physical size of the particle horizon that had existed at this time.  The angular diameter distance, in terms of redshift  z , is described by  dA(z)=r(z)/(1+z) . If we assume a flat cosmology then,  r(z)=∫temt0dta(t)=∫aem1daa2H(a)=∫0zdzH(z).
-While some scientists have claimed other objects (such as Abell 1835 IR1916) have higher redshifts (and therefore are seen in an earlier stage of the universe's evolution), IOK-1's age and composition have been more reliably established. In December 2012, astronomers reported that UDFj-39546284 is the most distant object known and has a redshift value of 11.9. The object, estimated to have existed around 380 million years after the Big Bang (which was about 13.8 billion years ago), is about 13.42 billion light travel distance years away. The existence of galaxies so soon after the Big Bang suggests that protogalaxies must have grown in the so-called ""dark ages"". As of May 5, 2015, the galaxy EGS-zs8-1 is the most distant and earliest galaxy measured, forming 670 million years after the Big Bang. The light from EGS-zs8-1 has taken 13 billion years to reach Earth, and is now 30 billion light-years away, because of the expansion of the universe during 13 billion years. On 17 August 2022, NASA released a large mosaic image of 690 individual frames taken by the Near Infrared Camera (NIRCam) on the James Webb Space Telescope (JWST) of numerous very early galaxies.In May 2023, a study in the journal Nature identified an ultra-faint galaxy named JD1. Galaxy JD1 was observed by the JWST using the near-infrared spectrograph instrument NIRSpec and was found to have a distance value of redshift z=9.79. This means that JD1 was observed at 480 million years after the Big Bang when the universe was only about 4% of its present age. Observations of this ultra-faint galaxy were aided by the effect of a gravitational lens in the galaxy cluster Abell 2744 which helped make the image of JD1 larger and 13 times brighter than it otherwise would be. This effect and the use of the JWST's NIRCam showed JD1's structure to be three starforming clumps of dust and gas. One of the authors of the study Tommaso Treu said: ""The combination of JWST and the magnifying power of gravitational lensing is a revolution. We are rewriting the book on how galaxies formed and evolved in the immediate aftermath of the Big Bang."" The detailed process by which the earliest galaxies formed is an open question in astrophysics. Theories can be divided into two categories: top-down and bottom-up. In top-down correlations (such as the Eggen–Lynden-Bell–Sandage [ELS] model), protogalaxies form in a large-scale simultaneous collapse lasting about one hundred million years. In bottom-up theories (such as the Searle-Zinn [SZ] model), small structures such as globular clusters form first, and then a number of such bodies accrete to form a larger galaxy.
-Other large numbers, as regards length and time, are found in astronomy and cosmology. For example, the current Big Bang model suggests that the universe is 13.8 billion years (4.355 × 1017 seconds) old, and that the observable universe is 93 billion light years across (8.8 × 1026 metres), and contains about 5 × 1022 stars, organized into around 125 billion (1.25 × 1011) galaxies, according to Hubble Space Telescope observations. There are about 1080 atoms in the observable universe, by rough estimation.According to Don Page, physicist at the University of Alberta, Canada, the longest finite time that has so far been explicitly calculated by any physicist is 10 10 10 10 10 1.1 years which corresponds to the scale of an estimated Poincaré recurrence time for the quantum state of a hypothetical box containing a black hole with the estimated mass of the entire universe, observable or not, assuming a certain inflationary model with an inflaton whose mass is 10−6 Planck masses. This time assumes a statistical model subject to Poincaré recurrence. A much simplified way of thinking about this time is in a model where the universe's history repeats itself arbitrarily many times due to properties of statistical mechanics; this is the time scale when it will first be somewhat similar (for a reasonable choice of ""similar"") to its current state again.
 A. Approximately z = 6.0, corresponding to 1 billion years after the Big Bang.
 B. Approximately z = 16.7, corresponding to 235.8 million years after the Big Bang.
 C. Approximately z = 3.0, corresponding to 5 billion years after the Big Bang.
 D. Approximately z = 10.0, corresponding to 13 billion years after the Big Bang.
 E. Approximately z = 13.0, corresponding to 30 billion light-years away from Earth. "
What is bollard pull primarily used for measuring?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is bollard pull primarily used for measuring?
 Context: -Bollard pull is a conventional measure of the pulling (or towing) power of a watercraft. It is defined as the force (usually in tonnes-force or kilonewtons (kN)) exerted by a vessel under full power, on a shore-mounted bollard through a tow-line, commonly measured in a practical test (but sometimes simulated) under test conditions that include calm water, no tide, level trim, and sufficient depth and side clearance for a free propeller stream. Like the horsepower or mileage rating of a car, it is a convenient but idealized number that must be adjusted for operating conditions that differ from the test. The bollard pull of a vessel may be reported as two numbers, the static or maximum bollard pull – the highest force measured – and the steady or continuous bollard pull, the average of measurements over an interval of, for example, 10 minutes. An equivalent measurement on land is known as drawbar pull, or tractive force, which is used to measure the total horizontal force generated by a locomotive, a piece of heavy machinery such as a tractor, or a truck, (specifically a ballast tractor), which is utilized to move a load.
-Practical bollard pull tests under simplified conditions are conducted for human powered vehicles. There, bollard pull is often a category in competitions and gives an indication of the power train efficiency. Although conditions for such measurements are inaccurate in absolute terms, they are the same for all competitors. Hence, they can still be valid for comparing several craft.
-A truck scale (US), weighbridge (non-US) or railroad scale is a large set of scales, usually mounted permanently on a concrete foundation, that is used to weigh entire rail or road vehicles and their contents. By weighing the vehicle both empty and when loaded, the load carried by the vehicle can be calculated.
The key component that uses a weighbridge in order to make the weigh measurement is load cells.
-A ballast tractor is a specially weighted tractor unit of a heavy hauler combination. It is designed to utilize a drawbar to pull or push heavy or exceptionally large trailer loads which are loaded in a hydraulic modular trailer. When feasible, lowboy-style semi-trailers are used to minimize a load's center of gravity. Typical drivetrains are 6×4 and 6×6 but also available in 8×6 and 8×8. Typical ballast tractor loads include oil rig modules, bridge sections, buildings, ship sections, and industrial machinery such as generators and turbines.
-The same type of ""weigh bar"" can be used to measure horizontal loads and ""drawbar pull"" of wheeled/tracked or vehicles or ""bollard pull"" of boats or the ""thrust"" of jet engines when a proper ""test rig"" is designed and constructed to provide ""frictionless"" fore-aft movement of the load relative to the weigh bars.
 A. The weight of heavy machinery
 B. The speed of locomotives
 C. The distance traveled by a truck
 D. The strength of tugboats
 E. The height of a ballast tractor "
What is the piezoelectric strain coefficient for AT-cut quartz crystals?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the piezoelectric strain coefficient for AT-cut quartz crystals?
 Context: -Amplitude of motion The amplitude of lateral displacement rarely exceeds a nanometer. More specifically one has u0=4(nπ)2dQUel with u0 the amplitude of lateral displacement, n the overtone order, d the piezoelectric strain coefficient, Q the quality factor, and Uel the amplitude of electrical driving. The piezoelectric strain coefficient is given as d = 3.1·10‑12 m/V for AT-cut quartz crystals. Due to the small amplitude, stress and strain usually are proportional to each other. The QCM operates in the range of linear acoustics.
-AT-cut crystals are singularly rotated Y-axis cuts in which the top and bottom half of the crystal move in opposite directions (thickness shear vibration) during oscillation.  The AT-cut crystal is easily manufactured. However, it has limitations at high and low temperature, as it is easily disrupted by internal stresses caused by temperature gradients in these temperature extremes (relative to room temperature, ~25 °C). These internal stress points produce undesirable frequency shifts in the crystal, decreasing its accuracy. The relationship between temperature and frequency is cubic. The cubic relationship has an inflection point near room temperature. As a consequence the AT-cut quartz crystal is most effective when operating at or near room temperature. For applications which are above room temperature, water cooling is often helpful.
-The strain-charge for a material of the 4mm (C4v) crystal class (such as a poled piezoelectric ceramic such as tetragonal PZT or BaTiO3) as well as the 6mm crystal class may also be written as (ANSI IEEE 176): 11 12 13 21 22 23 31 32 33 44 55 66 11 12 31 32 33 24 15 15 24 31 32 33 11 22 33 ][E1E2E3] where the first equation represents the relationship for the converse piezoelectric effect and the latter for the direct piezoelectric effect.Although the above equations are the most used form in literature, some comments about the notation are necessary. Generally, D and E are vectors, that is, Cartesian tensors of rank 1; and permittivity ε is a Cartesian tensor of rank 2. Strain and stress are, in principle, also rank-2 tensors. But conventionally, because strain and stress are all symmetric tensors, the subscript of strain and stress can be relabeled in the following fashion: 11 → 1; 22 → 2; 33 → 3; 23 → 4; 13 → 5; 12 → 6. (Different conventions may be used by different authors in literature. For example, some use 12 → 4; 23 → 5; 31 → 6 instead.) That is why S and T appear to have the ""vector form"" of six components. Consequently, s appears to be a 6-by-6 matrix instead of a rank-3 tensor. Such a relabeled notation is often called Voigt notation. Whether the shear strain components S4, S5, S6 are tensor components or engineering strains is another question. In the equation above, they must be engineering strains for the 6,6 coefficient of the compliance matrix to be written as shown, i.e., 2(sE11 − sE12). Engineering shear strains are double the value of the corresponding tensor shear, such as S6 = 2S12 and so on. This also means that s66 = 1/G12, where G12 is the shear modulus.
-Non linear piezoelectric effects in polar semiconductors are the manifestation that the strain induced piezoelectric polarization depends not just on the product of the first order piezoelectric coefficients times the strain tensor components but also on the product of the second order (or higher) piezoelectric coefficients times products of the strain tensor components. The idea was put forward for zincblende GaAs and InAs semiconductors since 2006, and then extended to all commonly used wurtzite and zincblende semiconductors. Given the difficulty of finding direct experimental evidence for the existence of these effects, there are different schools of thought on how one can calculate reliably all the piezoelectric coefficients.
-q=βr−(βr.r^)r^ β=(∂u1x1∂u1x2∂u1x3∂u2x1∂u2x2∂u2x3∂u3x1∂u3x2∂u3x3),r=(r1r2r3) The shifts are measured in the phosphor (detector) plane ( β3r3=0 ), and the relationship is simplified; thus, eight out of the nine displacement gradient tensor components can be calculated by measuring the shift at four distinct, widely spaced regions on the EBSP. This shift is then corrected to the sample frame (flipped around Y-axis) because EBSP is recorded on the phosphor screen and is inverted as in a mirror. They are then corrected around the X-axis by 24° (i.e., 20° sample tilt plus ≈4° camera tilt and assuming no angular effect from the beam movement). Using infinitesimal strain theory, the deformation gradient is then split into elastic strain (symmetric part, where  ij=ji ),  eij and lattice rotations (asymmetric part, where  ii=jj=0 ),  ωij .eij=12(βij+βijr),ωij=12(βij−βijr) These measurements do not provide information about the volumetric/hydrostatic strain tensors. By imposing a boundary condition that the stress normal to the surface ( 33 ) is zero (i.e., traction-free surface), and using Hooke's law with anisotropic elastic stiffness constants, the missing ninth degree of freedom can be estimated in this constrained minimisation problem by using a nonlinear solver.
 A. d = 1.9·10‑12 m/V
 B. d = 3.1·10‑12 m/V
 C. d = 4.2·10‑12 m/V
 D. d = 2.5·10‑12 m/V
 E. d = 5.8·10‑12 m/V "
What is the difference between probability mass function (PMF) and probability density function (PDF)?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the difference between probability mass function (PMF) and probability density function (PDF)?
 Context: -The terms probability distribution function and probability function have also sometimes been used to denote the probability density function. However, this use is not standard among probabilists and statisticians. In other sources, ""probability distribution function"" may be used when the probability distribution is defined as a function over general sets of values or it may refer to the cumulative distribution function, or it may be a probability mass function (PMF) rather than the density. ""Density function"" itself is also used for the probability mass function, leading to further confusion. In general though, the PMF is used in the context of discrete random variables (random variables that take values on a countable set), while the PDF is used in the context of continuous random variables.
-A probability mass function differs from a probability density function (PDF) in that the latter is associated with continuous rather than discrete random variables. A PDF must be integrated over an interval to yield a probability.The value of the random variable having the largest probability mass is called the mode.
-In statistics, especially in Bayesian statistics, the kernel of a probability density function (pdf) or probability mass function (pmf) is the form of the pdf or pmf in which any factors that are not functions of any of the variables in the domain are omitted. Note that such factors may well be functions of the parameters of the pdf or pmf. These factors form part of the normalization factor of the probability distribution, and are unnecessary in many situations. For example, in pseudo-random number sampling, most sampling algorithms ignore the normalization factor. In addition, in Bayesian analysis of conjugate prior distributions, the normalization factors are generally ignored during the calculations, and only the kernel considered. At the end, the form of the kernel is examined, and if it matches a known distribution, the normalization factor can be reinstated. Otherwise, it may be unnecessary (for example, if the distribution only needs to be sampled from).
-In probability and statistics, a probability mass function is a function that gives the probability that a discrete random variable is exactly equal to some value. Sometimes it is also known as the discrete probability density function. The probability mass function is often the primary means of defining a discrete probability distribution, and such functions exist for either scalar or multivariate random variables whose domain is discrete.
-The first illustration involves a continuous probability distribution, for which the random variables have a probability density function. The second illustration, for which most of the computation can be done by hand, involves a discrete probability distribution, which is characterized by a probability mass function.
 A. PMF is used only for continuous random variables, while PDF is used for both continuous and discrete random variables.
 B. PMF is used for both continuous and discrete random variables, while PDF is used only for continuous random variables.
 C. PMF is used for continuous random variables, while PDF is used for discrete random variables.
 D. PMF is used for discrete random variables, while PDF is used for continuous random variables.
 E. PMF and PDF are interchangeable terms used for the same concept in probability theory. "
"How do the Lunar Laser Ranging Experiment, radar astronomy, and the Deep Space Network determine distances to the Moon, planets, and spacecraft?","Answer the following question with A, B, C, D, or E. You can use the context provided. Question: How do the Lunar Laser Ranging Experiment, radar astronomy, and the Deep Space Network determine distances to the Moon, planets, and spacecraft?
 Context: -Distance measurement Radar systems measure the distance to a target by the time it takes a radio-wave pulse to return to the radar antenna after being reflected by the target: the distance to the target is half the round-trip transit time multiplied by the speed of light. A Global Positioning System (GPS) receiver measures its distance to GPS satellites based on how long it takes for a radio signal to arrive from each satellite, and from these distances calculates the receiver's position. Because light travels about 300000 kilometres (186000 mi) in one second, these measurements of small fractions of a second must be very precise. The Lunar Laser Ranging experiment, radar astronomy and the Deep Space Network determine distances to the Moon, planets and spacecraft, respectively, by measuring round-trip transit times.
-Millimeter-precision measurements of the lunar distance are made by measuring the time taken for laser beam light to travel between stations on Earth and retroreflectors placed on the Moon. The Moon is spiraling away from Earth at an average rate of 3.8 cm (1.5 in) per year, as detected by the Lunar Laser Ranging experiment.
-Solar System Besides terrestrial tests also astrometric tests using Lunar Laser Ranging (LLR), i.e. sending laser signals from Earth to Moon and back, have been conducted. They are ordinarily used to test general relativity and are evaluated using the Parameterized post-Newtonian formalism. However, since these measurements are based on the assumption that the speed of light is constant, they can also be used as tests of special relativity by analyzing potential distance and orbit oscillations. For instance, Zoltán Lajos Bay and White (1981) demonstrated the empirical foundations of the Lorentz group and thus special relativity by analyzing the planetary radar and LLR data.In addition to the terrestrial Kennedy–Thorndike experiments mentioned above, Müller & Soffel (1995) and Müller et al. (1999) tested the RMS velocity dependence parameter by searching for anomalous distance oscillations using LLR. Since time dilation is already confirmed to high precision, a positive result would prove that light speed depends on the observer's velocity and length contraction is direction dependent (like in the other Kennedy–Thorndike experiments). However, no anomalous distance oscillations have been observed, with a RMS velocity dependence limit of  12 10 −5 , comparable to that of Hils and Hall (1990, see table above on the right).
-Interferometry Interferometry is another method to find the wavelength of electromagnetic radiation for determining the speed of light. A coherent beam of light (e.g. from a laser), with a known frequency (f), is split to follow two paths and then recombined. By adjusting the path length while observing the interference pattern and carefully measuring the change in path length, the wavelength of the light (λ) can be determined. The speed of light is then calculated using the equation c = λf.
-There are different ways to determine the value of c. One way is to measure the actual speed at which light waves propagate, which can be done in various astronomical and Earth-based setups. However, it is also possible to determine c from other physical laws where it appears, for example, by determining the values of the electromagnetic constants ε0 and μ0 and using their relation to c. Historically, the most accurate results have been obtained by separately determining the frequency and wavelength of a light beam, with their product equalling c. This is described in more detail in the ""Interferometry"" section below.
 A. They determine the values of electromagnetic constants.
 B. They measure round-trip transit times.
 C. They measure the actual speed of light waves.
 D. They use interferometry to determine the speed of light.
 E. They separately determine the frequency and wavelength of a light beam. "
What is the Ozma Problem?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the Ozma Problem?
 Context: -The last several chapters deal with a conundrum called the Ozma Problem, which examines whether there is any fundamental asymmetry to the universe. This discussion concerns various aspects of atomic and subatomic physics and how they relate to mirror asymmetry and the related concepts of chirality, antimatter, magnetic and electrical polarity, parity, charge and spin. Time invariance (and reversal) is discussed. Implications for particle physics, theoretical physics and cosmology are covered and brought up to date (in later editions of the book) with regard to Grand Unified Theories, theories of everything, superstring theory and M-theory.
-In physical cosmology, the baryon asymmetry problem, also known as the matter asymmetry problem or the matter–antimatter asymmetry problem, is the observed imbalance in baryonic matter (the type of matter experienced in everyday life) and antibaryonic matter in the observable universe. Neither the standard model of particle physics nor the theory of general relativity provides a known explanation for why this should be so, and it is a natural assumption that the universe is neutral with all conserved charges. The Big Bang should have produced equal amounts of matter and antimatter. Since this does not seem to have been the case, it is likely some physical laws must have acted differently or did not exist for matter and antimatter. Several competing hypotheses exist to explain the imbalance of matter and antimatter that resulted in baryogenesis. However, there is as of yet no consensus theory to explain the phenomenon, which has been described as ""one of the great mysteries in physics"".
-The Ozma Problem The 18th chapter, ""The Ozma Problem"", poses a problem that Gardner claims would arise if Earth should ever enter into communication with life on another planet through Project Ozma. This is the problem of how to communicate the meaning of left and right, where the two communicants are conditionally not allowed to view any one object in common.
-The problem was first implied in Immanuel Kant's discussion of a hand isolated in space, which would have no meaning as left or right by itself; Gardner posits that Kant would today explain his problem using the reversibility of objects through a higher dimension. A three-dimensional hand can be reversed in a mirror or a hypothetical fourth dimension. In more easily visualizable terms, an outline of a hand in Flatland could be flipped over; the meaning of left or right would not apply until a being missing a corresponding hand came along. Charles Howard Hinton expressed the essential problem in 1888, as did William James in his The Principles of Psychology (1890). Gardner follows the thread of several false leads on the road to the solution of the problem, such as the magnetic poles of astronomical bodies and the chirality of life molecules, which could be arbitrary based on how life locally originated.The solution to the Ozma Problem was finally realized in the famous Wu experiment, conducted in 1956 by Chinese-American physicist Chien-Shiung Wu (1912–1997), involving the beta decay of cobalt-60. At a conference earlier that year, Richard Feynman had asked (on behalf of Martin M. Block) whether parity was sometimes violated, leading Tsung-Dao Lee and Chen-Ning Yang to propose Wu's experiment, for which Lee and Yang were awarded the 1957 Nobel Prize in Physics. It was the first experiment to disprove the conservation of parity, and according to Gardner, one could use it to convey the meaning of left and right to remote extraterrestrials. An earlier example of asymmetry had actually been detected as early as 1928 in the decay of a radionuclide of radium, but its significance was not then realized.
-The Ozsváth–Schücking metric, or the Ozsváth–Schücking solution, is a vacuum solution of the Einstein field equations. The metric was published by István Ozsváth and Engelbert Schücking in 1962. It is noteworthy among vacuum solutions for being the first known solution that is stationary, globally defined, and singularity-free but nevertheless not isometric to the Minkowski metric. This stands in contradiction to a claimed strong Mach principle, which would forbid a vacuum solution from being anything but Minkowski without singularities, where the singularities are to be construed as mass as in the Schwarzschild metric.With coordinates  {x0,x1,x2,x3} , define the following tetrad: e(0)=12+(x3)2(x3∂0−∂1+∂2) e(1)=14+2(x3)2[(x3−2+(x3)2)∂0+(1+(x3)2−x32+(x3)2)∂1+∂2] e(2)=14+2(x3)2[(x3+2+(x3)2)∂0+(1+(x3)2+x32+(x3)2)∂1+∂2] e(3)=∂3 It is straightforward to verify that e(0) is timelike, e(1), e(2), e(3) are spacelike, that they are all orthogonal, and that there are no singularities. The corresponding proper time is dτ2=−(dx0)2+4(x3)(dx0)(dx2)−2(dx1)(dx2)−2(x3)2(dx2)2−(dx3)2.
 A. The Ozma Problem is a chapter in a book that discusses the versatility of carbon and chirality in biochemistry.
 B. The Ozma Problem is a discussion about time invariance and reversal in particle physics, theoretical physics, and cosmology.
 C. The Ozma Problem is a conundrum that examines whether there is any fundamental asymmetry to the universe. It concerns various aspects of atomic and subatomic physics and how they relate to mirror asymmetry and the related concepts of chirality, antimatter, magnetic and electrical polarity, parity, charge and spin.
 D. The Ozma Problem is a measure of how symmetry and asymmetry have evolved from the beginning of life on Earth.
 E. The Ozma Problem is a comparison between the level of a desired signal and the level of background noise used in science and engineering. "
What is a Hilbert space in quantum mechanics?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is a Hilbert space in quantum mechanics?
 Context: -In the mathematically rigorous formulation of quantum mechanics, the state of a quantum mechanical system is a vector  ψ belonging to a (separable) complex Hilbert space  H . This vector is postulated to be normalized under the Hilbert space inner product, that is, it obeys  ⟨ψ,ψ⟩=1 , and it is well-defined up to a complex number of modulus 1 (the global phase), that is,  ψ and  eiαψ represent the same physical system. In other words, the possible states are points in the projective space of a Hilbert space, usually called the complex projective space. The exact nature of this Hilbert space is dependent on the system – for example, for describing position and momentum the Hilbert space is the space of complex square-integrable functions  L2(C) , while the Hilbert space for the spin of a single proton is simply the space of two-dimensional complex vectors  C2 with the usual inner product.
-Mathematical In a formal setup, any system in quantum mechanics is described by a state, which is a vector |Ψ⟩, residing in an abstract complex vector space, called a Hilbert space. It may be either infinite- or finite-dimensional. A usual presentation of that Hilbert space is a special function space, called L2(X), on certain set X, that is either some configuration space or a discrete set.
-Preliminaries Introductory courses on physics or chemistry typically introduce the Schrödinger equation in a way that can be appreciated knowing only the concepts and notations of basic calculus, particularly derivatives with respect to space and time. A special case of the Schrödinger equation that admits a statement in those terms is the position-space Schrödinger equation for a single nonrelativistic particle in one dimension: Here,  Ψ(x,t) is a wave function, a function that assigns a complex number to each point  x at each time  t . The parameter  m is the mass of the particle, and  V(x,t) is the potential that represents the environment in which the particle exists.: 74  The constant  i is the imaginary unit, and  ℏ is the reduced Planck constant, which has units of action (energy multiplied by time).: 10 Broadening beyond this simple case, the mathematical formulation of quantum mechanics developed by Paul Dirac, David Hilbert, John von Neumann, and Hermann Weyl defines the state of a quantum mechanical system to be a vector  |ψ⟩ belonging to a (separable) Hilbert space  H . This vector is postulated to be normalized under the Hilbert space's inner product, that is, in Dirac notation it obeys  ⟨ψ|ψ⟩=1 . The exact nature of this Hilbert space is dependent on the system – for example, for describing position and momentum the Hilbert space is the space of complex square-integrable functions  L2(C) , while the Hilbert space for the spin of a single proton is simply the space of two-dimensional complex vectors  C2 with the usual inner product.: 322 Physical quantities of interest – position, momentum, energy, spin – are represented by ""observables"", which are Hermitian (more precisely, self-adjoint) linear operators acting on the Hilbert space. A wave function can be an eigenvector of an observable, in which case it is called an eigenstate, and the associated eigenvalue corresponds to the value of the observable in that eigenstate. More generally, a quantum state will be a linear combination of the eigenstates, known as a quantum superposition. When an observable is measured, the result will be one of its eigenvalues with probability given by the Born rule: in the simplest case the eigenvalue  λ is non-degenerate and the probability is given by  |⟨λ|ψ⟩|2 , where  |λ⟩ is its associated eigenvector. More generally, the eigenvalue is degenerate and the probability is given by  ⟨ψ|Pλ|ψ⟩ , where  Pλ is the projector onto its associated eigenspace.A momentum eigenstate would be a perfectly monochromatic wave of infinite extent, which is not square-integrable. Likewise a position eigenstate would be a Dirac delta distribution, not square-integrable and technically not a function at all. Consequently, neither can belong to the particle's Hilbert space. Physicists sometimes introduce fictitious ""bases"" for a Hilbert space comprising elements outside that space. These are invented for calculational convenience and do not represent physical states.: 100–105  Thus, a position-space wave function  Ψ(x,t) as used above can be written as the inner product of a time-dependent state vector  |Ψ(t)⟩ with unphysical but convenient ""position eigenstates""  |x⟩ Time-dependent equation The form of the Schrödinger equation depends on the physical situation. The most general form is the time-dependent Schrödinger equation, which gives a description of a system evolving with time:: 143  where  t is time,  |Ψ(t)⟩ is the state vector of the quantum system ( Ψ being the Greek letter psi), and  H^ is an observable, the Hamiltonian operator.
-Specifically, in quantum mechanics a state space is a complex Hilbert space in which each unit vector represents a different state that could come out of a measurement. Each unit vector specifies a different dimension, so the numbers of dimensions in this Hilbert space depends on the system we choose to describe. Any state vector in this space can be written as a linear combination of unit vectors. Having an nonzero component along multiple dimensions is called a superposition. These state vectors, using Dirac's bra–ket notation, can often be treated like coordinate vectors and operated on using the rules of linear algebra. This Dirac formalism of quantum mechanics can replace calculation of complicated integrals with simpler vector operations.
-Background In elementary quantum mechanics, the state of a quantum-mechanical system is represented by a complex-valued wavefunction ψ(x, t). More abstractly, the state may be represented as a state vector, or ket, |ψ⟩. This ket is an element of a Hilbert space, a vector space containing all possible states of the system. A quantum-mechanical operator is a function which takes a ket |ψ⟩ and returns some other ket |ψ′⟩.
 A. A complex vector space where the state of a classical mechanical system is described by a vector |Ψ⟩.
 B. A physical space where the state of a classical mechanical system is described by a vector |Ψ⟩.
 C. A physical space where the state of a quantum mechanical system is described by a vector |Ψ⟩.
 D. A mathematical space where the state of a classical mechanical system is described by a vector |Ψ⟩.
 E. A complex vector space where the state of a quantum mechanical system is described by a vector |Ψ⟩. "
What is the significance of the speed of light in vacuum?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the significance of the speed of light in vacuum?
 Context: -The speed at which light waves propagate in vacuum is independent both of the motion of the wave source and of the inertial frame of reference of the observer. This invariance of the speed of light was postulated by Einstein in 1905, after being motivated by Maxwell's theory of electromagnetism and the lack of evidence for motion against the luminiferous aether; it has since been consistently confirmed by many experiments. It is only possible to verify experimentally that the two-way speed of light (for example, from a source to a mirror and back again) is frame-independent, because it is impossible to measure the one-way speed of light (for example, from a source to a distant detector) without some convention as to how clocks at the source and at the detector should be synchronized. However, by adopting Einstein synchronization for the clocks, the one-way speed of light becomes equal to the two-way speed of light by definition. The special theory of relativity explores the consequences of this invariance of c with the assumption that the laws of physics are the same in all inertial frames of reference. One consequence is that c is the speed at which all massless particles and waves, including light, must travel in vacuum.
-The speed of light in vacuum is defined to be exactly 299 792 458 m/s (approx. 186,282 miles per second). The fixed value of the speed of light in SI units results from the fact that the metre is now defined in terms of the speed of light. All forms of electromagnetic radiation move at exactly this same speed in vacuum.
-Faster-than-light communication is, according to relativity, equivalent to time travel. What we measure as the speed of light in vacuum (or near vacuum) is actually the fundamental physical constant c. This means that all inertial and, for the coordinate speed of light, non-inertial observers, regardless of their relative velocity, will always measure zero-mass particles such as photons traveling at c in vacuum. This result means that measurements of time and velocity in different frames are no longer related simply by constant shifts, but are instead related by Poincaré transformations. These transformations have important implications: The relativistic momentum of a massive particle would increase with speed in such a way that at the speed of light an object would have infinite momentum.
-Basics While the speed of light in vacuum is a universal constant (c = 299,792,458 m/s), the speed in a material may be significantly less, as it is perceived to be slowed by the medium. For example, in water it is only 0.75c. Matter can accelerate to a velocity higher than this (although still less than c, the speed of light in vacuum) during nuclear reactions and in particle accelerators. Cherenkov radiation results when a charged particle, most commonly an electron, travels through a dielectric (can be polarized electrically) medium with a speed greater than light's speed in that medium.
-The simplest picture of light given by classical physics is of a wave or disturbance in the electromagnetic field. In a vacuum, Maxwell's equations predict that these disturbances will travel at a specific speed, denoted by the symbol c. This well-known physical constant is commonly referred to as the speed of light. The postulate of the constancy of the speed of light in all inertial reference frames lies at the heart of special relativity and has given rise to a popular notion that the ""speed of light is always the same"". However, in many situations light is more than a disturbance in the electromagnetic field.
 A. The speed of light in vacuum is only relevant when measuring the one-way speed of light.
 B. The speed of light in vacuum is only relevant when measuring the two-way speed of light.
 C. The speed of light in vacuum is independent of the motion of the wave source and the observer's inertial frame of reference.
 D. The speed of light in vacuum is dependent on the motion of the wave source and the observer's inertial frame of reference.
 E. The speed of light in vacuum is only relevant when c appears explicitly in the units of measurement. "
What is the term used to describe the proportionality factor to the Stefan-Boltzmann law that is utilized in subsequent evaluations of the radiative behavior of grey bodies?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the term used to describe the proportionality factor to the Stefan-Boltzmann law that is utilized in subsequent evaluations of the radiative behavior of grey bodies?
 Context: -Stefan–Boltzmann lawThe Stefan–Boltzmann law describes the power radiated from a black body in terms of its temperature. Specifically, the Stefan–Boltzmann law states that the total energy radiated per unit surface area of a black body across all wavelengths per unit time  j⋆ (also known as the black-body radiant emittance) is directly proportional to the fourth power of the black body's thermodynamic temperature T: j⋆=σT4.
-The Stefan–Boltzmann law, also known as Stefan's law, describes the intensity of the thermal radiation emitted by matter in terms of that matter's temperature. It is named for Josef Stefan, who empirically derived the relationship, and Ludwig Boltzmann who derived the law theoretically.
For an ideal absorber/emitter or black body, the Stefan–Boltzmann law states that the total energy radiated per unit surface area per unit time (also known as the radiant exitance) is directly proportional to the fourth power of the black body's temperature, T: M∘=σT4.
-The Stefan–Boltzmann law, also known as Stefan's law, describes the intensity of the thermal radiation emitted by matter in terms of that matter's temperature. It is named for Josef Stefan, who empirically derived the relationship, and Ludwig Boltzmann who derived the law theoretically.
For an ideal absorber/emitter or black body, the Stefan–Boltzmann law states that the total energy radiated per unit surface area per unit time (also known as the radiant exitance) is directly proportional to the fourth power of the black body's temperature, T: M∘=σT4.
-The Stefan–Boltzmann law may be expressed as a formula for radiance as a function of temperature. Radiance is measured in watts per square metre per steradian (W m-2 sr-1). The Stefan–Boltzmann law for the radiance of a black body is:: 26  LΩ∘=M∘π=σπT4.
The Stefan–Boltzmann law expressed as a formula for radiation energy density is: we∘=4cM∘=4cσT4 where  c is the speed of light.
-The Stefan–Boltzmann law may be expressed as a formula for radiance as a function of temperature. Radiance is measured in watts per square metre per steradian (W m-2 sr-1). The Stefan–Boltzmann law for the radiance of a black body is:: 26  LΩ∘=M∘π=σπT4.
The Stefan–Boltzmann law expressed as a formula for radiation energy density is: we∘=4cM∘=4cσT4 where  c is the speed of light.
 A. Emissivity
 B. Wien's displacement law
 C. Reflectance
 D. Black-body radiation
 E. Albedo "
What is the reason for the formation of stars exclusively within molecular clouds?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the reason for the formation of stars exclusively within molecular clouds?
 Context: -Star formation The formation of stars occurs exclusively within molecular clouds. This is a natural consequence of their low temperatures and high densities, because the gravitational force acting to collapse the cloud must exceed the internal pressures that are acting ""outward"" to prevent a collapse. There is observed evidence that the large, star-forming clouds are confined to a large degree by their own gravity (like stars, planets, and galaxies) rather than by external pressure. The evidence comes from the fact that the ""turbulent"" velocities inferred from CO linewidth scale in the same manner as the orbital velocity (a virial relation).
-Star formation An example problem is that of star formation. Stars form out of the interstellar medium, with this formation mostly occurring in giant molecular clouds such as the Rosette Nebula. An interstellar cloud can collapse due to its self-gravity if it is large enough; however, in the ordinary interstellar medium this can only happen if the cloud has a mass of several thousands of solar masses—much larger than that of any star. Stars may still form, however, from processes that occur if the magnetic pressure is much larger than the thermal pressure, which is the case in giant molecular clouds. These processes rely on the interaction of magnetohydrodynamic waves with a thermal instability. A magnetohydrodynamic wave in a medium in which the magnetic pressure is much larger than the thermal pressure can produce dense regions, but they cannot by themselves make the density high enough for self-gravity to act. However, the gas in star forming regions is heated by cosmic rays and is cooled by radiative processes. The net result is that a gas in a thermal equilibrium state in which heating balances cooling can exist in three different phases at the same pressure: a warm phase with a low density, an unstable phase with intermediate density and a cold phase at low temperature. An increase in pressure due to a supernova or a spiral density wave can shift the gas from the warm phase to the unstable phase, with a magnetohydrodynamic wave then being able to produce dense fragments in the cold phase whose self-gravity is strong enough for them to collapse into stars.
-Star formation begins in relatively small molecular clouds called dense cores. Each dense core is initially in balance between self-gravity, which tends to compress the object, and both gas pressure and magnetic pressure, which tend to inflate it. As the dense core accrues mass from its larger, surrounding cloud, self-gravity begins to overwhelm pressure, and collapse begins. Theoretical modeling of an idealized spherical cloud initially supported only by gas pressure indicates that the collapse process spreads from the inside toward the outside. Spectroscopic observations of dense cores that do not yet contain stars indicate that contraction indeed occurs. So far, however, the predicted outward spread of the collapse region has not been observed.
-Star formation The formation of a star begins with gravitational instability within a molecular cloud, caused by regions of higher density—often triggered by compression of clouds by radiation from massive stars, expanding bubbles in the interstellar medium, the collision of different molecular clouds, or the collision of galaxies (as in a starburst galaxy). When a region reaches a sufficient density of matter to satisfy the criteria for Jeans instability, it begins to collapse under its own gravitational force.As the cloud collapses, individual conglomerations of dense dust and gas form ""Bok globules"". As a globule collapses and the density increases, the gravitational energy converts into heat and the temperature rises. When the protostellar cloud has approximately reached the stable condition of hydrostatic equilibrium, a protostar forms at the core. These pre-main-sequence stars are often surrounded by a protoplanetary disk and powered mainly by the conversion of gravitational energy. The period of gravitational contraction lasts about 10 million years for a star like the sun, up to 100 million years for a red dwarf.Early stars of less than 2 M☉ are called T Tauri stars, while those with greater mass are Herbig Ae/Be stars. These newly formed stars emit jets of gas along their axis of rotation, which may reduce the angular momentum of the collapsing star and result in small patches of nebulosity known as Herbig–Haro objects.
-The densest molecular clouds have significantly higher pressure than the interstellar average, since they are bound together by their own gravity. When stars form in such clouds, especially OB stars, they convert the surrounding gas into the warm ionized phase, a temperature increase of several hundred. Initially the gas is still at molecular cloud densities, and so at vastly higher pressure than the ISM average: this is a classical H II region. The large overpressure causes the ionized gas to expand away from the remaining molecular gas (a Champagne flow), and the flow will continue until either the molecular cloud is fully evaporated or the OB stars reach the end of their lives, after a few millions years. At this point the OB stars explode as supernovas, creating blast waves in the warm gas that increase temperatures to the coronal phase (supernova remnants, SNR). These too expand and cool over several million years until they return to average ISM pressure.
 A. The formation of stars occurs exclusively outside of molecular clouds.
 B. The low temperatures and high densities of molecular clouds cause the gravitational force to exceed the internal pressures that are acting ""outward"" to prevent a collapse.
 C. The low temperatures and low densities of molecular clouds cause the gravitational force to be less than the internal pressures that are acting ""outward"" to prevent a collapse.
 D. The high temperatures and low densities of molecular clouds cause the gravitational force to exceed the internal pressures that are acting ""outward"" to prevent a collapse.
 E. The high temperatures and high densities of molecular clouds cause the gravitational force to be less than the internal pressures that are acting ""outward"" to prevent a collapse. "
What is the identity operation in symmetry groups?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the identity operation in symmetry groups?
 Context: -The symmetry group operations (symmetry operations) are the isometries of three-dimensional space R3 that leave the origin fixed, forming the group O(3). These operations can be categorized as: The direct (orientation-preserving) symmetry operations, which form the group SO(3): The identity operation, denoted by E or the identity matrix I.
Rotation about an axis through the origin by an angle θ. Rotation by θ = 360°/n for any positive integer n is denoted Cn (from the Schoenflies notation for the group Cn that it generates). The identity operation, also written C1, is a special case of the rotation operator.
The indirect (orientation-reversing) operations: Inversion, denoted i or Ci. The matrix notation is −I.
Reflection in a plane through the origin, denoted σ.
-Basic point group symmetry operations The five basic symmetry operations mentioned above are: Identity Operation E (from the German 'Einheit' meaning unity): The identity operation leaves the molecule unchanged. It forms the identity element in the symmetry group. Though its inclusion seems to be trivial, it is important also because even for the most asymmetric molecule, this symmetry is present. The corresponding symmetry element is the entire molecule itself.
-In chemistry, there are five important symmetry operations. They are identity operation (E), rotation operation or proper rotation (Cn), reflection operation (σ), inversion (i) and rotation reflection operation or improper rotation (Sn). The identity operation (E) consists of leaving the molecule as it is. This is equivalent to any number of full rotations around any axis. This is a symmetry of all molecules, whereas the symmetry group of a chiral molecule consists of only the identity operation. An identity operation is a characteristic of every molecule even if it has no symmetry. Rotation around an axis (Cn) consists of rotating the molecule around a specific axis by a specific angle. It is rotation through the angle 360°/n, where n is an integer, about a rotation axis. For example, if a water molecule rotates 180° around the axis that passes through the oxygen atom and between the hydrogen atoms, it is in the same configuration as it started. In this case, n = 2, since applying it twice produces the identity operation. In molecules with more than one rotation axis, the Cn axis having the largest value of n is the highest order rotation axis or principal axis. For example in boron trifluoride (BF3), the highest order of rotation axis is C3, so the principal axis of rotation is C3.
-Molecular symmetry is responsible for many physical and spectroscopic properties of compounds and provides relevant information about how chemical reactions occur. In order to assign a point group for any given molecule, it is necessary to find the set of symmetry operations present on it. The symmetry operation is an action, such as a rotation around an axis or a reflection through a mirror plane. In other words, it is an operation that moves the molecule such that it is indistinguishable from the original configuration. In group theory, the rotation axes and mirror planes are called ""symmetry elements"". These elements can be a point, line or plane with respect to which the symmetry operation is carried out. The symmetry operations of a molecule determine the specific point group for this molecule.
-In group theory, geometry, representation theory and molecular geometry, a symmetry operation is a geometric transformation of an object that leaves the object looking the same after it has been carried out. For example, as transformations of an object in space, rotations, reflections and inversions are all symmetry operations. Such symmetry operations are performed with respect to symmetry elements (for example, a point, line or plane). In the context of molecular symmetry, a symmetry operation is a permutation of atoms such that the molecule or crystal is transformed into a state indistinguishable from the starting state.
 A. The identity operation leaves the molecule unchanged and forms the identity element in the symmetry group.
 B. The identity operation rotates the molecule about its center of mass.
 C. The identity operation inverts the molecule about its center of inversion.
 D. The identity operation reflects the molecule across a plane of symmetry.
 E. The identity operation translates the molecule in 3-D space. "
What is a regular polytope?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is a regular polytope?
 Context: -In mathematics, a regular polytope is a polytope whose symmetry group acts transitively on its flags, thus giving it the highest degree of symmetry. All its elements or j-faces (for all 0 ≤ j ≤ n, where n is the dimension of the polytope) — cells, faces and so on — are also transitive on the symmetries of the polytope, and are regular polytopes of dimension ≤ n.
-A regular polyhedron is a polyhedron whose symmetry group acts transitively on its flags. A regular polyhedron is highly symmetrical, being all of edge-transitive, vertex-transitive and face-transitive. In classical contexts, many different equivalent definitions are used; a common one is that the faces are congruent regular polygons which are assembled in the same way around each vertex.
-For example, a flag of a polyhedron comprises one vertex, one edge incident to that vertex, and one polygonal face incident to both, plus the two improper faces.
A polytope may be regarded as regular if, and only if, its symmetry group is transitive on its flags. This definition excludes chiral polytopes.
-Regular polytopes Regular polytopes have the highest degree of symmetry of all polytopes. The symmetry group of a regular polytope acts transitively on its flags; hence, the dual polytope of a regular polytope is also regular.
There are three main classes of regular polytope which occur in any number of dimensions: Simplices, including the equilateral triangle and the regular tetrahedron.
Hypercubes or measure polytopes, including the square and the cube.
-And so on, a regular n-polytope is an n-dimensional polytope whose (n − 1)-dimensional faces are all regular and congruent, and whose vertex figures are all regular and congruent.This is a ""recursive"" definition. It defines regularity of higher dimensional figures in terms of regular figures of a lower dimension. There is an equivalent (non-recursive) definition, which states that a polytope is regular if it has a sufficient degree of symmetry.
 A. A regular polytope is a geometric shape whose symmetry group is transitive on its diagonals.
 B. A regular polytope is a geometric shape whose symmetry group is transitive on its vertices.
 C. A regular polytope is a geometric shape whose symmetry group is transitive on its flags.
 D. A regular polytope is a geometric shape whose symmetry group is transitive on its edges.
 E. A regular polytope is a geometric shape whose symmetry group is transitive on its faces. "
What is the reason behind the largest externally observed electrical effects when two conductors are separated by the smallest distance without touching?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the reason behind the largest externally observed electrical effects when two conductors are separated by the smallest distance without touching?
 Context: -Contact electrification If two conducting surfaces are moved relative to each other, and there is potential difference in the space between them, then an electric current will be driven. This is because the surface charge on a conductor depends on the magnitude of the electric field, which in turn depends on the distance between the surfaces. The externally observed electrical effects are largest when the conductors are separated by the smallest distance without touching (once brought into contact, the charge will instead flow internally through the junction between the conductors). Since two conductors in equilibrium can have a built-in potential difference due to work function differences, this means that bringing dissimilar conductors into contact, or pulling them apart, will drive electric currents. These contact currents can damage sensitive microelectronic circuitry and occur even when the conductors would be grounded in the absence of motion.
-Conductors, typically in the form of wires, may be used to transmit electrical energy or signals using an alternating current flowing through that conductor. The charge carriers constituting that current, usually electrons, are driven by an electric field due to the source of electrical energy. A current in a conductor produces a magnetic field in and around the conductor. When the intensity of current in a conductor changes, the magnetic field also changes. The change in the magnetic field, in turn, creates an electric field which opposes the change in current intensity. This opposing electric field is called “counter-electromotive force” (back EMF). The back EMF is strongest at the center of the conductor, and forces the conducting electrons to the outside of the conductor, as shown in the diagram on the right.Regardless of the driving force, the current density is found to be greatest at the conductor's surface, with a reduced magnitude deeper in the conductor. That decline in current density is known as the skin effect and the skin depth is a measure of the depth at which the current density falls to 1/e of its value near the surface.
-Electrostatic pressure On a conductor, a surface charge will experience a force in the presence of an electric field. This force is the average of the discontinuous electric field at the surface charge. This average in terms of the field just outside the surface amounts to: P=ε02E2, This pressure tends to draw the conductor into the field, regardless of the sign of the surface charge.
-A surface charge is an electric charge present on a two-dimensional surface. These electric charges are constrained on this 2-D surface, and surface charge density, measured in coulombs per square meter (C•m−2), is used to describe the charge distribution on the surface. The electric potential is continuous across a surface charge and the electric field is discontinuous, but not infinite; this is unless the surface charge consists of a dipole layer. In comparison, the potential and electric field both diverge at any point charge or linear charge.
-The electric potential is the same everywhere inside the conductor and is constant across the surface of the conductor. This follows from the first statement because the field is zero everywhere inside the conductor and therefore the potential is constant within the conductor too.
The electric field is perpendicular to the surface of a conductor. If this were not the case, the field would have a nonzero component on the surface of the conductor, which would cause the charges in the conductor to move around until that component of the field is zero.
 A. The surface charge on a conductor depends on the magnitude of the electric field, which in turn depends on the temperature between the surfaces.
 B. The surface charge on a conductor depends on the magnitude of the magnetic field, which in turn depends on the distance between the surfaces.
 C. The surface charge on a conductor depends on the magnitude of the electric field, which in turn depends on the angle between the surfaces.
 D. The surface charge on a conductor depends on the magnitude of the electric field, which in turn depends on the distance between the surfaces.
 E. The surface charge on a conductor depends on the magnitude of the electric field, which in turn depends on the pressure between the surfaces. "
What is the formalism that angular momentum is associated with in rotational invariance?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the formalism that angular momentum is associated with in rotational invariance?
 Context: -In modern (20th century) theoretical physics, angular momentum (not including any intrinsic angular momentum – see below) is described using a different formalism, instead of a classical pseudovector. In this formalism, angular momentum is the 2-form Noether charge associated with rotational invariance. As a result, angular momentum is not conserved for general curved spacetimes, unless it happens to be asymptotically rotationally invariant.In classical mechanics, the angular momentum of a particle can be reinterpreted as a plane element: in which the exterior product (∧) replaces the cross product (×) (these products have similar characteristics but are nonequivalent). This has the advantage of a clearer geometric interpretation as a plane element, defined using the vectors x and p, and the expression is true in any number of dimensions. In Cartesian coordinates: or more compactly in index notation: The angular velocity can also be defined as an anti-symmetric second order tensor, with components ωij. The relation between the two anti-symmetric tensors is given by the moment of inertia which must now be a fourth order tensor: Again, this equation in L and ω as tensors is true in any number of dimensions. This equation also appears in the geometric algebra formalism, in which L and ω are bivectors, and the moment of inertia is a mapping between them.
-Noether's theorem states that every conservation law is associated with a symmetry (invariant) of the underlying physics. The symmetry associated with conservation of angular momentum is rotational invariance. The fact that the physics of a system is unchanged if it is rotated by any angle about an axis implies that angular momentum is conserved.
-Angular momentum is an important dynamical quantity derived from position and momentum. It is a measure of an object's rotational motion and resistance to changes in its rotation. Also, in the same way momentum conservation corresponds to translational symmetry, angular momentum conservation corresponds to rotational symmetry – the connection between symmetries and conservation laws is made by Noether's theorem. While these concepts were originally discovered in classical mechanics, they are also true and significant in special and general relativity. In terms of abstract algebra, the invariance of angular momentum, four-momentum, and other symmetries in spacetime, are described by the Lorentz group, or more generally the Poincaré group.
-In physics, if a system behaves the same regardless of how it is oriented in space, then its Lagrangian is rotationally invariant. According to Noether's theorem, if the action (the integral over time of its Lagrangian) of a physical system is invariant under rotation, then angular momentum is conserved.
Application to quantum mechanics In quantum mechanics, rotational invariance is the property that after a rotation the new system still obeys Schrödinger's equation. That is [R,E−H]=0 for any rotation R. Since the rotation does not depend explicitly on time, it commutes with the energy operator. Thus for rotational invariance we must have [R, H] = 0.
For infinitesimal rotations (in the xy-plane for this example; it may be done likewise for any plane) by an angle dθ the (infinitesimal) rotation operator is R=1+Jzdθ, then [1+Jzdθ,ddt]=0, thus ddtJz=0, in other words angular momentum is conserved.
-Like linear momentum, angular momentum is vector quantity, and its conservation implies that the direction of the spin axis tends to remain unchanged. For this reason, the spinning top remains upright whereas a stationary one falls over immediately.
The angular momentum equation can be used to relate the moment of the resultant force on a body about an axis (sometimes called torque), and the rate of rotation about that axis.
 A. Angular momentum is the 1-form Noether charge associated with rotational invariance.
 B. Angular momentum is the 3-form Noether charge associated with rotational invariance.
 C. Angular momentum is the 5-form Noether charge associated with rotational invariance.
 D. Angular momentum is the 2-form Noether charge associated with rotational invariance.
 E. Angular momentum is the 4-form Noether charge associated with rotational invariance. "
"Which hand should be used to apply the right-hand rule when tightening or loosening nuts, screws, bolts, bottle caps, and jar lids?","Answer the following question with A, B, C, D, or E. You can use the context provided. Question: Which hand should be used to apply the right-hand rule when tightening or loosening nuts, screws, bolts, bottle caps, and jar lids?
 Context: -To apply the right-hand rule, place one's loosely clenched right hand above the object with the thumb pointing in the direction one wants the screw, nut, bolt, or cap ultimately to move, and the curl of the fingers, from the palm to the tips, will indicate in which way one needs to turn the screw, nut, bolt or cap to achieve the desired result. Almost all threaded objects obey this rule except for a few left-handed exceptions described below.
-Shop-work Typical nuts, screws, bolts, bottle caps, and jar lids are tightened (moved away from the observer) clockwise and loosened (moved towards the observer) counterclockwise in accordance with the right-hand rule.
-By common convention, right-handedness is the default handedness for screw threads. Therefore, most threaded parts and fasteners have right-handed threads. Left-handed thread applications include: Where the rotation of a shaft would cause a conventional right-handed nut to loosen rather than to tighten due to applied torque or to fretting induced precession. Examples include: The left hand pedal on a bicycle.
-The reason for the clockwise standard for most screws and bolts is that supination of the arm, which is used by a right-handed person to tighten a screw clockwise, is generally stronger than pronation used to loosen.  Sometimes the opposite (left-handed, counterclockwise, reverse) sense of threading is used for a special reason. A thread might need to be left-handed to prevent operational stresses from loosening it. For example, some older cars and trucks had right-handed lug nuts on the right wheels and left-handed lug nuts on the left wheels, so that, as the vehicle moved forward, the lug nuts tended to tighten rather than loosen. For bicycle pedals, the one on the left must be reverse-threaded to prevent it unscrewing during use. Similarly, the flyer whorl of a spinning wheel uses a left-hand thread to keep it from loosening. A turnbuckle has right-handed threads on one end and left-handed threads on the other. Some gas fittings are left-handed to prevent disastrous misconnections: oxygen fittings are right-handed, but acetylene, propane, and other flammable gases are unmistakably distinguished by left-handed fittings.
-By common convention, right-handedness is the default handedness for screw threads. Therefore, most threaded parts and fasteners have right-handed threads. One explanation for why right-handed threads became standard is that for a right-handed person, tightening a right-handed screw with a screwdriver is easier than tightening a left-handed screw, because it uses the stronger supinator muscle of the arm rather than the weaker pronator muscle. Since most people are right-handed, right-handed threads became standard on threaded fasteners.
 A. One's dominant hand
 B. The right hand
 C. Both hands
 D. The left hand
 E. Either hand "
What is the Minkowski diagram used for?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the Minkowski diagram used for?
 Context: -Overview The term Minkowski diagram refers to a specific form of spacetime diagram frequently used in special relativity. A Minkowski diagram is a two-dimensional graphical depiction of a portion of Minkowski space, usually where space has been curtailed to a single dimension. The units of measurement in these diagrams are taken such that the light cone at an event consists of the lines of slope plus or minus one through that event. The horizontal lines correspond to the usual notion of simultaneous events for a stationary observer at the origin.
-Graphical representation of the Lorentz transformation Spacetime diagrams (Minkowski diagrams) are an extremely useful aid to visualizing how coordinates transform between different reference frames. Although it is not as easy to perform exact computations using them as directly invoking the Lorentz transformations, their main power is their ability to provide an intuitive grasp of the results of a relativistic scenario.To draw a spacetime diagram, begin by considering two Galilean reference frames, S and S', in standard configuration, as shown in Fig. 2-1.: 155–199 Fig. 3-1a. Draw the  x and  t axes of frame S. The  x axis is horizontal and the  t (actually  ct ) axis is vertical, which is the opposite of the usual convention in kinematics. The  ct axis is scaled by a factor of  c so that both axes have common units of length. In the diagram shown, the gridlines are spaced one unit distance apart. The 45° diagonal lines represent the worldlines of two photons passing through the origin at time  0.
-Minkowski's principal tool is the Minkowski diagram, and he uses it to define concepts and demonstrate properties of Lorentz transformations (e.g. proper time and length contraction) and to provide geometrical interpretation to the generalization of Newtonian mechanics to relativistic mechanics. For these special topics, see the referenced articles, as the presentation below will be principally confined to the mathematical structure (Minkowski metric and from it derived quantities and the Poincaré group as symmetry group of spacetime) following from the invariance of the spacetime interval on the spacetime manifold as consequences of the postulates of special relativity, not to specific application or derivation of the invariance of the spacetime interval. This structure provides the background setting of all present relativistic theories, barring general relativity for which flat Minkowski spacetime still provides a springboard as curved spacetime is locally Lorentzian.
-The most well-known class of spacetime diagrams are known as Minkowski diagrams, developed by Hermann Minkowski in 1908. Minkowski diagrams are two-dimensional graphs that depict events as happening in a universe consisting of one space dimension and one time dimension. Unlike a regular distance-time graph, the distance is displayed on the horizontal axis and time on the vertical axis. Additionally, the time and space units of measurement are chosen in such a way that an object moving at the speed of light is depicted as following a 45° angle to the diagram's axes.
-A particular Minkowski diagram illustrates the result of a Lorentz transformation. The Lorentz transformation relates two inertial frames of reference, where an observer stationary at the event (0, 0) makes a change of velocity along the x-axis. As shown in Fig 2-1, the new time axis of the observer forms an angle α with the previous time axis, with α < π/4. In the new frame of reference the simultaneous events lie parallel to a line inclined by α to the previous lines of simultaneity. This is the new x-axis. Both the original set of axes and the primed set of axes have the property that they are orthogonal with respect to the Minkowski inner product or relativistic dot product.
 A. The Minkowski diagram is used to define concepts and demonstrate properties of Newtonian mechanics and to provide geometrical interpretation to the generalization of Lorentz transformations to relativistic mechanics.
 B. The Minkowski diagram is used to define concepts and demonstrate properties of general relativity and to provide geometrical interpretation to the generalization of special relativity to relativistic mechanics.
 C. The Minkowski diagram is used to define concepts and demonstrate properties of Lorentz transformations and to provide geometrical interpretation to the generalization of quantum mechanics to relativistic mechanics.
 D. The Minkowski diagram is used to define concepts and demonstrate properties of special relativity and to provide geometrical interpretation to the generalization of general relativity to relativistic mechanics.
 E. The Minkowski diagram is used to define concepts and demonstrate properties of Lorentz transformations and to provide geometrical interpretation to the generalization of Newtonian mechanics to relativistic mechanics. "
What are the two main interpretations for the disparity between the presence of matter and antimatter in the observable universe?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What are the two main interpretations for the disparity between the presence of matter and antimatter in the observable universe?
 Context: -There are two main interpretations for this disparity: either the universe began with a small preference for matter (total baryonic number of the universe different from zero), or the universe was originally perfectly symmetric, but somehow a set of phenomena contributed to a small imbalance in favour of matter over time. The second point of view is preferred, although there is no clear experimental evidence indicating either of them to be the correct one.
-Matter–antimatter asymmetry. The universe is made out of mostly matter. However, the standard model predicts that matter and antimatter should have been created in (almost) equal amounts if the initial conditions of the universe did not involve disproportionate matter relative to antimatter. Yet, there is no mechanism in the Standard Model to sufficiently explain this asymmetry.
-There is strong evidence that the observable universe is composed almost entirely of ordinary matter, as opposed to an equal mixture of matter and antimatter. This asymmetry of matter and antimatter in the visible universe is one of the great unsolved problems in physics. The process by which this inequality between matter and antimatter particles developed is called baryogenesis.
-There is considerable speculation both in science and science fiction as to why the observable universe is apparently almost entirely matter (in the sense of quarks and leptons but not antiquarks or antileptons), and whether other places are almost entirely antimatter (antiquarks and antileptons) instead. In the early universe, it is thought that matter and antimatter were equally represented, and the disappearance of antimatter requires an asymmetry in physical laws called CP (charge-parity) symmetry violation, which can be obtained from the Standard Model, but at this time the apparent asymmetry of matter and antimatter in the visible universe is one of the great unsolved problems in physics. Possible processes by which it came about are explored in more detail under baryogenesis.
-According to the theory of the Big Bang, matter and antimatter would have existed in the same amount at the beginning of the Universe. If this was true, particles and antiparticles would have annihilated each other, creating photons, and thus the Universe would have been only compounded by light (one particle of matter for 1018 photons). However, only matter has remained and at a rate of one billion times more particles than expected. What happened then, for the antimatter to disappear in favor of matter? A possible answer to this question is baryogenesis, the hypothetical physical process that took place during the early universe that produced baryonic asymmetry, i.e. the imbalance of matter (baryons) and antimatter (antibaryons) in the observed universe. However, baryogenesis is only possible under the following conditions proposed by Andrei Sakharov in 1967: Baryon number  B violation.
 A. The universe began with a small preference for matter, or it was originally perfectly asymmetric, but a set of phenomena contributed to a small imbalance in favor of antimatter over time.
 B. The universe began with a small preference for antimatter, or it was originally perfectly symmetric, but a set of phenomena contributed to a small imbalance in favor of antimatter over time.
 C. The universe began with equal amounts of matter and antimatter, or it was originally perfectly symmetric, but a set of phenomena contributed to a small imbalance in favor of antimatter over time.
 D. The universe began with a small preference for matter, or it was originally perfectly symmetric, but a set of phenomena contributed to a small imbalance in favor of matter over time.
 E. The universe began with equal amounts of matter and antimatter, or it was originally perfectly asymmetric, but a set of phenomena contributed to a small imbalance in favor of matter over time. "
What is the Ramsauer-Townsend effect?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the Ramsauer-Townsend effect?
 Context: -The Ramsauer–Townsend effect, also sometimes called the Ramsauer effect or the Townsend effect, is a physical phenomenon involving the scattering of low-energy electrons by atoms of a noble gas. The effect can not be explained by classical mechanics, but requires the wave theory of quantum mechanics.
-If one tries to predict the probability of collision with a classical model that treats the electron and atom as hard spheres, one finds that the probability of collision should be independent of the incident electron energy (see Kukolich ). However, Ramsauer and Townsend observed that for slow-moving electrons in argon, krypton, or xenon, the probability of collision between the electrons and gas atoms obtains a minimum value for electrons with a certain amount of kinetic energy (about 1 electron volts for xenon gas). This is the Ramsauer–Townsend effect.
-Predicting from theory the kinetic energy that will produce a Ramsauer–Townsend minimum is quite complicated since the problem involves understanding the wave nature of particles. However, the problem has been extensively investigated both experimentally and theoretically and is well understood (see Johnson and Guet).
In 1970 Gryzinski has proposed classical explanation of Ramsauer effect using effective picture of atom as oscillating multipole of electric field (dipole, quadrupole, octupole), which was a consequence of his free-fall atomic model.
-No good explanation for the phenomenon existed until the introduction of quantum mechanics, which explains that the effect results from the wave-like properties of the electron. A simple model of the collision that makes use of wave theory can predict the existence of the Ramsauer–Townsend minimum. Bohr presents one such model that considers the atom as a finite square potential well.
-The effect is named for Carl Ramsauer (1879-1955) and John Sealy Townsend (1868-1957), who each independently studied the collisions between atoms and low-energy electrons in the early 1920s.
 A. The Ramsauer-Townsend effect is a physical phenomenon that involves the scattering of low-energy electrons by atoms of a non-noble gas. It can be explained by classical mechanics.
 B. The Ramsauer-Townsend effect is a physical phenomenon that involves the scattering of low-energy electrons by atoms of a noble gas. It requires the wave theory of quantum mechanics to be explained.
 C. The Ramsauer-Townsend effect is a physical phenomenon that involves the scattering of high-energy electrons by atoms of a noble gas. It can be explained by classical mechanics.
 D. The Ramsauer-Townsend effect is a physical phenomenon that involves the scattering of high-energy electrons by atoms of a non-noble gas. It requires the wave theory of quantum mechanics to be explained.
 E. The Ramsauer-Townsend effect is a physical phenomenon that involves the scattering of electrons by atoms of any gas. It can be explained by classical mechanics. "
What is Minkowski space?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is Minkowski space?
 Context: -For an overview, Minkowski space is a 4-dimensional real vector space equipped with a non-degenerate, symmetric bilinear form on the tangent space at each point in spacetime, here simply called the Minkowski inner product, with metric signature either (+ − − −) or (− + + +). The tangent space at each event is a vector space of the same dimension as spacetime, 4.
-In mathematical physics, Minkowski space (or Minkowski spacetime) () combines inertial space and time manifolds (x,y) with a non-inertial reference frame of space and time (x',t') into a four-dimensional model relating a position (inertial frame of reference) to the field (physics). A four-vector (x,y,z,t) consists of a coordinate axes such as a Euclidean space plus time. This may be used with the non-inertial frame to illustrate specifics of motion, but should not be confused with the spacetime model generally.  The model helps show how a spacetime interval between any two events is independent of the inertial frame of reference in which they are recorded. Mathematician Hermann Minkowski developed it from the work of Hendrik Lorentz, Henri Poincaré, and others, and said it ""was grown on experimental physical grounds.""  Minkowski space is closely associated with Einstein's theories of special relativity and general relativity and is the most common mathematical structure by which special relativity is formalized. While the individual components in Euclidean space and time might differ due to length contraction and time dilation, in Minkowski spacetime, all frames of reference will agree on the total interval in spacetime between events. Minkowski space differs from four-dimensional Euclidean space insofar as it treats time differently than the three spatial dimensions.
-Minkowski space (or Minkowski spacetime) is a mathematical setting in which special relativity is conveniently formulated. Minkowski space is named for the German mathematician Hermann Minkowski, who around 1907 realized that the theory of special relativity (previously developed by Poincaré and Einstein) could be elegantly described using a four-dimensional spacetime, which combines the dimension of time with the three dimensions of space.
-Introducing more terminology (but not more structure), Minkowski space is thus a pseudo-Euclidean space with total dimension n = 4 and signature (3, 1) or (1, 3). Elements of Minkowski space are called events. Minkowski space is often denoted R3,1 or R1,3 to emphasize the chosen signature, or just M. It is perhaps the simplest example of a pseudo-Riemannian manifold.
-In mathematics and physics, super Minkowski space or Minkowski superspace is a supersymmetric extension of Minkowski space, sometimes used as the base manifold (or rather, supermanifold) for superfields. It is acted on by the super Poincaré algebra.
 A. Minkowski space is a physical space where objects move in a straight line unless acted upon by a force.
 B. Minkowski space is a mathematical model that combines inertial space and time manifolds with a non-inertial reference frame of space and time into a four-dimensional model relating a position to the field.
 C. Minkowski space is a mathematical model that combines space and time into a two-dimensional model relating a position to the field.
 D. Minkowski space is a mathematical model that combines space and time into a three-dimensional model relating a position to the field.
 E. Minkowski space is a physical space where objects move in a curved line unless acted upon by a force. "
What is the Optical Signal-to-Noise Ratio (OSNR)?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the Optical Signal-to-Noise Ratio (OSNR)?
 Context: -Optical signals have a carrier frequency (about 200 THz and more) that is much higher than the modulation frequency. This way the noise covers a bandwidth that is much wider than the signal itself. The resulting signal influence relies mainly on the filtering of the noise. To describe the signal quality without taking the receiver into account, the optical SNR (OSNR) is used. The OSNR is the ratio between the signal power and the noise power in a given bandwidth. Most commonly a reference bandwidth of 0.1 nm is used. This bandwidth is independent of the modulation format, the frequency and the receiver. For instance an OSNR of 20 dB/0.1 nm could be given, even the signal of 40 GBit DPSK would not fit in this bandwidth. OSNR is measured with an optical spectrum analyzer.
-Signal-to-noise ratio is defined as the ratio of the power of a signal (meaningful input) to the power of background noise (meaningless or unwanted input): SNR=PsignalPnoise, where P is average power. Both signal and noise power must be measured at the same or equivalent points in a system, and within the same system bandwidth.
-In scientific imaging, the two-dimensional spectral signal-to-noise ratio (SSNR) is a signal-to-noise ratio measure which measures the normalised cross-correlation coefficient between several two-dimensional images over corresponding rings in Fourier space as a function of spatial frequency (Unser, Trus & Steven 1987). It is a multi-particle extension of the Fourier ring correlation (FRC), which is related to the Fourier shell correlation. The SSNR is a popular method for finding the resolution of a class average in cryo-electron microscopy.
-Peak signal-to-noise ratio (PSNR) is an engineering term for the ratio between the maximum possible power of a signal and the power of corrupting noise that affects the fidelity of its representation. Because many signals have a very wide dynamic range, PSNR is usually expressed as a logarithmic quantity using the decibel scale.
PSNR is commonly used to quantify reconstruction quality for images and video subject to lossy compression.
-Signal-to-noise ratio (SNR or S/N) is a measure used in science and engineering that compares the level of a desired signal to the level of background noise. SNR is defined as the ratio of signal power to noise power, often expressed in decibels. A ratio higher than 1:1 (greater than 0 dB) indicates more signal than noise.
 A. The Optical Signal-to-Noise Ratio (OSNR) is the ratio between the modulation frequency and the carrier frequency of an optical signal, used to describe the signal quality in systems where dynamic range is less than 6.02m.
 B. The Optical Signal-to-Noise Ratio (OSNR) is the ratio between the signal power and the noise power in a given bandwidth, used to describe the signal quality without taking the receiver into account.
 C. The Optical Signal-to-Noise Ratio (OSNR) is the ratio between the signal power and the noise power in a given bandwidth, used to describe the signal quality in situations where the dynamic range is less than 6.02m.
 D. The Optical Signal-to-Noise Ratio (OSNR) is the ratio between the signal power and the noise power in a fixed bandwidth of 6.02m, used to describe the signal quality in systems where dynamic range is less than 6.02m.
 E. The Optical Signal-to-Noise Ratio (OSNR) is the ratio between the signal power and the noise power in a given bandwidth, used to describe the signal quality in situations where the dynamic range is large or unpredictable. "
What is the interpretation of supersymmetry in stochastic supersymmetric theory?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the interpretation of supersymmetry in stochastic supersymmetric theory?
 Context: -Spontaneous breakdown of a topological supersymmetry Kinematic dynamo can be also viewed as the phenomenon of the spontaneous breakdown of the topological supersymmetry of the associated stochastic differential equation related to the flow of the background matter. Within stochastic supersymmetric theory, this supersymmetry is an intrinsic property of all stochastic differential equations, its interpretation is that the model’s phase space preserves continuity via continuous time flows. When the continuity of that flow spontaneously breaks down, the system is in the stochastic state of deterministic chaos. In other words, kinematic dynamo arises because of chaotic flow in the underlying background matter.
-Supersymmetric theory of stochastic dynamics or stochastics (STS) is an exact theory of stochastic (partial) differential equations (SDEs), the class of mathematical models with the widest applicability covering, in particular, all continuous time dynamical systems, with and without noise. The main utility of the theory from the physical point of view is a rigorous theoretical explanation of the ubiquitous spontaneous long-range dynamical behavior that manifests itself across disciplines via such phenomena as 1/f, flicker, and crackling noises and the power-law statistics, or Zipf's law, of instantonic processes like earthquakes and neuroavalanches. From the mathematical point of view, STS is interesting because it bridges the two major parts of mathematical physics – the dynamical systems theory and topological field theories. Besides these and related disciplines such as algebraic topology and supersymmetric field theories, STS is also connected with the traditional theory of stochastic differential equations and the theory of pseudo-Hermitian operators.
-In a supersymmetric theory the equations for force and the equations for matter are identical. In theoretical and mathematical physics, any theory with this property has the principle of supersymmetry (SUSY). Dozens of supersymmetric theories exist. Supersymmetry is a spacetime symmetry between two basic classes of particles: bosons, which have an integer-valued spin and follow Bose–Einstein statistics, and fermions, which have a half-integer-valued spin and follow Fermi–Dirac statistics.In supersymmetry, each particle from one class would have an associated particle in the other, known as its superpartner, the spin of which differs by a half-integer. For example, if the electron exists in a supersymmetric theory, then there would be a particle called a selectron (superpartner electron), a bosonic partner of the electron. In the simplest supersymmetry theories, with perfectly ""unbroken"" supersymmetry, each pair of superpartners would share the same mass and internal quantum numbers besides spin. More complex supersymmetry theories have a spontaneously broken symmetry, allowing superpartners to differ in mass.Supersymmetry has various applications to different areas of physics, such as quantum mechanics, statistical mechanics, quantum field theory, condensed matter physics, nuclear physics, optics, stochastic dynamics, astrophysics, quantum gravity, and cosmology. Supersymmetry has also been applied to high energy physics, where a supersymmetric extension of the Standard Model is a possible candidate for physics beyond the Standard Model. However, no supersymmetric extensions of the Standard Model have been experimentally verified.
-Supersymmetric theory of stochastic dynamics can be interesting in different ways. For example, STS offers a promising realization of the concept of supersymmetry. In general, there are two major problems in the context of supersymmetry. The first is establishing connections between this mathematical entity and the real world. Within STS, supersymmetry is the most common symmetry in nature because it is pertinent to all continuous time dynamical systems. The second is the spontaneous breakdown of supersymmetry. This problem is particularly important for particle physics because supersymmetry of elementary particles, if exists at extremely short scale, must be broken spontaneously at large scale. This problem is nontrivial because supersymmetries are hard to break spontaneously, the very reason behind the introduction of soft or explicit supersymmetry breaking. Within STS, spontaneous breakdown of supersymmetry is indeed a nontrivial dynamical phenomenon that has been variously known across disciplines as chaos, turbulence, self-organized criticality etc.
-The main idea of the theory is to study, instead of trajectories, the SDE-defined temporal evolution of differential forms. This evolution has an intrinsic BRST or topological supersymmetry representing the preservation of topology and/or the concept of proximity in the phase space by continuous time dynamics. The theory identifies a model as chaotic, in the generalized, stochastic sense, if its ground state is not supersymmetric, i.e., if the supersymmetry is broken spontaneously. Accordingly, the emergent long-range behavior that always accompanies dynamical chaos and its derivatives such as turbulence and self-organized criticality can be understood as a consequence of the Goldstone theorem.
 A. Supersymmetry is a type of hydromagnetic dynamo that arises when the magnetic field becomes strong enough to affect the fluid motions.
 B. Supersymmetry is a measure of the amplitude of the dynamo in the induction equation of the kinematic approximation.
 C. Supersymmetry is a measure of the strength of the magnetic field in the induction equation of the kinematic dynamo.
 D. Supersymmetry is a property of deterministic chaos that arises from the continuity of the flow in the model's phase space.
 E. Supersymmetry is an intrinsic property of all stochastic differential equations, and it preserves continuity in the model's phase space via continuous time flows. "
"What is the purpose of expressing a map's scale as a ratio, such as 1:10,000?","Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the purpose of expressing a map's scale as a ratio, such as 1:10,000?
 Context: -South-up map orientation is the orientation of a map with south up, at the top of the map, amounting to a 180-degree rotation of the map from the standard convention of north-up. Maps in this orientation are sometimes called upside down maps or reversed maps.Other maps with non-standard orientation include T and O maps, polar maps, and Dymaxion maps.
-Many maps are drawn to a scale expressed as a ratio, such as 1:10,000, which means that 1 unit of measurement on the map corresponds to 10,000 of that same unit on the ground. The scale statement can be accurate when the region mapped is small enough for the curvature of the Earth to be neglected, such as a city map. Mapping larger regions, where the curvature cannot be ignored, requires projections to map from the curved surface of the Earth to the plane. The impossibility of flattening the sphere to the plane without distortion means that the map cannot have a constant scale. Rather, on most projections, the best that can be attained is an accurate scale along one or two paths on the projection. Because scale differs everywhere, it can only be measured meaningfully as point scale per location. Most maps strive to keep point scale variation within narrow bounds. Although the scale statement is nominal it is usually accurate enough for most purposes unless the map covers a large fraction of the earth. At the scope of a world map, scale as a single number is practically meaningless throughout most of the map. Instead, it usually refers to the scale along the equator.
-An origin must be assigned to a specific spatial location or landmark, and the orientation of the axes must be defined using available directional cues for all but one axis.Consider as an example superimposing 3D Cartesian coordinates over all points on the Earth (that is, geospatial 3D). Kilometers are a good choice of units, since the original definition of the kilometer was geospatial, with 10,000 km equaling the surface distance from the equator to the North Pole. Based on symmetry, the gravitational center of the Earth suggests a natural placement of the origin (which can be sensed via satellite orbits). The axis of Earth's rotation provides a natural orientation for the X, Y, and Z axes, strongly associated with ""up vs. down"", so positive Z can adopt the direction from the geocenter to the North Pole. A location on the equator is needed to define the X-axis, and the prime meridian stands out as a reference orientation, so the X-axis takes the orientation from the geocenter out to 0 degrees longitude, 0 degrees latitude. With three dimensions, and two perpendicular axes orientations pinned down for X and Z, the Y-axis is determined by the first two choices. In order to obey the right-hand rule, the Y-axis must point out from the geocenter to 90 degrees longitude, 0 degrees latitude. From a longitude of −73.985656 degrees, a latitude 40.748433 degrees, and Earth radius of 40,000/2π km, and transforming from spherical to Cartesian coordinates, one can estimate the geocentric coordinates of the Empire State Building, (x, y, z) = (1,330.53 km, 4,635.75 km, 4,155.46 km). GPS navigation relies on such geocentric coordinates.
-Map scales require careful discussion. A town plan may be constructed as an exact scale drawing, but for larger areas a map projection is necessary and no projection can represent the Earth's surface at a uniform scale. In general the scale of a projection depends on position and direction. The variation of scale may be considerable in small scale maps which may cover the globe. In large scale maps of small areas the variation of scale may be insignificant for most purposes but it is always present. The scale of a map projection must be interpreted as a nominal scale. (The usage large and small in relation to map scales relates to their expressions as fractions. The fraction 1/10,000 used for a local map is much larger than the1/100,000,000 used for a global map. There is no fixed dividing line between small and large scales.) A scale model is a representation or copy of an object that is larger or smaller than the actual size of the object being represented. Very often the scale model is smaller than the original and used as a guide to making the object in full size.
-Throughout history, maps have been made with varied orientations, and reversing the orientation of maps is technically very easy to do. As such, some cartographers maintain that the issue of south-up map orientation is itself trivial. More noteworthy than the technical matter of orientation, per se, is the history of explicitly using south-up map orientation as a political statement, that is, creating south-up oriented maps with the express rationale of reacting to the north-up oriented world maps that have dominated map publication during the modern age.
 A. To indicate the use of south-up orientation, as used in Ancient Africa and some maps in Brazil today.
 B. To indicate the orientation of the map, such as whether the 0° meridian is at the top or bottom of the page.
 C. To indicate the projection used to create the map, such as Buckminster Fuller's Dymaxion projection.
 D. To indicate the arrangement of the map, such as the world map of Gott, Vanderbei, and Goldberg arranged as a pair of disks back-to-back.
 E. To indicate the relationship between the size of the map and the size of the area being represented. "
What is the main sequence in astronomy?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the main sequence in astronomy?
 Context: -In astronomy, the main sequence is a continuous and distinctive band of stars that appears on plots of stellar color versus brightness. These color-magnitude plots are known as Hertzsprung–Russell diagrams after their co-developers, Ejnar Hertzsprung and Henry Norris Russell. Stars on this band are known as main-sequence stars or dwarf stars. These are the most numerous true stars in the universe and include the Sun.
-main sequence A category of stars which form a continuous and distinctive band on plots of stellar temperature versus luminosity, in particular the Hertzsprung–Russell diagram. These stars are characterized by being in hydrostatic equilibrium and undergoing nuclear fusion of hydrogen-1 in their core region. The Sun is a main-sequence star.
major axis See semi-major axis.
March equinox Also the Northward equinox.
-The main sequence luminosity function maps the distribution of main sequence stars according to their luminosity. It is used to compare star formation and death rates, and evolutionary models, with observations. Main sequence luminosity functions vary depending on their host galaxy and on selection criteria for the stars, for example in the Solar neighbourhood or the Small Magellanic Cloud.
-This is a single-lined spectroscopic binary star system in a circular orbit with an orbital period of 3.7005 days. It is an ellipsoidal variable, which means the orbit is sufficiently close that the shapes of the components are being distorted by their mutual gravitation. This is causing the visual magnitude of the system to vary regularly by 0m.05 over the course of each orbit, as the orientation of the stars change with respect to the Earth. Detailed analysis of the light curve suggests that the primary star is also pulsating and is probably a Slowly pulsating B-type star.The primary component is a B-type giant star with a stellar classification of B2 III. It is only about 16 million years old and spins with a projected rotational velocity of 90 km/s. Despite the spectral class, the primary star is thought to be at or near the end of its main sequence evolution. It has about 12.5 times the mass of the Sun and radiates 11,262 times the solar luminosity from its outer atmosphere at an effective temperature of 14,496 K.The secondary star is not detectable clearly, but modelling of the brightness variations and orbit suggest that it is a main sequence star with a spectral class of about B6. It is smaller, cooler, and much less luminous than the primary, and orbits at about 26 astronomical units.
-A new star will sit at a specific point on the main sequence of the Hertzsprung–Russell diagram, with the main-sequence spectral type depending upon the mass of the star. Small, relatively cold, low-mass red dwarfs fuse hydrogen slowly and will remain on the main sequence for hundreds of billions of years or longer, whereas massive, hot O-type stars will leave the main sequence after just a few million years. A mid-sized yellow dwarf star, like the Sun, will remain on the main sequence for about 10 billion years. The Sun is thought to be in the middle of its main sequence lifespan.
 A. The main sequence is a type of galaxy that contains a large number of stars.
 B. The main sequence is a type of black hole that is formed from the collapse of a massive star.
 C. The main sequence is a continuous and distinctive band of stars that appears on plots of stellar color versus brightness. Stars on this band are known as main-sequence stars or dwarf stars.
 D. The main sequence is a group of planets that orbit around a star in a solar system.
 E. The main sequence is a type of nebula that is formed from the explosion of a supernova. "
"Who proposed the concept of ""maximal acceleration""?","Answer the following question with A, B, C, D, or E. You can use the context provided. Question: Who proposed the concept of ""maximal acceleration""?
 Context: -The gradual acceptance of Einstein's theories of relativity and the quantized nature of light transmission, and of Niels Bohr's model of the atom created as many problems as they solved, leading to a full-scale effort to reestablish physics on new fundamental principles. Expanding relativity to cases of accelerating reference frames (the ""general theory of relativity"") in the 1910s, Einstein posited an equivalence between the inertial force of acceleration and the force of gravity, leading to the conclusion that space is curved and finite in size, and the prediction of such phenomena as gravitational lensing and the distortion of time in gravitational fields.
-Early researchers (before the 1950s): Max Born Albert Einstein Niels Bohr J. S. Bell Hugh Everett III David Bohm1950s–2010s: Roland Omnès W. H. Zurek Erich Joos Max Tegmark Maximilian Schlosshauer H. D. Zeh David Deutsch Robert B. Griffiths Bernard d'Espagnat Carl von Weizsäcker2000s or later: Bob Coecke Robert Spekkens 
-Albert Einstein presented the theories of special relativity and general relativity in publications that either contained no formal references to previous literature, or referred only to a small number of his predecessors for fundamental results on which he based his theories, most notably to the work of Henri Poincaré and Hendrik Lorentz for special relativity, and to the work of David Hilbert, Carl F. Gauss, Bernhard Riemann, and Ernst Mach for general relativity. Subsequently, claims have been put forward about both theories, asserting that they were formulated, either wholly or in part, by others before Einstein. At issue is the extent to which Einstein and various other individuals should be credited for the formulation of these theories, based on priority considerations.
-Albert Einstein presented the theories of special relativity and general relativity in publications that either contained no formal references to previous literature, or referred only to a small number of his predecessors for fundamental results on which he based his theories, most notably to the work of Henri Poincaré and Hendrik Lorentz for special relativity, and to the work of David Hilbert, Carl F. Gauss, Bernhard Riemann, and Ernst Mach for general relativity. Subsequently, claims have been put forward about both theories, asserting that they were formulated, either wholly or in part, by others before Einstein. At issue is the extent to which Einstein and various other individuals should be credited for the formulation of these theories, based on priority considerations.
-The idea to use centrifugal acceleration to simulate increased gravitational acceleration was first proposed by Phillips (1869). Pokrovsky and Fedorov (1936) in the Soviet Union and Bucky (1931) in the United States were the first to implement the idea. Andrew N. Schofield (e.g. Schofield 1980) played a key role in modern development of centrifuge modeling.
 A. Max Planck
 B. Niels Bohr
 C. Eduardo R. Caianiello
 D. Hideki Yukawa
 E. Albert Einstein "
What is indirect photophoresis?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is indirect photophoresis?
 Context: -Direct photophoresis is caused by the transfer of photon momentum to a particle by refraction and reflection. Movement of particles in the forward direction occurs when the particle is transparent and has an index of refraction larger compared to its surrounding medium. Indirect photophoresis occurs as a result of an increase in the kinetic energy of molecules when particles absorb incident light only on the irradiated side, thus creating a temperature gradient within the particle. In this situation the surrounding gas layer reaches temperature equilibrium with the surface of the particle. Molecules with higher kinetic energy in the region of higher gas temperature impinge on the particle with greater momenta than molecules in the cold region; this causes a migration of particles in a direction opposite to the surface temperature gradient. The component of the photophoretic force responsible for this phenomenon is called the radiometric force. This comes as a result of uneven distribution of radiant energy (source function within a particle).
-Photophoresis denotes the phenomenon that small particles suspended in gas (aerosols) or liquids (hydrocolloids) start to migrate when illuminated by a sufficiently intense beam of light. The existence of this phenomenon is owed to a non-uniform distribution of temperature of an illuminated particle in a fluid medium. Separately from photophoresis, in a fluid mixture of different kinds of particles, the migration of some kinds of particles may be due to differences in their absorptions of thermal radiation and other thermal effects collectively known as thermophoresis. In laser photophoresis, particles migrate once they have a refractive index different from their surrounding medium. The migration of particles is usually possible when the laser is slightly or not focused. A particle with a higher refractive index compared to its surrounding molecule moves away from the light source due to momentum transfer from absorbed and scattered light photons. This is referred to as a radiation pressure force. This force depends on light intensity and particle size but has nothing to do with the surrounding medium. Just like in Crookes radiometer, light can heat up one side and gas molecules bounce from that surface with greater velocity, hence push the particle to the other side. Under certain conditions, with particles of diameter comparable to the wavelength of light, the phenomenon of a negative indirect photophoresis occurs, due to the unequal heat generation on the laser irradiation between the back and front sides of particles, this produces a temperature gradient in the medium around the particle such that molecules at the far side of the particle from the light source may get to heat up more, causing the particle to move towards the light source.If the suspended particle is rotating, it will also experience the Yarkovsky effect.
-Indirect photophoretic force depends on the physical properties of the particle and the surrounding medium.
-Electrophoresis involves the migration of macromolecules under the influence of an electric field. Electrophoretic light scattering involves passing an electric field through a liquid which makes particles move. The bigger the charge is on the particles, the faster they are able to move.
-Electrophoresis involves the migration of macromolecules under the influence of an electric field. Electrophoretic light scattering involves passing an electric field through a liquid which makes particles move. The bigger the charge is on the particles, the faster they are able to move.
 A. Indirect photophoresis is a phenomenon that occurs when particles absorb incident light uniformly, creating a temperature gradient within the particle, and causing a migration of particles in a random direction.
 B. Indirect photophoresis is a phenomenon that occurs when particles absorb incident light only on the irradiated side, creating a temperature gradient within the particle, and causing a migration of particles in the same direction as the surface temperature gradient.
 C. Indirect photophoresis is a phenomenon that occurs when particles absorb incident light uniformly, creating a temperature gradient within the particle, and causing a migration of particles in the same direction as the surface temperature gradient.
 D. Indirect photophoresis is a phenomenon that occurs when particles absorb incident light only on the irradiated side, creating a temperature gradient within the particle, and causing a migration of particles in a direction opposite to the surface temperature gradient.
 E. Indirect photophoresis is a phenomenon that occurs when particles absorb incident light uniformly, creating a temperature gradient within the particle, and causing a migration of particles in a direction opposite to the surface temperature gradient. "
What does Earnshaw's theorem state?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What does Earnshaw's theorem state?
 Context: -Earnshaw's theorem states that a collection of point charges cannot be maintained in a stable stationary equilibrium configuration solely by the electrostatic interaction of the charges. This was first proven by British mathematician Samuel Earnshaw in 1842.
It is usually cited in reference to magnetic fields, but was first applied to electrostatic fields.
Earnshaw's theorem applies to classical inverse-square law forces (electric and gravitational) and also to the magnetic forces of permanent magnets, if the magnets are hard (the magnets do not vary in strength with external fields). Earnshaw's theorem forbids magnetic levitation in many common situations.
If the materials are not hard, Braunbeck's extension shows that materials with relative magnetic permeability greater than one (paramagnetism) are further destabilising, but materials with a permeability less than one (diamagnetic materials) permit stable configurations.
-Similar to point masses, in electromagnetism physicists discuss a point charge, a point particle with a nonzero electric charge. The fundamental equation of electrostatics is Coulomb's law, which describes the electric force between two point charges. Another result, Earnshaw's theorem, states that a collection of point charges cannot be maintained in a static equilibrium configuration solely by the electrostatic interaction of the charges. The electric field associated with a classical point charge increases to infinity as the distance from the point charge decreases towards zero, which suggests that the model is no longer accurate in this limit.
-For quite some time, Earnshaw's theorem posed a startling question of why matter is stable and holds together, since much evidence was found that matter was held together electromagnetically despite the proven instability of static charge configurations. Since Earnshaw's theorem only applies to stationary charges, there were attempts to explain stability of atoms using planetary models, such as Nagaoka's Saturnian model (1904) and Rutherford's planetary model (1911), where the point electrons are circling a positive point charge in the center. Yet, the stability of such planetary models was immediately questioned: electrons have nonzero acceleration when moving along a circle, and hence they would radiate the energy via a non-stationary electromagnetic field. Bohr's model of 1913 formally prohibited this radiation without giving an explanation for its absence.
-Detailed proofs Earnshaw's theorem was originally formulated for electrostatics (point charges) to show that there is no stable configuration of a collection of point charges. The proofs presented here for individual dipoles should be generalizable to collections of magnetic dipoles because they are formulated in terms of energy, which is additive. A rigorous treatment of this topic is, however, currently beyond the scope of this article.
-Earnshaw's theorem proved conclusively that it is not possible to levitate stably using only static, macroscopic, paramagnetic fields. The forces acting on any paramagnetic object in any combinations of gravitational, electrostatic, and magnetostatic fields will make the object's position, at best, unstable along at least one axis, and it can be in unstable equilibrium along all axes. However, several possibilities exist to make levitation viable, for example, the use of electronic stabilization or diamagnetic materials (since relative magnetic permeability is less than one); it can be shown that diamagnetic materials are stable along at least one axis, and can be stable along all axes. Conductors can have a relative permeability to alternating magnetic fields of below one, so some configurations using simple AC-driven electromagnets are self stable.
 A. A collection of point charges can be maintained in a stable stationary equilibrium configuration solely by the gravitational interaction of the charges.
 B. A collection of point charges can be maintained in a stable stationary equilibrium configuration solely by the electrostatic interaction of the charges.
 C. A collection of point charges can be maintained in a stable stationary equilibrium configuration solely by the magnetic interaction of the charges, if the magnets are hard.
 D. A collection of point charges cannot be maintained in a stable stationary equilibrium configuration solely by the electrostatic interaction of the charges.
 E. A collection of point charges can be maintained in a stable stationary equilibrium configuration solely by the magnetic interaction of the charges. "
What is radiosity in radiometry?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is radiosity in radiometry?
 Context: -In radiometry, radiosity is the radiant flux leaving (emitted, reflected and transmitted by) a surface per unit area, and spectral radiosity is the radiosity of a surface per unit frequency or wavelength, depending on whether the spectrum is taken as a function of frequency or of wavelength. The SI unit of radiosity is the watt per square metre (W/m2), while that of spectral radiosity in frequency is the watt per square metre per hertz (W·m−2·Hz−1) and that of spectral radiosity in wavelength is the watt per square metre per metre (W·m−3)—commonly the watt per square metre per nanometre (W·m−2·nm−1). The CGS unit erg per square centimeter per second (erg·cm−2·s−1) is often used in astronomy. Radiosity is often called intensity in branches of physics other than radiometry, but in radiometry this usage leads to confusion with radiant intensity.
-The radiosity method, in the context of computer graphics, derives from (and is fundamentally the same as) the radiosity method in heat transfer. In this context, radiosity is the total radiative flux (both reflected and re-radiated) leaving a surface; this is also sometimes known as radiant exitance. Calculation of radiosity, rather than surface temperatures, is a key aspect of the radiosity method that permits linear matrix methods to be applied to the problem.
-More correctly, radiosity B is the energy per unit area leaving the patch surface per discrete time interval and is the combination of emitted and reflected energy: cos cos ⁡θx′⋅Vis(x,x′)dA′ where: B(x)i dAi is the total energy leaving a small area dAi around a point x.
E(x)i dAi is the emitted energy.
ρ(x) is the reflectivity of the point, giving reflected energy per unit area by multiplying by the incident energy per unit area (the total energy which arrives from other patches).
S denotes that the integration variable x' runs over all the surfaces in the scene r is the distance between x and x'  θx and θx' are the angles between the line joining x and x' and vectors normal to the surface at x and x' respectively.
-In 3D computer graphics, radiosity is an application of the finite element method to solving the rendering equation for scenes with surfaces that reflect light diffusely. Unlike rendering methods that use Monte Carlo algorithms (such as path tracing), which handle all types of light paths, typical radiosity only account for paths (represented by the code ""LD*E"") which leave a light source and are reflected diffusely some number of times (possibly zero) before hitting the eye. Radiosity is a global illumination algorithm in the sense that the illumination arriving on a surface comes not just directly from the light sources, but also from other surfaces reflecting light. Radiosity is viewpoint independent, which increases the calculations involved, but makes them useful for all viewpoints.
-Radiosity is a method which attempts to simulate the way in which directly illuminated surfaces act as indirect light sources that illuminate other surfaces. This produces more realistic shading and seems to better capture the 'ambience' of an indoor scene. A classic example is a way that shadows 'hug' the corners of rooms.
The optical basis of the simulation is that some diffused light from a given point on a given surface is reflected in a large spectrum of directions and illuminates the area around it.
The simulation technique may vary in complexity. Many renderings have a very rough estimate of radiosity, simply illuminating an entire scene very slightly with a factor known as ambiance. However, when advanced radiosity estimation is coupled with a high quality ray tracing algorithm, images may exhibit convincing realism, particularly for indoor scenes.
 A. Radiosity is the radiant flux entering a surface per unit area, including emitted, reflected, and transmitted radiation.
 B. Radiosity is the radiant flux entering a surface per unit area, including absorbed, reflected, and transmitted radiation.
 C. Radiosity is the radiant flux leaving a surface per unit area, including absorbed, reflected, and transmitted radiation.
 D. Radiosity is the radiant flux leaving a surface per unit area, including emitted, reflected, and transmitted radiation.
 E. Radiosity is the radiant flux leaving a surface per unit volume, including emitted, reflected, and transmitted radiation. "
What is a virtual particle?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is a virtual particle?
 Context: -A virtual particle is a theoretical transient particle that exhibits some of the characteristics of an ordinary particle, while having its existence limited by the uncertainty principle. The concept of virtual particles arises in the perturbation theory of quantum field theory where interactions between ordinary particles are described in terms of exchanges of virtual particles. A process involving virtual particles can be described by a schematic representation known as a Feynman diagram, in which virtual particles are represented by internal lines.Virtual particles do not necessarily carry the same mass as the corresponding real particle, although they always conserve energy and momentum. The closer its characteristics come to those of ordinary particles, the longer the virtual particle exists. They are important in the physics of many processes, including particle scattering and Casimir forces. In quantum field theory, forces—such as the electromagnetic repulsion or attraction between two charges—can be thought of as due to the exchange of virtual photons between the charges. Virtual photons are the exchange particle for the electromagnetic interaction.
-In formal terms, a particle is considered to be an eigenstate of the particle number operator a†a, where a is the particle annihilation operator and a† the particle creation operator (sometimes collectively called ladder operators). In many cases, the particle number operator does not commute with the Hamiltonian for the system. This implies the number of particles in an area of space is not a well-defined quantity but, like other quantum observables, is represented by a probability distribution. Since these particles are not certain to exist, they are called virtual particles or vacuum fluctuations of vacuum energy. In a certain sense, they can be understood to be a manifestation of the time-energy uncertainty principle in a vacuum.An important example of the ""presence"" of virtual particles in a vacuum is the Casimir effect. Here, the explanation of the effect requires that the total energy of all of the virtual particles in a vacuum can be added together. Thus, although the virtual particles themselves are not directly observable in the laboratory, they do leave an observable effect: Their zero-point energy results in forces acting on suitably arranged metal plates or dielectrics. On the other hand, the Casimir effect can be interpreted as the relativistic van der Waals force.
-The concept of virtual particles arises in the perturbation theory of quantum field theory, an approximation scheme in which interactions (in essence, forces) between actual particles are calculated in terms of exchanges of virtual particles. Such calculations are often performed using schematic representations known as Feynman diagrams, in which virtual particles appear as internal lines. By expressing the interaction in terms of the exchange of a virtual particle with four-momentum q, where q is given by the difference between the four-momenta of the particles entering and leaving the interaction vertex, both momentum and energy are conserved at the interaction vertices of the Feynman diagram.: 119 A virtual particle does not precisely obey the energy–momentum relation m2c4 = E2 − p2c2. Its kinetic energy may not have the usual relationship to velocity. It can be negative.: 110  This is expressed by the phrase off mass shell.: 119  The probability amplitude for a virtual particle to exist tends to be canceled out by destructive interference over longer distances and times. As a consequence, a real photon is massless and thus has only two polarization states, whereas a virtual one, being effectively massive, has three polarization states.
-In particle physics, this inequality permits a qualitative understanding of virtual particles, which carry momentum. The exchange of virtual particles with real particles is responsible for the creation of all known fundamental forces (more accurately known as fundamental interactions). Virtual photons are also responsible for the electrostatic interaction between electric charges (which results in Coulomb's law), for spontaneous radiative decay of excited atomic and nuclear states, for the Casimir force, for the Van der Waals force and some other observable phenomena.
-Quantum tunnelling may be considered a manifestation of virtual particle exchanges.: 235  The range of forces carried by virtual particles is limited by the uncertainty principle, which regards energy and time as conjugate variables; thus, virtual particles of larger mass have more limited range.Written in the usual mathematical notations, in the equations of physics, there is no mark of the distinction between virtual and actual particles. The amplitudes of processes with a virtual particle interfere with the amplitudes of processes without it, whereas for an actual particle the cases of existence and non-existence cease to be coherent with each other and do not interfere any more. In the quantum field theory view, actual particles are viewed as being detectable excitations of underlying quantum fields. Virtual particles are also viewed as excitations of the underlying fields, but appear only as forces, not as detectable particles. They are ""temporary"" in the sense that they appear in some calculations, but are not detected as single particles. Thus, in mathematical terms, they never appear as indices to the scattering matrix, which is to say, they never appear as the observable inputs and outputs of the physical process being modelled.
 A. A particle that is not affected by the strong force.
 B. A particle that is not affected by the weak force.
 C. A particle that is created in a laboratory for experimental purposes.
 D. A particle that is not directly observable but is inferred from its effects on measurable particles.
 E. A particle that is directly observable and can be measured in experiments. "
"Who proposed the principle of ""complexity from noise"" and when was it first introduced?","Answer the following question with A, B, C, D, or E. You can use the context provided. Question: Who proposed the principle of ""complexity from noise"" and when was it first introduced?
 Context: -An early example of algorithm complexity analysis is the running time analysis of the Euclidean algorithm done by Gabriel Lamé in 1844.
Before the actual research explicitly devoted to the complexity of algorithmic problems started off, numerous foundations were laid out by various researchers. Most influential among these was the definition of Turing machines by Alan Turing in 1936, which turned out to be a very robust and flexible simplification of a computer.
-Complexity and chaos theory Complex systems theory is rooted in chaos theory, which in turn has its origins more than a century ago in the work of the French mathematician Henri Poincaré. Chaos is sometimes viewed as extremely complicated information, rather than as an absence of order. Chaotic systems remain deterministic, though their long-term behavior can be difficult to predict with any accuracy. With perfect knowledge of the initial conditions and the relevant equations describing the chaotic system's behavior, one can theoretically make perfectly accurate predictions of the system, though in practice this is impossible to do with arbitrary accuracy. Ilya Prigogine argued that complexity is non-deterministic and gives no way whatsoever to precisely predict the future.The emergence of complex systems theory shows a domain between deterministic order and randomness which is complex. This is referred to as the ""edge of chaos"".
-The cybernetician William Ross Ashby formulated the original principle of self-organization in 1947. It states that any deterministic dynamic system automatically evolves towards a state of equilibrium that can be described in terms of an attractor in a basin of surrounding states. Once there, the further evolution of the system is constrained to remain in the attractor. This constraint implies a form of mutual dependency or coordination between its constituent components or subsystems. In Ashby's terms, each subsystem has adapted to the environment formed by all other subsystems.The cybernetician Heinz von Foerster formulated the principle of ""order from noise"" in 1960. It notes that self-organization is facilitated by random perturbations (""noise"") that let the system explore a variety of states in its state space. This increases the chance that the system will arrive into the basin of a ""strong"" or ""deep"" attractor, from which it then quickly enters the attractor itself. The biophysicist Henri Atlan developed this concept by proposing the principle of ""complexity from noise"" (French: le principe de complexité par le bruit) first in the 1972 book L'organisation biologique et la théorie de l'information and then in the 1979 book Entre le cristal et la fumée. The physicist and chemist Ilya Prigogine formulated a similar principle as ""order through fluctuations"" or ""order out of chaos"". It is applied in the method of simulated annealing for problem solving and machine learning.
-The composer Iannis Xenakis (1960) was the first to explicate a compositional theory for grains of sound. He began by adopting the following lemma: ""All sound, even continuous musical variation, is conceived as an assemblage of a large number of elementary sounds adequately disposed in time. In the attack, body, and decline of a complex sound, thousands of pure sounds appear in a more or less short interval of time  Δt ."" Xenakis created granular sounds using analog tone generators and tape splicing. These appear in the composition Analogique A-B for string orchestra and tape (1959).
-Since the ground-breaking 1965 paper by Juris Hartmanis and Richard E. Stearns and the 1979 book by Michael Garey and David S. Johnson on NP-completeness, the term ""computational complexity"" (of algorithms) has become commonly referred to as asymptotic computational complexity.  Further, unless specified otherwise, the term ""computational complexity"" usually refers to the upper bound for the asymptotic computational complexity of an algorithm or a problem, which is usually written in terms of the big O notation, e.g..  O(n3).
 A. Ilya Prigogine in 1979
 B. Henri Atlan in 1972
 C. Democritus and Lucretius in ancient times
 D. None of the above.
 E. René Descartes in 1637 "
What is the order parameter that breaks the electromagnetic gauge symmetry in superconductors?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the order parameter that breaks the electromagnetic gauge symmetry in superconductors?
 Context: -In superconductors, there is a condensed-matter collective field ψ, which acts as the order parameter breaking the electromagnetic gauge symmetry.
-Higgs mechanism The strong, weak, and electromagnetic forces can all be understood as arising from gauge symmetries, which is a redundancy in the description of the symmetry. The Higgs mechanism, the spontaneous symmetry breaking of gauge symmetries, is an important component in understanding the superconductivity of metals and the origin of particle masses in the standard model of particle physics.  The term ""spontaneous symmetry breaking"" is a misnomer here as Elitzur's theorem states that local gauge symmetries can never be spontaneously broken. Rather, after gauge fixing, the global symmetry (or redundancy) can be broken in a manner formally resembling spontaneous symmetry breaking.  One important consequence of the distinction between true symmetries and gauge symmetries, is that the massless Nambu–Goldstone resulting from spontaneous breaking of a gauge symmetry are absorbed in the description of the gauge vector field, providing massive vector field modes, like the plasma mode in a superconductor, or the Higgs mode observed in particle physics.
-The scanning SQUID microscope was originally developed for an experiment to test the pairing symmetry of the high-temperature cuprate superconductor YBCO. Standard superconductors are isotropic with respect to their superconducting properties, that is, for any direction of electron momentum  k in the superconductor, the magnitude of the order parameter and consequently the superconducting energy gap will be the same. However, in the high-temperature cuprate superconductors, the order parameter instead follows the equation  Δ(k)=Δ0(cos(kxa)−(kya)) , meaning that when crossing over any of the [110] directions in momentum space one will observe a sign change in the order parameter. The form of this function is equal to that of the l = 2 spherical harmonic function, giving it the name d-wave superconductivity. As the superconducting electrons are described by a single coherent wavefunction, proportional to exp(-iφ), where φ is known as the phase of the wavefunction, this property can be also interpreted as a phase shift of π under a 90 degree rotation.
-Based on Landau's previously established theory of second-order phase transitions, Ginzburg and Landau argued that the free energy density  fs of a superconductor near the superconducting transition can be expressed in terms of a complex order parameter field  ψ(r)=|ψ(r)|eiϕ(r) , where the quantity  |ψ(r)|2 is a measure of the local density of superconducting electrons  ns(r) analogous to a quantum mechanical wave function. While  ψ(r) is nonzero below a phase transition into a superconducting state, no direct interpretation of this parameter was given in the original paper. Assuming smallness of  |ψ| and smallness of its gradients, the free energy density has the form of a field theory and exhibits U(1) gauge symmetry: where fn is the free energy density of the normal phase, α and  β are phenomenological parameters, m∗ is an effective mass, e∗ is an effective charge (usually  2e , where e is the charge of an electron), A is the magnetic vector potential, and B=∇×A is the magnetic field.The total free energy is given by  F=∫fsd3r . By minimizing  F with respect to variations in the order parameter  ψ and the vector potential  A , one arrives at the Ginzburg–Landau equations where  J denotes the dissipation-less electric current density and Re the real part. The first equation — which bears some similarities to the time-independent Schrödinger equation, but is principally different due to a nonlinear term — determines the order parameter,  ψ . The second equation then provides the superconducting current.
-The original idea on the parameter κ belongs to Landau. The ratio κ = λ/ξ is presently known as the Ginzburg–Landau parameter. It has been proposed by Landau that Type I superconductors are those with 0 < κ < 1/√2, and Type II superconductors those with κ > 1/√2.
 A. None of the above.
 B. A thin cylindrical plastic rod.
 C. A condensed-matter collective field ψ.
 D. The cosmic microwave background.
 E. A component of the Higgs field. "
What is the reason for the sun appearing slightly yellowish when viewed from Earth?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the reason for the sun appearing slightly yellowish when viewed from Earth?
 Context: -As a tropospheric cloud matures, the dense water droplets may combine to produce larger droplets, which may combine to form droplets large enough to fall as rain. By this process of accumulation, the space between droplets becomes increasingly larger, permitting light to penetrate farther into the cloud. If the cloud is sufficiently large and the droplets within are spaced far enough apart, it may be that a percentage of the light which enters the cloud is not reflected back out before it is absorbed. A simple example of this is being able to see farther in heavy rain than in heavy fog. This process of reflection/absorption is what causes the range of cloud color from white to black.Other colors occur naturally in clouds. Bluish-grey is the result of light scattering within the cloud. In the visible spectrum, blue and green are at the short end of light's visible wavelengths, while red and yellow are at the long end. The short rays are more easily scattered by water droplets, and the long rays are more likely to be absorbed. The bluish color is evidence that such scattering is being produced by rain-sized droplets in the cloud. A cumulonimbus cloud emitting green is a sign that it is a severe thunderstorm, capable of heavy rain, hail, strong winds and possible tornadoes. The exact cause of green thunderstorms is still unknown, but it could be due to the combination of reddened sunlight passing through very optically thick clouds. Yellowish clouds may occur in the late spring through early fall months during forest fire season. The yellow color is due to the presence of pollutants in the smoke. Yellowish clouds caused by the presence of nitrogen dioxide are sometimes seen in urban areas with high air pollution levels.Red, orange and pink clouds occur almost entirely at sunrise and sunset and are the result of the scattering of sunlight by the atmosphere. When the angle between the Sun and the horizon is less than 10 percent, as it is just after sunrise or just prior to sunset, sunlight becomes too red due to refraction for any colors other than those with a reddish hue to be seen. The clouds do not become that color; they are reflecting long and unscattered rays of sunlight, which are predominant at those hours. The effect is much like if a person were to shine a red spotlight on a white sheet. In combination with large, mature thunderheads this can produce blood-red clouds. Clouds look darker in the near-infrared because water absorbs solar radiation at those wavelengths.
-Except for direct sunlight, most of the light in the daytime sky is caused by scattering, which is dominated by a small-particle limit called Rayleigh scattering. The scattering due to molecule-sized particles (as in air) is greater in the directions both toward and away from the source of light than it is in directions perpendicular to the incident path. Scattering is significant for light at all visible wavelengths, but is stronger at the shorter (bluer) end of the visible spectrum, meaning that the scattered light is bluer than its source: the Sun. The remaining direct sunlight, having lost some of its shorter-wavelength components, appears slightly less blue.Scattering also occurs even more strongly in clouds. Individual water droplets refract white light into a set of colored rings. If a cloud is thick enough, scattering from multiple water droplets will wash out the set of colored rings and create a washed-out white color.The sky can turn a multitude of colors such as red, orange, purple, and yellow (especially near sunset or sunrise) when the light must travel a much longer path (or optical depth) through the atmosphere. Scattering effects also partially polarize light from the sky and are most pronounced at an angle 90° from the Sun. Scattered light from the horizon travels through as much as 38 times the air mass as does light from the zenith, causing a blue gradient looking vivid at the zenith and pale near the horizon. Red light is also scattered if there is enough air between the source and the observer, causing parts of the sky to change color as the Sun rises or sets. As the air mass nears infinity, scattered daylight appears whiter and whiter.Apart from the Sun, distant clouds or snowy mountaintops may appear yellow. The effect is not very obvious on clear days, but is very pronounced when clouds cover the line of sight, reducing the blue hue from scattered sunlight. At higher altitudes, the sky tends toward darker colors since scattering is reduced due to lower air density. An extreme example is the Moon, where no atmospheric scattering occurs, making the lunar sky black even when the Sun is visible.Sky luminance distribution models have been recommended by the International Commission on Illumination (CIE) for the design of daylighting schemes. Recent developments relate to ""all sky models"" for modelling sky luminance under weather conditions ranging from clear to overcast.
-Sunlight and neutrinos The Sun emits light across the visible spectrum, so its color is white, with a CIE color-space index near (0.3, 0.3), when viewed from space or when the Sun is high in the sky. The Solar radiance per wavelength peaks in the green portion of the spectrum when viewed from space. When the Sun is very low in the sky, atmospheric scattering renders the Sun yellow, red, orange, or magenta, and in rare occasions even green or blue. Despite its typical whiteness (white sunrays, white ambient light, white illumination of the Moon, etc.), some cultures mentally picture the Sun as yellow and some even red; the reasons for this are cultural and exact ones are the subject of debate.
-The blue colour of the sky results from Rayleigh scattering, as the size of the gas particles in the atmosphere is much smaller than the wavelength of visible light. Rayleigh scattering is much greater for blue light than for other colours due to its shorter wavelength. As sunlight passes through the atmosphere, its blue component is Rayleigh scattered strongly by atmospheric gases but the longer wavelength (e.g. red/yellow) components are not. The sunlight arriving directly from the Sun therefore appears to be slightly yellow, while the light scattered through rest of the sky appears blue. During sunrises and sunsets, the effect of Rayleigh scattering on the spectrum of the transmitted light is much greater due to the greater distance the light rays have to travel through the high-density air near the Earth's surface.
-The color of light from the sky is a result of Rayleigh scattering of sunlight, which results in a perceived blue color. On a sunny day, Rayleigh scattering gives the sky a blue gradient, darkest around the zenith and brightest near the horizon. Light rays coming from the zenith take the shortest-possible path (1⁄38) through the air mass, yielding less scattering. Light rays coming from the horizon take the longest-possible path through the air, yielding more scattering.The blueness is at the horizon because the blue light coming from great distances is also preferentially scattered. This results in a red shift of the distant light sources that is compensated by the blue hue of the scattered light in the line of sight. In other words, the red light scatters also; if it does so at a point a great distance from the observer it has a much higher chance of reaching the observer than blue light. At distances nearing infinity, the scattered light is therefore white. Distant clouds or snowy mountaintops will seem yellow for that reason; that effect is not obvious on clear days, but very pronounced when clouds are covering the line of sight reducing the blue hue from scattered sunlight.
 A. The sun appears yellowish due to a reflection of the Earth's atmosphere.
 B. The longer wavelengths of light, such as red and yellow, are not scattered away and are directly visible when looking towards the sun.
 C. The sun appears yellowish due to the scattering of all colors of light, mainly blue and green, in the Earth's atmosphere.
 D. The sun emits a yellow light due to its own spectrum, which is visible when viewed from Earth.
 E. The atmosphere absorbs the shorter wavelengths of light, such as blue and red, leaving only the longer wavelengths of light, such as green and yellow, visible when looking towards the sun. "
What is the Landau-Lifshitz-Gilbert equation used for in physics?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the Landau-Lifshitz-Gilbert equation used for in physics?
 Context: -In physics, the Landau–Lifshitz–Gilbert equation, named for Lev Landau, Evgeny Lifshitz, and T. L. Gilbert, is a name used for a differential equation describing the precessional motion of magnetization M in a solid. It is a modification by Gilbert of the original equation of Landau and Lifshitz.
-In 1955 Gilbert replaced the damping term in the Landau–Lifshitz (LL) equation by one that depends on the time derivative of the magnetization: This is the Landau–Lifshitz–Gilbert (LLG) equation, where η is the damping parameter, which is characteristic of the material. It can be transformed into the Landau–Lifshitz equation: where and λ=γ2η1+γ2η2Ms2.
In this form of the LL equation, the precessional term γ' depends on the damping term. This better represents the behavior of real ferromagnets when the damping is large.
-In a ferromagnet, the magnitude of the magnetization M at each point equals the saturation magnetization Ms (although it can be smaller when averaged over a chunk of volume). The Landau–Lifshitz–Gilbert equation predicts the rotation of the magnetization in response to torques. An earlier, but equivalent, equation (the Landau–Lifshitz equation) was introduced by Landau & Lifshitz (1935): where γ is the electron gyromagnetic ratio and λ is a phenomenological damping parameter, often replaced by λ=αγMs, where α is a dimensionless constant called the damping factor. The effective field Heff is a combination of the external magnetic field, the demagnetizing field (magnetic field due to the magnetization), and some quantum mechanical effects. To solve this equation, additional equations for the demagnetizing field must be included.
-The purpose of dynamic micromagnetics is to predict the time evolution of the magnetic configuration of a sample subject to some non-steady conditions such as the application of a field pulse or an AC field. This is done by solving the Landau-Lifshitz-Gilbert equation, which is a partial differential equation describing the evolution of the magnetization in terms of the local effective field acting on it.
-With these considerations, the differential equation governing the behavior of a magnetic moment in the presence of an applied magnetic field with damping can be written in the most familiar form of the Landau-Lifshitz-Gilbert equation, d m d t = − γ μ 0 m × H e f f + α m ( m × d m d t ) {\frac {{\mathrm {d} }{\mathbf {m} }}{{\mathrm {d} }t}}=-\gamma \mu _{0}{\mathbf {m} }\times {\mathbf {H_{eff}} }+{\frac {\alpha }{m}}\left({\mathbf {m} }\times {\frac {{\mathrm {d} }{\mathbf {m} }}{{\mathrm {d} }t}}\right) .Since without damping  d m d t {\tfrac {{\mathrm {d} }{\mathbf {m} }}{{\mathrm {d} }t}} is directed perpendicular to both the moment and the field, the damping term of the Landau-Lifshitz-Gilbert equation provides for a change in the moment towards the applied field. The Landau-Lifshitz-Gilbert equation can also be written in terms of torques, d m d t = − γ ( τ + τ d ) {\frac {{\mathrm {d} }{\mathbf {m} }}{{\mathrm {d} }t}}=-\gamma \left({\boldsymbol {\tau }}+{\boldsymbol {\tau _{d}}}\right) ,where the damping torque is given by τ d = − α γ m ( m × d m d t ) {\boldsymbol {\tau _{d}}}=-{\frac {\alpha }{\gamma m}}\left({\mathbf {m} }\times {\frac {{\mathrm {d} }{\mathbf {m} }}{{\mathrm {d} }t}}\right) .By way of the micromagnetic theory, the Landau-Lifshitz-Gilbert equation also applies to the mesoscopic- and macroscopic-scale magnetization  M M of a sample by simple substitution, d M d t = − γ μ 0 M × H e f f + α M ( M × d M d t ) {\frac {{\mathrm {d} }{\mathbf {M} }}{{\mathrm {d} }t}}=-\gamma \mu _{0}{\mathbf {M} }\times {\mathbf {H_{eff}} }+{\frac {\alpha }{M}}\left({\mathbf {M} }\times {\frac {{\mathrm {d} }{\mathbf {M} }}{{\mathrm {d} }t}}\right) .
 A. The Landau-Lifshitz-Gilbert equation is a differential equation used to describe the precessional motion of magnetization M in a liquid, and is commonly used in micromagnetics to model the effects of a magnetic field on ferromagnetic materials.
 B. The Landau-Lifshitz-Gilbert equation is a differential equation used to describe the precessional motion of magnetization M in a solid, and is commonly used in astrophysics to model the effects of a magnetic field on celestial bodies.
 C. The Landau-Lifshitz-Gilbert equation is a differential equation used to describe the precessional motion of magnetization M in a solid, and is commonly used in micromagnetics to model the effects of a magnetic field on ferromagnetic materials.
 D. The Landau-Lifshitz-Gilbert equation is a differential equation used to describe the precessional motion of magnetization M in a solid, and is commonly used in macro-magnetics to model the effects of a magnetic field on ferromagnetic materials.
 E. The Landau-Lifshitz-Gilbert equation is a differential equation used to describe the precessional motion of magnetization M in a liquid, and is commonly used in macro-magnetics to model the effects of a magnetic field on ferromagnetic materials. "
What is spatial dispersion?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is spatial dispersion?
 Context: -In the physics of continuous media, spatial dispersion is a phenomenon where material parameters such as permittivity or conductivity have dependence on wavevector. Normally, such a dependence is assumed to be absent for simplicity, however spatial dispersion exists to varying degrees in all materials.
-In electromagnetics and optics, the term dispersion generally refers to aforementioned temporal or frequency dispersion. Spatial dispersion refers to the non-local response of the medium to the space; this can be reworded as the wavevector dependence of the permittivity. For an exemplary anisotropic medium, the spatial relation between electric and electric displacement field can be expressed as a convolution: Di(t,r)=Ei(t,r)+∫0∞∫fik(τ;r,r′)Ek(t−τ,r′)dV′dτ, where the kernel  fik is dielectric response (susceptibility); its indices make it in general a tensor to account for the anisotropy of the medium. Spatial dispersion is negligible in most macroscopic cases, where the scale of variation of  Ek(t−τ,r′) is much larger than atomic dimensions, because the dielectric kernel dies out at macroscopic distances. Nevertheless, it can result in non-negligible macroscopic effects, particularly in conducting media such as metals, electrolytes and plasmas. Spatial dispersion also plays role in optical activity and Doppler broadening, as well as in the theory of metamaterials.
-Spatial dispersion can be compared to temporal dispersion, the latter often just called dispersion. Temporal dispersion represents memory effects in systems, commonly seen in optics and electronics. Spatial dispersion on the other hand represents spreading effects and is usually significant only at microscopic length scales. Spatial dispersion contributes relatively small perturbations to optics, giving weak effects such as optical activity. Spatial dispersion and temporal dispersion may occur in the same system.
-In electromagnetism, spatial dispersion plays a role in a few material effects such as optical activity and doppler broadening. Spatial dispersion also plays an important role in the understanding of electromagnetic metamaterials. Most commonly, the spatial dispersion in permittivity ε is of interest.
Crystal optics Inside crystals there may be a combination of spatial dispersion, temporal dispersion, and anisotropy. The constitutive relation for the polarization vector can be written as: Pi(k→,ω)=∑j(ϵij(k→,ω)−ϵ0δij)Ej(k→,ω), i.e., the permittivity is a wavevector- and frequency-dependent tensor.
Considering Maxwell's equations, one can find the plane wave normal modes inside such crystals. These occur when the following relationship is satisfied for a nonzero electric field vector  E→ 0.
Spatial dispersion in  ϵ(k→,ω) can lead to strange phenomena, such as the existence of multiple modes at the same frequency and wavevector direction, but with different wavevector magnitudes.
Nearby crystal surfaces and boundaries, it is no longer valid to describe system response in terms of wavevectors. For a full description it is necessary to return to a full nonlocal response function (without translational symmetry), however the end effect can sometimes be described by ""additional boundary conditions"" (ABC's).
In isotropic media In materials that have no relevant crystalline structure, spatial dispersion can be important.
-The origin of spatial dispersion is nonlocal response, where response to a force field appears at many locations, and can appear even in locations where the force is zero. This usually arises due to a spreading of effects by the hidden microscopic degrees of freedom.As an example, consider the current  J(x,t) that is driven in response to an electric field  E(x,t) , which is varying in space (x) and time (t). Simplified laws such as Ohm's law would say that these are directly proportional to each other,  J=σE , but this breaks down if the system has memory (temporal dispersion) or spreading (spatial dispersion). The most general linear response is given by: J(x,t)=∫−∞−∞dx′∫−∞−∞dt′σ(x,x′,t,t′)E(x′,t′), where  σ(x,x′,t,t′)dx′dt′ is the nonlocal conductivity function.
 A. Spatial dispersion is a phenomenon in the physics of continuous media where material parameters such as permittivity or conductivity have dependence on time. It represents memory effects in systems, commonly seen in optics and electronics.
 B. Spatial dispersion is a phenomenon in the physics of continuous media where material parameters such as permittivity or conductivity have dependence on time. It represents spreading effects and is usually significant only at microscopic length scales.
 C. Spatial dispersion is a phenomenon in the physics of continuous media where material parameters such as permittivity or conductivity have no dependence on wavevector. It represents memory effects in systems, commonly seen in optics and electronics.
 D. Spatial dispersion is a phenomenon in the physics of continuous media where material parameters such as permittivity or conductivity have dependence on wavevector. It represents spreading effects and is usually significant only at microscopic length scales.
 E. Spatial dispersion is a phenomenon in the physics of continuous media where material parameters such as permittivity or conductivity have dependence on wavevector. It represents memory effects in systems, commonly seen in optics and electronics. "
What are the constituents of cold dark matter?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What are the constituents of cold dark matter?
 Context: -The constituents of cold dark matter are unknown. Possibilities range from large objects like MACHOs (such as black holes and Preon stars) or RAMBOs (such as clusters of brown dwarfs), to new particles such as [WIMPs and axions The 1997 DAMA/NaI experiment and its successor DAMA/LIBRA in 2013, claimed to directly detect dark matter particles passing through the Earth, but many researchers remain skeptical, as negative results from similar experiments seem incompatible with the DAMA results.
-Dark matter is detected through its gravitational interactions with ordinary matter and radiation. As such, it is very difficult to determine what the constituents of cold dark matter are. The candidates fall roughly into three categories: Axions, very light particles with a specific type of self-interaction that makes them a suitable CDM candidate. In recent years, axions have become one of the most promising candidates for dark matter. Axions have the theoretical advantage that their existence solves the strong CP problem in quantum chromodynamics, but axion particles have only been theorized and never detected. Axions are an example of a more general category of particle called a WISP (weakly interacting ""slender"" or ""slim"" particle), which are the low-mass counterparts of WIMPs.Massive compact halo objects (MACHOs), large, condensed objects such as black holes, neutron stars, white dwarfs, very faint stars, or non-luminous objects like planets. The search for these objects consists of using gravitational lensing to detect the effects of these objects on background galaxies. Most experts believe that the constraints from those searches rule out MACHOs as a viable dark matter candidate.Weakly interacting massive particles (WIMPs). There is no currently known particle with the required properties, but many extensions of the standard model of particle physics predict such particles. The search for WIMPs involves attempts at direct detection by highly sensitive detectors, as well as attempts at production of WIMPs by particle accelerators. Historically, WIMPs were regarded as one of the most promising candidates for the composition of dark matter, but in recent years WIMPs have since been supplanted by axions with the non-detection of WIMPs in experiments. The DAMA/NaI experiment and its successor DAMA/LIBRA have claimed to have directly detected dark matter particles passing through the Earth, but many scientists remain skeptical because no results from similar experiments seem compatible with the DAMA results.
-The main theoretical characteristics of a WIMP are: Interactions only through the weak nuclear force and gravity, or possibly other interactions with cross-sections no higher than the weak scale; Large mass compared to standard particles (WIMPs with sub-GeV masses may be considered to be light dark matter).Because of their lack of electromagnetic interaction with normal matter, WIMPs would be invisible through normal electromagnetic observations. Because of their large mass, they would be relatively slow moving and therefore ""cold"". Their relatively low velocities would be insufficient to overcome the mutual gravitational attraction, and as a result, WIMPs would tend to clump together. WIMPs are considered one of the main candidates for cold dark matter, the others being massive compact halo objects (MACHOs) and axions. These names were deliberately chosen for contrast, with MACHOs named later than WIMPs. In contrast to MACHOs, there are no known stable particles within the Standard Model of particle physics that have all the properties of WIMPs. The particles that have little interaction with normal matter, such as neutrinos, are all very light, and hence would be fast moving, or ""hot"".
-Baryonic matter Most of the ordinary matter familiar to astronomers, including planets, brown dwarfs, red dwarfs, visible stars, white dwarfs, neutron stars, and black holes, is called baryonic matter (referring to the baryons that dominate the mass of most ordinary matter). Solitary black holes, neutron stars, burnt-out dwarfs, and other massive objects that that are hard to detect are collectively known as MACHOs; some scientists initially hoped that baryonic MACHOs could account for and explain all the dark matter.However, multiple lines of evidence suggest the majority of dark matter is not baryonic: Sufficient diffuse, baryonic gas or dust would be visible when backlit by stars.
-Exotic dark matter In Lambda-CDM, dark matter is an extremely inert form of matter that does not interact with both ordinary matter (baryons) and light, but still exerts gravitational effects. To produce the large-scale structure we see today, dark matter is ""cold"" (the 'C' in Lambda-CDM), i.e. non-relativistic. Dark matter has not been conclusively identified, and its exact nature is the subject of intense study. The leading dark matter candidates are weakly interacting massive particles (WIMPs) and axions. Both of these are new elementary particles not included in the Standard Model of Particle Physics. A major difference between the two is their mass: WIMPs generally have masses in the GeV range, while axions are much lighter, with masses in the meV range or lower.
 A. They are unknown, but possibilities include large objects like MACHOs or new particles such as WIMPs and axions.
 B. They are known to be black holes and Preon stars.
 C. They are only MACHOs.
 D. They are clusters of brown dwarfs.
 E. They are new particles such as RAMBOs. "
What is the mechanism of FTIR?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the mechanism of FTIR?
 Context: -Frustrated TIR can be observed by looking into the top of a glass of water held in one's hand (Fig. 10). If the glass is held loosely, contact may not be sufficiently close and widespread to produce a noticeable effect. But if it is held more tightly, the ridges of one's fingerprints interact strongly with the evanescent waves, allowing the ridges to be seen through the otherwise totally reflecting glass-air surface.The same effect can be demonstrated with microwaves, using paraffin wax as the ""internal"" medium (where the incident and reflected waves exist). In this case the permitted gap width might be (e.g.) 1 cm or several cm, which is easily observable and adjustable.The term frustrated TIR also applies to the case in which the evanescent wave is scattered by an object sufficiently close to the reflecting interface. This effect, together with the strong dependence of the amount of scattered light on the distance from the interface, is exploited in total internal reflection microscopy.The mechanism of FTIR is called evanescent-wave coupling, and is a good analog to visualize quantum tunneling. Due to the wave nature of matter, an electron has a non-zero probability of ""tunneling"" through a barrier, even if classical mechanics would say that its energy is insufficient. Similarly, due to the wave nature of light, a photon has a non-zero probability of crossing a gap, even if ray optics would say that its approach is too oblique.
-Nanoscale and spectroscopy below the diffraction limit The spatial resolution of FTIR can be further improved below the micrometer scale by integrating it into scanning near-field optical microscopy platform. The corresponding technique is called nano-FTIR and allows for performing broadband spectroscopy on materials in ultra-small quantities (single viruses and protein complexes) and with 10 to 20 nm spatial resolution.
-Nano-FTIR detects the tip-scattered light interferometrically. The sample stage is placed into one arm of a conventional Michelson interferometer, while a mirror on a piezo stage is placed into another, reference arm. Recording the backscattered signal while translating the reference mirror yields an interferogram. The subsequent Fourier transform of this interferogram returns the near-field spectra of the sample.  Placement of the sample stage into one of the interferometer's arms (instead of outside of the interferometer as typically implemented in conventional FTIR) is a key element of nano-FTIR. It boosts the weak near-field signal due to interference with the strong reference field, helps to eliminate the background caused by parasitic scattering off everything that falls into large diffraction-limited beam focus, and most importantly, allows for recording of both amplitude s and phase φ spectra of the tip-scattered radiation. With the detection of phase, nano-FTIR provides complete information about near fields, which is essential for quantitative studies and many other applications. For example, for soft matter samples (organics, polymers, biomaterials, etc.), φ directly relates to the absorption in the sample material. This permits a direct comparison of nano-FTIR spectra with conventional absorption spectra of the sample material, thus allowing for simple spectroscopic identification according to standard FTIR databases.
-FTIR is a method of measuring infrared absorption and emission spectra. For a discussion of why people measure infrared absorption and emission spectra, i.e. why and how substances absorb and emit infrared light, see the article: Infrared spectroscopy.
-As a direct consequence of being quantitative technique (i.e. capable of highly reproducible detection of both near-field amplitude & phase and well understood near-field interaction models), nano-FTIR also provides means for the quantitative studies of the sample interior (within the probing range of the tip near field, of course). This is often achieved by a simple method of utilizing signals recorded at multiple demodulation orders naturally returned by nano-FTIR in the process of background suppression. It has been shown that higher harmonics probe smaller volumes below the tip, thus encoding the volumetric structure of a sample. This way, nano-FTIR has a demonstrated capability for the recovery of thickness and permittivity of layered films and nanostructures, which has been utilized for the nanoscale depth profiling of multiphase materials and high-Tc cuprate nanoconstriction devices patterned by focused ion beams. In other words, nano-FTIR has a unique capability of recovering the same information about thin-film samples that is typically returned by ellipsometry or impedance spectroscopy, yet with nanoscale spatial resolution. This capability proved crucial for disentangling different surface states in topological insulators.
 A. The mechanism of FTIR is called ray optics, which is a good analog to visualize quantum tunneling.
 B. The mechanism of FTIR is called scattering, which is a good analog to visualize quantum tunneling.
 C. The mechanism of FTIR is called frustrated TIR, which is a good analog to visualize quantum tunneling.
 D. The mechanism of FTIR is called evanescent-wave coupling, which is a good analog to visualize quantum tunneling.
 E. The mechanism of FTIR is called total internal reflection microscopy, which is a good analog to visualize quantum tunneling. "
What is the origin of the permanent moment in paramagnetism?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the origin of the permanent moment in paramagnetism?
 Context: -Constituent atoms or molecules of paramagnetic materials have permanent magnetic moments (dipoles), even in the absence of an applied field. The permanent moment generally is due to the spin of unpaired electrons in atomic or molecular electron orbitals (see Magnetic moment). In pure paramagnetism, the dipoles do not interact with one another and are randomly oriented in the absence of an external field due to thermal agitation, resulting in zero net magnetic moment. When a magnetic field is applied, the dipoles will tend to align with the applied field, resulting in a net magnetic moment in the direction of the applied field. In the classical description, this alignment can be understood to occur due to a torque being provided on the magnetic moments by an applied field, which tries to align the dipoles parallel to the applied field. However, the true origins of the alignment can only be understood via the quantum-mechanical properties of spin and angular momentum.
-Magnetic moments are permanent dipole moments within an atom that comprise electron angular momentum and spin by the relation μl = el/2me, where me is the mass of an electron, μl is the magnetic moment, and l is the angular momentum; this ratio is called the gyromagnetic ratio.
The electrons in an atom contribute magnetic moments from their own angular momentum and from their orbital momentum around the nucleus. Magnetic moments from the nucleus are insignificant in contrast to the magnetic moments from the electrons. Thermal contributions result in higher energy electrons disrupting the order and the destruction of the alignment between dipoles.
Ferromagnetic, paramagnetic, ferrimagnetic and antiferromagnetic materials have different intrinsic magnetic moment structures. At a material's specific Curie temperature (TC), these properties change. The transition from antiferromagnetic to paramagnetic (or vice versa) occurs at the Néel temperature (TN), which is analogous to Curie temperature.
Orientations of magnetic moments in materials 
-The preferred classical explanation of a magnetic moment has changed over time. Before the 1930s, textbooks explained the moment using hypothetical magnetic point charges. Since then, most have defined it in terms of Ampèrian currents. In magnetic materials, the cause of the magnetic moment are the spin and orbital angular momentum states of the electrons, and varies depending on whether atoms in one region are aligned with atoms in another.
-In the 19th century, it was thought that magnetic fields are due to currents in matter, and Ampère postulated that permanent magnets are caused by permanent atomic currents. The motion of classical charged particles could not explain permanent currents though, as shown by Larmor. In order to have ferromagnetism, the atoms must have permanent magnetic moments which are not due to the motion of classical charges.
-Paramagnetism In a paramagnetic material there are unpaired electrons; i.e., atomic or molecular orbitals with exactly one electron in them. While paired electrons are required by the Pauli exclusion principle to have their intrinsic ('spin') magnetic moments pointing in opposite directions, causing their magnetic fields to cancel out, an unpaired electron is free to align its magnetic moment in any direction. When an external magnetic field is applied, these magnetic moments will tend to align themselves in the same direction as the applied field, thus reinforcing it.
 A. The permanent moment is generally due to the spin of unpaired electrons in atomic or molecular electron orbitals.
 B. The permanent moment is due to the alignment of dipoles perpendicular to the applied field.
 C. The permanent moment is due to the torque provided on the magnetic moments by an applied field, which tries to align the dipoles perpendicular to the applied field.
 D. The permanent moment is due to the quantum-mechanical properties of spin and angular momentum.
 E. The permanent moment is due to the interaction of dipoles with one another and are randomly oriented in the absence of an external field due to thermal agitation. "
What is the reason that Newton's second law cannot be used to calculate the development of a physical system in quantum mechanics?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the reason that Newton's second law cannot be used to calculate the development of a physical system in quantum mechanics?
 Context: -Spin is an intrinsic form of angular momentum carried by elementary particles, and thus by composite particles such as hadrons, atomic nuclei, and atoms.: 183–184  Spin should not be understood as in the ""rotating internal mass"" sense: spin is a quantized wave property.The existence of electron spin angular momentum is inferred from experiments, such as the Stern–Gerlach experiment, in which silver atoms were observed to possess two possible discrete angular momenta despite having no orbital angular momentum. The existence of the electron spin can also be inferred theoretically from the spin–statistics theorem and from the Pauli exclusion principle—and vice versa, given the particular spin of the electron, one may derive the Pauli exclusion principle.
-Relation to Newton's second law of motion While angular momentum total conservation can be understood separately from Newton's laws of motion as stemming from Noether's theorem in systems symmetric under rotations, it can also be understood simply as an efficient method of calculation of results that can also be otherwise arrived at directly from Newton's second law, together with laws governing the forces of nature (such as Newton's third law, Maxwell's equations and Lorentz force). Indeed, given initial conditions of position and velocity for every point, and the forces at such a condition, one may use Newton's second law to calculate the second derivative of position, and solving for this gives full information on the development of the physical system with time. Note, however, that this is no longer true in quantum mechanics, due to the existence of particle spin, which is angular momentum that cannot be described by the cumulative effect of point-like motions in space.
-In a conservative field, the total mechanical energy (kinetic and potential) is conserved: (where 'ṙ' denotes the derivative of 'r' with respect to time, that is the velocity,'I' denotes moment of inertia of that body and 'ω' denotes angular velocity), and in a central force field, so is the angular momentum: because the torque exerted by the force is zero. As a consequence, the body moves on the plane perpendicular to the angular momentum vector and containing the origin, and obeys Kepler's second law. (If the angular momentum is zero, the body moves along the line joining it with the origin.) It can also be shown that an object that moves under the influence of any central force obeys Kepler's second law. However, the first and third laws depend on the inverse-square nature of Newton's law of universal gravitation and do not hold in general for other central forces.
-Newton's derivation begins with a particle moving under an arbitrary central force F1(r); the motion of this particle under this force is described by its radius r(t) from the center as a function of time, and also its angle θ1(t). In an infinitesimal time dt, the particle sweeps out an approximate right triangle whose area is dA1=12r2dθ1 Since the force acting on the particle is assumed to be a central force, the particle sweeps out equal angles in equal times, by Newton's Proposition 2. Expressed another way, the rate of sweeping out area is constant dA1dt=12r2dθ1dt=constant This constant areal velocity can be calculated as follows. At the apapsis and periapsis, the positions of closest and furthest distance from the attracting center, the velocity and radius vectors are perpendicular; therefore, the angular momentum L1 per mass m of the particle (written as h1) can be related to the rate of sweeping out areas h1=L1m=rv1=r2dθ1dt=2dA1dt Now consider a second particle whose orbit is identical in its radius, but whose angular variation is multiplied by a constant factor k θ2(t)=kθ1(t) The areal velocity of the second particle equals that of the first particle multiplied by the same factor k h2=2dA2dt=r2dθ2dt=kr2dθ1dt=2kdA1dt=kh1 Since k is a constant, the second particle also sweeps out equal areas in equal times. Therefore, by Proposition 2, the second particle is also acted upon by a central force F2(r). This is the conclusion of Proposition 43.
-General considerations A rotational analog of Newton's third law of motion might be written, ""In a closed system, no torque can be exerted on any matter without the exertion on some other matter of an equal and opposite torque about the same axis."" Hence, angular momentum can be exchanged between objects in a closed system, but total angular momentum before and after an exchange remains constant (is conserved).Seen another way, a rotational analogue of Newton's first law of motion might be written, ""A rigid body continues in a state of uniform rotation unless acted by an external influence."" Thus with no external influence to act upon it, the original angular momentum of the system remains constant.The conservation of angular momentum is used in analyzing central force motion. If the net force on some body is directed always toward some point, the center, then there is no torque on the body with respect to the center, as all of the force is directed along the radius vector, and none is perpendicular to the radius. Mathematically, torque  τ=r×F=0, because in this case  r and  F are parallel vectors. Therefore, the angular momentum of the body about the center is constant. This is the case with gravitational attraction in the orbits of planets and satellites, where the gravitational force is always directed toward the primary body and orbiting bodies conserve angular momentum by exchanging distance and velocity as they move about the primary. Central force motion is also used in the analysis of the Bohr model of the atom.
 A. The existence of particle spin, which is linear momentum that can be described by the cumulative effect of point-like motions in space.
 B. The existence of particle spin, which is angular momentum that is always equal to zero.
 C. The existence of particle spin, which is linear momentum that cannot be described by the cumulative effect of point-like motions in space.
 D. The existence of particle spin, which is angular momentum that cannot be described by the cumulative effect of point-like motions in space.
 E. The existence of particle spin, which is angular momentum that can be described by the cumulative effect of point-like motions in space. "
"What is the butterfly effect, as defined by Lorenz in his book ""The Essence of Chaos""?","Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the butterfly effect, as defined by Lorenz in his book ""The Essence of Chaos""?
 Context: -Butterfly effect The butterfly effect is the notion that small events can have large, widespread consequences. The term describes events observed in chaos theory where a very small change in initial conditions results in vastly different outcomes. The term was coined by mathematician Edward Lorenz years after the phenomenon was first described.The butterfly effect has found its way into popular imagination. For example, in Ray Bradbury's 1952 short story A Sound of Thunder, the killing of a single insect millions of years in the past drastically changes the world, and in the 2004 film The Butterfly Effect, the protagonist's small changes to his past results in extreme changes.
-In chaos theory, the butterfly effect is the sensitive dependence on initial conditions in which a small change in one state of a deterministic nonlinear system can result in large differences in a later state.
-The butterfly effect describes a phenomenon in chaos theory whereby a minor change in circumstances can cause a large change in outcome. The scientific concept is attributed to Edward Lorenz, a mathematician and meteorologist who used the metaphor to describe his research findings related to chaos theory and weather prediction, initially in a 1972 paper titled ""Predictability: Does the Flap of a Butterfly's Wings in Brazil Set Off a Tornado in Texas?"" The butterfly metaphor is attributed to the 1952 Ray Bradbury short story ""A Sound of Thunder"".The concept has been widely adopted by popular culture, and interpreted to mean that small events have a rippling effect that cause much larger events to occur, and has become a common reference.
-The branch of mathematics known as Chaos Theory focuses on the behavior of systems that are highly sensitive to initial conditions. It suggests that a small change in an initial condition can completely alter the progression of a system. This phenomenon is known as the butterfly effect, which claims that a butterfly flapping its wings in Brazil can cause a tornado in Texas. The nature of chaos theory suggests that the predictability of any system is limited because it is impossible to know all of the minutiae of a system at the present time. In principal, the deterministic systems that chaos theory attempts to analyze can be predicted, but uncertainty in a forecast increases exponentially with elapsed time.As documented in, three major kinds of butterfly effects within Lorenz studies include: the sensitive dependence on initial conditions, the ability of a tiny perturbation to create an organized circulation at large distances, and the hypothetical role of small-scale processes in contributing to finite predictability. The three kinds of butterfly effects are not exactly the same.
-The term is closely associated with the work of mathematician and meteorologist Edward Norton Lorenz. He noted that the butterfly effect is derived from the metaphorical example of the details of a tornado (the exact time of formation, the exact path taken) being influenced by minor perturbations such as a distant butterfly flapping its wings several weeks earlier. Lorenz originally used a seagull causing a storm but was persuaded to make it more poetic with the use of a butterfly and tornado by 1972. He discovered the effect when he observed runs of his weather model with initial condition data that were rounded in a seemingly inconsequential manner. He noted that the weather model would fail to reproduce the results of runs with the unrounded initial condition data. A very small change in initial conditions had created a significantly different outcome.The idea that small causes may have large effects in weather was earlier acknowledged by French mathematician and engineer Henri Poincaré. American mathematician and philosopher Norbert Wiener also contributed to this theory. Lorenz's work placed the concept of instability of the Earth's atmosphere onto a quantitative base and linked the concept of instability to the properties of large classes of dynamic systems which are undergoing nonlinear dynamics and deterministic chaos.The butterfly effect concept has since been used outside the context of weather science as a broad term for any situation where a small change is supposed to be the cause of larger consequences.
 A. The butterfly effect is the phenomenon that a small change in the initial conditions of a dynamical system can cause subsequent states to differ greatly from the states that would have followed without the alteration, as defined by Einstein in his book ""The Theory of Relativity.""
 B. The butterfly effect is the phenomenon that a large change in the initial conditions of a dynamical system has no effect on subsequent states, as defined by Lorenz in his book ""The Essence of Chaos.""
 C. The butterfly effect is the phenomenon that a small change in the initial conditions of a dynamical system can cause significant differences in subsequent states, as defined by Lorenz in his book ""The Essence of Chaos.""
 D. The butterfly effect is the phenomenon that a small change in the initial conditions of a dynamical system has no effect on subsequent states, as defined by Lorenz in his book ""The Essence of Chaos.""
 E. The butterfly effect is the phenomenon that a large change in the initial conditions of a dynamical system can cause significant differences in subsequent states, as defined by Lorenz in his book ""The Essence of Chaos."" "
What is the role of CYCLOIDEA genes in the evolution of bilateral symmetry?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the role of CYCLOIDEA genes in the evolution of bilateral symmetry?
 Context: -Evolution of symmetry in plants Early flowering plants had radially symmetric flowers but since then many plants have evolved bilaterally symmetrical flowers. The evolution of bilateral symmetry is due to the expression of CYCLOIDEA genes. Evidence for the role of the CYCLOIDEA gene family comes from mutations in these genes which cause a reversion to radial symmetry. The CYCLOIDEA genes encode transcription factors, proteins which control the expression of other genes. This allows their expression to influence developmental pathways relating to symmetry. For example, in Antirrhinum majus, CYCLOIDEA is expressed during early development in the dorsal domain of the flower meristem and continues to be expressed later on in the dorsal petals to control their size and shape. It is believed that the evolution of specialized pollinators may play a part in the transition of radially symmetrical flowers to bilaterally symmetrical flowers.
-Arabidopsis thaliana has a gene called AGAMOUS that plays an important role in defining how many petals and sepals and other organs are generated. Mutations in this gene give rise to the floral meristem obtaining an indeterminate fate, and proliferation of floral organs in double-flowered forms of roses, carnations and morning glory. These phenotypes have been selected by horticulturists for their increased number of petals. Several studies on diverse plants like petunia, tomato, Impatiens, maize, etc. have suggested that the enormous diversity of flowers is a result of small changes in genes controlling their development.The Floral Genome Project confirmed that the ABC Model of flower development is not conserved across all angiosperms. Sometimes expression domains change, as in the case of many monocots, and also in some basal angiosperms like Amborella. Different models of flower development like the Fading boundaries model, or the Overlapping-boundaries model which propose non-rigid domains of expression, may explain these architectures. There is a possibility that from the basal to the modern angiosperms, the domains of floral architecture have become more and more fixed through evolution.
-Arabidopsis thaliana has a gene called AGAMOUS that plays an important role in defining how many petals and sepals and other organs are generated. Mutations in this gene give rise to the floral meristem obtaining an indeterminate fate, and many floral organs keep on getting produced. We have flowers like roses, carnations and morning glory, for example, that have very dense floral organs. These flowers have been selected by horticulturists since long for increased number of petals. Researchers have found that the morphology of these flowers is because of strong mutations in the AGAMOUS homolog in these plants, which leads to them making a large number of petals and sepals. Several studies on diverse plants like petunia, tomato, impatiens, maize etc. have suggested that the enormous diversity of flowers is a result of small changes in genes controlling their development.Some of these changes also cause changes in expression patterns of the developmental genes, resulting in different phenotypes. The Floral Genome Project looked at the EST data from various tissues of many flowering plants. The researchers confirmed that the ABC Model of flower development is not conserved across all angiosperms. Sometimes expression domains change, as in the case of many monocots, and also in some basal angiosperms like Amborella. Different models of flower development like the fading boundaries model, or the overlapping-boundaries model which propose non-rigid domains of expression, may explain these architectures. There is a possibility that from the basal to the modern angiosperms, the domains of floral architecture have gotten more and more fixed through evolution.
-Function The genes identified in the tissues of A. thaliana were able to be separated out and categorized based on location and function for both male and female reproductive organs. The male reproductive organs of the plant are much easier to be utilized for experimental procedures due to their capability to be easily isolated from the plant compared to the female organs. The genes that have been linked to pollen and pollen production show strong co-expression due to their classification as duplicated genes across various tissues. It has been identified that genes associated with pollen and the pollen tubes have a relatively high number of expressed polymorphisms through purifying selection. Identifiable features of adaptive evolution expressed in pollen associated genes are comparable to the increased levels of adaptive evolution in other comparable species. With adaptation regulated by mutation rates, the sex-biased genes associated with the male organs of the plant could show higher adaptation rates due to their presence being in a haploid state. In this haploid state, mutations are directly exposed to the opportunity of rapid selection. Pollen interactions associated with sporophytic tissue are not expressed in genes linked to female reproductive organs. The mechanisms involved in pollen formation and development of the pollen tube are important for pollen selection as well as protein composition of the pollen. Pollen surface proteins are produced in the sporophytic tissue of the anther and have expressed higher levels of purified selection with an increase in adaptive evolution from the oleopollenins of the anther.
-Although asymmetry is typically associated with being unfit, some species have evolved to be asymmetrical as an important adaptation. Many members of the phylum Porifera (sponges) have no symmetry, though some are radially symmetric.
Symmetry breaking The presence of these asymmetrical features requires a process of symmetry breaking during development, both in plants and animals. Symmetry breaking occurs at several different levels in order to generate the anatomical asymmetry which we observe. These levels include asymmetric gene expression, protein expression, and activity of cells.
 A. CYCLOIDEA genes are responsible for the selection of symmetry in the evolution of animals.
 B. CYCLOIDEA genes are responsible for the evolution of specialized pollinators in plants, which in turn led to the transition of radially symmetrical flowers to bilaterally symmetrical flowers.
 C. CYCLOIDEA genes are responsible for the expression of dorsal petals in Antirrhinum majus, which control their size and shape.
 D. CYCLOIDEA genes are responsible for the expression of transcription factors that control the expression of other genes, allowing their expression to influence developmental pathways relating to symmetry.
 E. CYCLOIDEA genes are responsible for mutations that cause a reversion to radial symmetry. "
What is the required excess quark per billion quark-antiquark pairs in the early universe in order to provide all the observed matter in the universe?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the required excess quark per billion quark-antiquark pairs in the early universe in order to provide all the observed matter in the universe?
 Context: -The Standard Model can incorporate baryogenesis, though the amount of net baryons (and leptons) thus created may not be sufficient to account for the present baryon asymmetry. There is a required one excess quark per billion quark-antiquark pairs in the early universe in order to provide all the observed matter in the universe. This insufficiency has not yet been explained, theoretically or otherwise.
-The majority of ordinary matter in the universe is found in atomic nuclei, which are made of neutrons and protons. These nucleons are made up of smaller particles called quarks, and antimatter equivalents for each are predicted to exist by the Dirac equation in 1928. Since then, each kind of antiquark has been experimentally verified. Hypotheses investigating the first few instants of the universe predict a composition with an almost equal number of quarks and antiquarks. Once the universe expanded and cooled to a critical temperature of approximately 2×1012 K, quarks combined into normal matter and antimatter and proceeded to annihilate up to the small initial asymmetry of about one part in five billion, leaving the matter around us. Free and separate individual quarks and antiquarks have never been observed in experiments—quarks and antiquarks are always found in groups of three (baryons), or bound in quark–antiquark pairs (mesons). Likewise, there is no experimental evidence that there are any significant concentrations of antimatter in the observable universe.
-One of the outstanding problems in modern physics is the predominance of matter over antimatter in the universe. The universe, as a whole, seems to have a nonzero positive baryon number density – that is, there is more matter than antimatter. Since it is assumed in cosmology that the particles we see were created using the same physics we measure today, it would normally be expected that the overall baryon number should be zero, as matter and antimatter should have been created in equal amounts. This has led to a number of proposed mechanisms for symmetry breaking that favour the creation of normal matter (as opposed to antimatter) under certain conditions. This imbalance would have been exceptionally small, on the order of 1 in every 1010 particles a small fraction of a second after the Big Bang, but after most of the matter and antimatter annihilated, what was left over was all the baryonic matter in the current universe, along with a much greater number of bosons.
-Because of the extremely high energies involved, quark-antiquark pairs are produced by pair production and thus QGP is a roughly equal mixture of quarks and antiquarks of various flavors, with only a slight excess of quarks. This property is not a general feature of conventional plasmas, which may be too cool for pair production (see however pair instability supernova).
-According to the current models of big bang nucleosynthesis, the primordial composition of visible matter of the universe should be about 75% hydrogen and 25% helium-4 (in mass). Neutrons are made up of one up and two down quarks, while protons are made of two up and one down quark. Since the other common elementary particles (such as electrons, neutrinos, or weak bosons) are so light or so rare when compared to atomic nuclei, we can neglect their mass contribution to the observable universe's total mass. Therefore, one can conclude that most of the visible mass of the universe consists of protons and neutrons, which, like all baryons, in turn consist of up quarks and down quarks.
 A. One
 B. Five
 C. Three
 D. Two
 E. Four "
"What is the meaning of the term ""horror vacui""?","Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the meaning of the term ""horror vacui""?
 Context: -In visual art, horror vacui (Latin for 'fear of empty space'; UK: ; US: ), or kenophobia (Greek for 'fear of the empty'), is a phenomenon in which the entire surface of a space or an artwork is filled with detail and content, leaving as little perceived emptiness as possible. It relates to the antiquated physical idea, horror vacui, proposed by Aristotle who held that ""nature abhors an empty space"".
-In physics, horror vacui, or plenism (), commonly stated as ""nature abhors a vacuum"", is a postulate attributed to Aristotle, who articulated a belief, later criticized by the atomism of Epicurus and Lucretius, that nature contains no vacuums because the denser surrounding material continuum would immediately fill the rarity of an incipient void. He also argued against the void in a more abstract sense (as ""separable""), for example, that by definition a void (equivocally?) itself, is nothing, and following Plato, nothing cannot rightly be said to exist. Furthermore, insofar as it would be featureless, it could neither be encountered by the senses, nor could its supposition lend additional explanatory power. Hero of Alexandria challenged the theory in the first century AD, but his attempts to create an artificial vacuum failed. The theory was debated in the context of 17th-century fluid mechanics, by Thomas Hobbes and Robert Boyle, among others, and through the early 18th century by Sir Isaac Newton and Gottfried Leibniz.
-Medieval thought experiments into the idea of a vacuum considered whether a vacuum was present, if only for an instant, between two flat plates when they were rapidly separated. There was much discussion of whether the air moved in quickly enough as the plates were separated, or, as Walter Burley postulated, whether a 'celestial agent' prevented the vacuum arising. The commonly held view that nature abhorred a vacuum was called horror vacui. There was even speculation that even God could not create a vacuum if he wanted and the 1277 Paris condemnations of Bishop Étienne Tempier, which required there to be no restrictions on the powers of God, led to the conclusion that God could create a vacuum if he so wished. Jean Buridan reported in the 14th century that teams of ten horses could not pull open bellows when the port was sealed.
-A vacuum (PL: vacuums or vacua) is a space devoid of matter. The word is derived from the Latin adjective vacuus for ""vacant"" or ""void"". An approximation to such vacuum is a region with a gaseous pressure much less than atmospheric pressure. Physicists often discuss ideal test results that would occur in a perfect vacuum, which they sometimes simply call ""vacuum"" or free space, and use the term partial vacuum to refer to an actual imperfect vacuum as one might have in a laboratory or in space. In engineering and applied physics on the other hand, vacuum refers to any space in which the pressure is considerably lower than atmospheric pressure. The Latin term in vacuo is used to describe an object that is surrounded by a vacuum.
-In 350 BCE, Greek philosopher Aristotle suggested that nature abhors a vacuum, a principle that became known as the horror vacui. This concept built upon a 5th-century BCE ontological argument by the Greek philosopher Parmenides, who denied the possible existence of a void in space. Based on this idea that a vacuum could not exist, in the West it was widely held for many centuries that space could not be empty. As late as the 17th century, the French philosopher René Descartes argued that the entirety of space must be filled.In ancient China, the 2nd-century astronomer Zhang Heng became convinced that space must be infinite, extending well beyond the mechanism that supported the Sun and the stars. The surviving books of the Hsüan Yeh school said that the heavens were boundless, ""empty and void of substance"". Likewise, the ""sun, moon, and the company of stars float in the empty space, moving or standing still"".The Italian scientist Galileo Galilei knew that air had mass and so was subject to gravity. In 1640, he demonstrated that an established force resisted the formation of a vacuum. It would remain for his pupil Evangelista Torricelli to create an apparatus that would produce a partial vacuum in 1643. This experiment resulted in the first mercury barometer and created a scientific sensation in Europe. The French mathematician Blaise Pascal reasoned that if the column of mercury was supported by air, then the column ought to be shorter at higher altitude where the air pressure is lower. In 1648, his brother-in-law, Florin Périer, repeated the experiment on the Puy de Dôme mountain in central France and found that the column was shorter by three inches. This decrease in pressure was further demonstrated by carrying a half-full balloon up a mountain and watching it gradually expand, then contract upon descent.
 A. The quantified extension of volume in empty space.
 B. The commonly held view that nature abhorred a vacuum.
 C. The medieval thought experiment into the idea of a vacuum.
 D. The success of Descartes' namesake coordinate system.
 E. The spatial-corporeal component of Descartes' metaphysics. "
What is the Droste effect?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the Droste effect?
 Context: -The effect is named after a Dutch brand of cocoa, with an image designed by Jan Misset in 1904. It has since been used in the packaging of a variety of products. The effect is seen in the Dutch artist M. C. Escher's 1956 lithograph Print Gallery, which portrays a gallery that depicts itself. Apart from advertising, the Droste effect is displayed in the model village at Bourton-on-the-Water: this contains a model of itself, with two further iterations. The effect has been a motif, too, for the cover of many comic books, where it was especially popular in the 1940s.
-The Droste effect (Dutch pronunciation: [ˈdrɔstə]), known in art as an example of mise en abyme, is the effect of a picture recursively appearing within itself, in a place where a similar picture would realistically be expected to appear. This produces a loop which in theory could go on forever, but in practice only continues as far as the image's resolution allows.
-M. C. Escher The Dutch artist M. C. Escher made use of the Droste effect in his 1956 lithograph Print Gallery, which portrays a gallery containing a print which depicts the gallery, each time both reduced and rotated, but with a void at the centre of the image. The work has attracted the attention of mathematicians including Hendrik Lenstra. They devised a method of filling in the artwork's central void in an additional application of the Droste effect by successively rotating and shrinking an image of the artwork.
-Origins The Droste effect is named after the image on the tins and boxes of Droste cocoa powder which displayed a nurse carrying a serving tray with a cup of hot chocolate and a box with the same image, designed by Jan Misset. This familiar image was introduced in 1904 and maintained for decades with slight variations from 1912 by artists including Adolphe Mouron. The poet and columnist Nico Scheepmaker introduced wider usage of the term in the late 1970s.
-Advertising In the 20th century, the Droste effect was used to market a variety of products. The packaging of Land O'Lakes butter featured a Native American woman holding a package of butter with a picture of herself. Morton Salt similarly made use of the effect. The cover of the 1969 vinyl album Ummagumma by Pink Floyd shows the band members sitting in various places, with a picture on the wall showing the same scene, but the order of the band members rotated. The logo of The Laughing Cow cheese spread brand pictures a cow with earrings. On closer inspection, these are seen to be images of the circular cheese spread package, each bearing the image of the laughing cow.  The Droste effect is a theme in Russell Hoban's children's novel, The Mouse and His Child, appearing in the form of a label on a can of ""Bonzo Dog Food"" which depicts itself.
 A. The Droste effect is a type of optical illusion that creates the appearance of a three-dimensional image within a two-dimensional picture.
 B. The Droste effect is a type of packaging design used by a variety of products, named after a Dutch brand of cocoa, with an image designed by Jan Misset in 1904.
 C. The Droste effect is a type of painting technique used by Dutch artist M. C. Escher in his 1956 lithograph Print Gallery, which portrays a gallery that depicts itself.
 D. The Droste effect is a recursive image effect in which a picture appears within itself in a place where a similar picture would realistically be expected to appear. This creates a loop that can continue as far as the image's resolution allows, and is named after a Dutch brand of cocoa.
 E. The Droste effect is a type of recursive algorithm used in computer programming to create self-referential images. "
What is water hammer?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is water hammer?
 Context: -Water hammer: Water hammer (or more generally, fluid hammer) is a pressure surge or wave caused when a fluid (usually a liquid but sometimes also a gas) in motion is forced to stop or change direction suddenly (momentum change). Water hammer commonly occurs when a valve closes suddenly at an end of a pipeline system, and a pressure wave propagates in the pipe. It's also called hydraulic shock.
-Hydraulic hammer A hydraulic hammer is a modern type of piling hammer used instead of diesel and air hammers for driving steel pipe, precast concrete, and timber piles. Hydraulic hammers are more environmentally acceptable than older, less efficient hammers as they generate less noise and pollutants. In many cases the dominant noise is caused by the impact of the hammer on the pile, or the impacts between components of the hammer, so that the resulting noise level can be similar to diesel hammers.
-Hydraulic shock (colloquial: water hammer; fluid hammer) is a pressure surge or wave caused when a fluid in motion, usually a liquid but sometimes also a gas is forced to stop or change direction suddenly; a momentum change. This phenomenon commonly occurs when a valve closes suddenly at an end of a pipeline system, and a pressure wave propagates in the pipe.
-The water hammer principle can be used to create a simple water pump called a hydraulic ram.
Leaks can sometimes be detected using water hammer.
Enclosed air pockets can be detected in pipelines.
The water hammer from a liquid jet created by a collapsing microcavity is studied for potential applications noninvasive transdermal drug delivery.
-A water hammer was a Victorian toy in which a tube was half filled with fluid, the remainder being a vacuum. Each time the tube was inverted or shaken, the impact of the fluid at each end would sound like a hammer blow.This is associated with increased stroke volume of the left ventricle and decrease in the peripheral resistance leading to the widened pulse pressure of aortic regurgitation.
 A. Water hammer is a type of water turbine used in hydroelectric generating stations to generate electricity.
 B. Water hammer is a type of air trap or standpipe used to dampen the sound of moving water in plumbing systems.
 C. Water hammer is a type of plumbing tool used to break pipelines and absorb the potentially damaging forces caused by moving water.
 D. Water hammer is a type of water pump used to increase the pressure of water in pipelines.
 E. Water hammer is a loud banging noise resembling a hammering sound that occurs when moving water is suddenly stopped, causing a rise in pressure and resulting shock wave. "
What is the reason for the stochastic nature of all observed resistance-switching processes?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the reason for the stochastic nature of all observed resistance-switching processes?
 Context: -A ""resistance switching"" event can simply be enforced by setting the external bias to a value above a certain threshold value. This is the trivial case, i.e., the free-energy barrier for the transition {i} → {j} is reduced to zero. In case one applies biases below the threshold value, there is still a finite probability that the device will switch in course of time (triggered by a random thermal fluctuation), but – as one is dealing with probabilistic processes – it is impossible to predict when the switching event will occur. That is the basic reason for the stochastic nature of all observed resistance-switching (ReRAM) processes. If the free-energy barriers are not high enough, the memory device can even switch without having to do anything.
-The above-mentioned thermodynamic principle furthermore implies that the operation of two-terminal non-volatile memory devices (e.g. ""resistance-switching"" memory devices (ReRAM)) cannot be associated with the memristor concept, i.e., such devices cannot by itself remember their current or voltage history. Transitions between distinct internal memory or resistance states are of probabilistic nature. The probability for a transition from state {i} to state {j} depends on the height of the free-energy barrier between both states. The transition probability can thus be influenced by suitably driving the memory device, i.e., by ""lowering"" the free-energy barrier for the transition {i} → {j} by means of, for example, an externally applied bias.
-Within this context, Meuffels and Soni pointed to a fundamental thermodynamic principle: Non-volatile information storage requires the existence of free-energy barriers that separate the distinct internal memory states of a system from each other; otherwise, one would be faced with an ""indifferent"" situation, and the system would arbitrarily fluctuate from one memory state to another just under the influence of thermal fluctuations. When unprotected against thermal fluctuations, the internal memory states exhibit some diffusive dynamics, which causes state degradation. The free-energy barriers must therefore be high enough to ensure a low bit-error probability of bit operation. Consequently, there is always a lower limit of energy requirement – depending on the required bit-error probability – for intentionally changing a bit value in any memory device.In the general concept of memristive system the defining equations are (see Theory): y(t)=g(x,u,t)u(t),x˙=f(x,u,t), where u(t) is an input signal, and y(t) is an output signal. The vector x represents a set of n state variables describing the different internal memory states of the device. ẋ is the time-dependent rate of change of the state vector x with time.
-When one wants to go beyond mere curve fitting and aims at a real physical modeling of non-volatile memory elements, e.g., resistive random-access memory devices, one has to keep an eye on the aforementioned physical correlations. To check the adequacy of the proposed model and its resulting state equations, the input signal u(t) can be superposed with a stochastic term ξ(t), which takes into account the existence of inevitable thermal fluctuations. The dynamic state equation in its general form then finally reads: x˙=f(x,u(t)+ξ(t),t), where ξ(t) is, e.g., white Gaussian current or voltage noise. On base of an analytical or numerical analysis of the time-dependent response of the system towards noise, a decision on the physical validity of the modeling approach can be made, e.g., would the system be able to retain its memory states in power-off mode? Such an analysis was performed by Di Ventra and Pershin with regard to the genuine current-controlled memristor. As the proposed dynamic state equation provides no physical mechanism enabling such a memristor to cope with inevitable thermal fluctuations, a current-controlled memristor would erratically change its state in course of time just under the influence of current noise. Di Ventra and Pershin thus concluded that memristors whose resistance (memory) states depend solely on the current or voltage history would be unable to protect their memory states against unavoidable Johnson–Nyquist noise and permanently suffer from information loss, a so-called ""stochastic catastrophe"". A current-controlled memristor can thus not exist as a solid-state device in physical reality.
-When a two-terminal non-volatile memory device is found to be in a distinct resistance state {j}, there exists therefore no physical one-to-one relationship between its present state and its foregoing voltage history. The switching behavior of individual non-volatile memory devices thus cannot be described within the mathematical framework proposed for memristor/memristive systems.
 A. The free-energy barriers for the transition {i} → {j} are not high enough, and the memory device can switch without having to do anything.
 B. The device is subjected to random thermal fluctuations, which trigger the switching event, but it is impossible to predict when it will occur.
 C. The memory device is found to be in a distinct resistance state {j}, and there exists no physical one-to-one relationship between its present state and its foregoing voltage history.
 D. The device is subjected to biases below the threshold value, which still allows for a finite probability of switching, but it is possible to predict when it will occur.
 E. The external bias is set to a value above a certain threshold value, which reduces the free-energy barrier for the transition {i} → {j} to zero. "
What is the Einstein@Home project?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the Einstein@Home project?
 Context: -The Einstein@Home project is a distributed computing project similar to SETI@home intended to detect this type of simple gravitational wave. By taking data from LIGO and GEO, and sending it out in little pieces to thousands of volunteers for parallel analysis on their home computers, Einstein@Home can sift through the data far more quickly than would be possible otherwise.
-The Einstein@Home project is a distributed computing project similar to SETI@home intended to detect this type of gravitational wave. By taking data from LIGO and GEO, and sending it out in little pieces to thousands of volunteers for parallel analysis on their home computers, Einstein@Home can sift through the data far more quickly than would be possible otherwise.
-Einstein@Home is a volunteer computing project that searches for signals from spinning neutron stars in data from gravitational-wave detectors, from large radio telescopes, and from a gamma-ray telescope. Neutron stars are detected by their pulsed radio and gamma-ray emission as radio and/or gamma-ray pulsars. They also might be observable as continuous gravitational wave sources if they are rapidly spinning and non-axisymmetrically deformed. The project was officially launched on 19 February 2005 as part of the American Physical Society's contribution to the World Year of Physics 2005 event.Einstein@Home searches data from the LIGO gravitational-wave detectors. The project conducts the most sensitive all-sky searches for continuous gravitational waves. While no such signal has yet been detected, the upper limits set by Einstein@Home analyses provide astrophysical constraints on the Galactic population of spinning neutron stars.
-The Einstein@Home project was originally created to perform all-sky searches for previously unknown continuous gravitational-wave (CW) sources using data from the Laser Interferometer Gravitational-Wave Observatory (LIGO) detector instruments in Washington and Louisiana, USA. The best understood potential CW sources are rapidly spinning neutron stars (including pulsars) which are expected to emit gravitational waves due to a deviation from Rotational symmetry. Besides validating Einstein's theory of General Relativity, direct detection of gravitational waves would also constitute an important new astronomical tool. As most neutron stars are electromagnetically invisible, gravitational-wave observations might also reveal completely new populations of neutron stars. A CW detection could potentially be extremely helpful in neutron-star astrophysics and would eventually provide unique insights into the nature of matter at high densities, because it provides a way of examining the bulk motion of the matter.Since March 2009, part of the Einstein@Home computing power has also been used to analyze data taken by the PALFA Consortium at the Arecibo Observatory in Puerto Rico. This search effort is designed to find radio pulsars in tight binary systems. It is expected that there is one radio pulsar detectable from Earth in an orbital system with a period of less than one hour. A similar search has also been performed on two archival data sets from the Parkes Multi-beam Pulsar Survey. The Einstein@Home radio pulsar search employs mathematical methods developed for the search for gravitational waves.Since July 2011, Einstein@Home is also analyzing data from the Large Area Telescope (LAT), the primary instrument on Fermi Gamma-ray Space Telescope to search for pulsed gamma-ray emission from spinning neutron stars (gamma-ray pulsars). Some neutron stars are only detectable by their pulsed gamma-ray emission, which originates in a different area of the neutron star magnetosphere than the radio emission. Identifying the neutron star's rotation rate is computationally difficult, because for a typical gamma-ray pulsar only thousands of gamma-ray photons will be detected by the LAT over the course of billions of rotations. The Einstein@Home analysis of the LAT data makes use of methods initially developed for the detection of continuous gravitational waves.
-Einstein@Home uses the power of volunteer computing in solving the computationally intensive problem of analyzing a large volume of data. Such an approach was pioneered by the SETI@home project, which is designed to look for signs of extraterrestrial life by analyzing radio wave data. Einstein@Home runs through the same software platform as SETI@home, the Berkeley Open Infrastructure for Network Computing (BOINC). As of July 2022, more than 487,000 volunteers in 226 countries had participated in the project, making it the third-most-popular active BOINC application. Users regularly contribute about 12.7 petaFLOPS of computational power, which would rank Einstein@Home among the top 45 on the TOP500 list of supercomputers.
 A. The Einstein@Home project is a project that aims to detect signals from supernovae or binary black holes. It takes data from LIGO and GEO and sends it out in little pieces to thousands of volunteers for parallel analysis on their home computers.
 B. The Einstein@Home project is a project that aims to detect signals from supernovae or binary black holes. It takes data from SETI and GEO and sends it out in little pieces to thousands of volunteers for parallel analysis on their home computers.
 C. The Einstein@Home project is a distributed computing project that aims to detect simple gravitational waves with constant frequency. It takes data from LIGO and GEO and sends it out in little pieces to thousands of volunteers for parallel analysis on their home computers.
 D. The Einstein@Home project is a project that aims to detect simple gravitational waves with constant frequency. It takes data from LIGO and GEO and sends it out in large pieces to thousands of volunteers for parallel analysis on their home computers.
 E. The Einstein@Home project is a project that aims to detect simple gravitational waves with constant frequency. It takes data from SETI and GEO and sends it out in little pieces to thousands of volunteers for parallel analysis on their home computers. "
What happens to an initially inhomogeneous physical system that is isolated by a thermodynamic operation?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What happens to an initially inhomogeneous physical system that is isolated by a thermodynamic operation?
 Context: -An isolated physical system may be inhomogeneous, or may be composed of several subsystems separated from each other by walls. If an initially inhomogeneous physical system, without internal walls, is isolated by a thermodynamic operation, it will in general over time change its internal state. Or if it is composed of several subsystems separated from each other by walls, it may change its state after a thermodynamic operation that changes its walls. Such changes may include change of temperature or spatial distribution of temperature, by changing the state of constituent materials. A rod of iron, initially prepared to be hot at one end and cold at the other, when isolated, will change so that its temperature becomes uniform all along its length; during the process, the rod is not in thermal equilibrium until its temperature is uniform. In a system prepared as a block of ice floating in a bath of hot water, and then isolated, the ice can melt; during the melting, the system is not in thermal equilibrium; but eventually, its temperature will become uniform; the block of ice will not re-form. A system prepared as a mixture of petrol vapour and air can be ignited by a spark and produce carbon dioxide and water; if this happens in an isolated system, it will increase the temperature of the system, and during the increase, the system is not in thermal equilibrium; but eventually, the system will settle to a uniform temperature.
-Two initial thermodynamic systems, each isolated in their separate states of internal thermodynamic equilibrium, can, by a thermodynamic operation, be coalesced into a single new final isolated thermodynamic system. If the initial systems differ in chemical constitution, then the eventual thermodynamic equilibrium of the final system can be the result of chemical reaction. Alternatively, an isolated thermodynamic system, in the absence of some catalyst, can be in a metastable equilibrium; introduction of a catalyst, or some other thermodynamic operation, such as release of a spark, can trigger a chemical reaction. The chemical reaction will, in general, transform some chemical potential energy into thermal energy. If the joint system is kept isolated, then its internal energy remains unchanged. Such thermal energy manifests itself, however, in changes in the non-chemical state variables (such as temperature, pressure, volume) of the joint systems, as well as the changes in the mole numbers of the chemical constituents that describe the chemical reaction.
-A collection of matter may be entirely isolated from its surroundings. If it has been left undisturbed for an indefinitely long time, classical thermodynamics postulates that it is in a state in which no changes occur within it, and there are no flows within it. This is a thermodynamic state of internal equilibrium. (This postulate is sometimes, but not often, called the ""minus first"" law of thermodynamics. One textbook calls it the ""zeroth law"", remarking that the authors think this more befitting that title than its more customary definition, which apparently was suggested by Fowler.) Such states are a principal concern in what is known as classical or equilibrium thermodynamics, for they are the only states of the system that are regarded as well defined in that subject. A system in contact equilibrium with another system can by a thermodynamic operation be isolated, and upon the event of isolation, no change occurs in it. A system in a relation of contact equilibrium with another system may thus also be regarded as being in its own state of internal thermodynamic equilibrium.
-Homogeneity in the absence of external forces A thermodynamic system consisting of a single phase in the absence of external forces, in its own internal thermodynamic equilibrium, is homogeneous. This means that the material in any small volume element of the system can be interchanged with the material of any other geometrically congruent volume element of the system, and the effect is to leave the system thermodynamically unchanged. In general, a strong external force field makes a system of a single phase in its own internal thermodynamic equilibrium inhomogeneous with respect to some intensive variables. For example, a relatively dense component of a mixture can be concentrated by centrifugation.
-An isolated system obeys the conservation law that its total energy–mass stays constant. Most often, in thermodynamics, mass and energy are treated as separately conserved.
 A. It will change its internal state only if it is composed of a single subsystem and has internal walls.
 B. It will change its internal state only if it is composed of several subsystems separated from each other by walls.
 C. It will remain in its initial state indefinitely.
 D. It will generally change its internal state over time.
 E. It will change its internal state only if it is composed of a single subsystem. "
"What is the concept of simultaneity in Einstein's book, Relativity?","Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the concept of simultaneity in Einstein's book, Relativity?
 Context: -Absolute simultaneity refers to the concurrence of events in time at different locations in space in a manner agreed upon in all frames of reference. The theory of relativity does not have a concept of absolute time because there is a relativity of simultaneity. An event that is simultaneous with another event in one frame of reference may be in the past or future of that event in a different frame of reference,: 59  which negates absolute simultaneity.
-In physics, the relativity of simultaneity is the concept that distant simultaneity – whether two spatially separated events occur at the same time – is not absolute, but depends on the observer's reference frame. This possibility was raised by mathematician Henri Poincaré in 1900, and thereafter became a central idea in the special theory of relativity.
-Einstein (The Meaning of Relativity): ""Two events taking place at the points A and B of a system K are simultaneous if they appear at the same instant when observed from the middle point, M, of the interval AB. Time is then defined as the ensemble of the indications of similar clocks, at rest relative to K, which register the same simultaneously."" Einstein wrote in his book, Relativity, that simultaneity is also relative, i.e., two events that appear simultaneous to an observer in a particular inertial reference frame need not be judged as simultaneous by a second observer in a different inertial frame of reference.
-The simultaneity of both systems, whatever meaning is attributed to it, could only be observed by comparing two distant measurements, within the constraints of the speed of light. The simultaneity's influence cannot be causal, nor can it transmit information (which amounts to the same thing). This property is therefore compatible with the theory of relativity, according to which no information can travel faster than the speed of light.
-A fuller explanation of the concept of coordinate time arises from its relations with proper time and with clock synchronization. Synchronization, along with the related concept of simultaneity, has to receive careful definition in the framework of general relativity theory, because many of the assumptions inherent in classical mechanics and classical accounts of space and time had to be removed. Specific clock synchronization procedures were defined by Einstein and give rise to a limited concept of simultaneity.Two events are called simultaneous in a chosen reference frame if and only if the chosen coordinate time has the same value for both of them; and this condition allows for the physical possibility and likelihood that they will not be simultaneous from the standpoint of another reference frame.But outside special relativity, the coordinate time is not a time that could be measured by a clock located at the place that nominally defines the reference frame, e.g. a clock located at the solar system barycenter would not measure the coordinate time of the barycentric reference frame, and a clock located at the geocenter would not measure the coordinate time of a geocentric reference frame.
 A. Simultaneity is relative, meaning that two events that appear simultaneous to an observer in a particular inertial reference frame need not be judged as simultaneous by a second observer in a different inertial frame of reference.
 B. Simultaneity is relative, meaning that two events that appear simultaneous to an observer in a particular inertial reference frame will always be judged as simultaneous by a second observer in a different inertial frame of reference.
 C. Simultaneity is absolute, meaning that two events that appear simultaneous to an observer in a particular inertial reference frame will always be judged as simultaneous by a second observer in a different inertial frame of reference.
 D. Simultaneity is a concept that applies only to Newtonian theories and not to relativistic theories.
 E. Simultaneity is a concept that applies only to relativistic theories and not to Newtonian theories. "
What is the Josephson effect?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the Josephson effect?
 Context: -In physics, the Josephson effect is a phenomenon that occurs when two superconductors are placed in proximity, with some barrier or restriction between them. It is an example of a macroscopic quantum phenomenon, where the effects of quantum mechanics are observable at ordinary, rather than atomic, scale. The Josephson effect has many practical applications because it exhibits a precise relationship between different physical measures, such as voltage and frequency, facilitating highly accurate measurements.
-In 1962, Brian Josephson made the important theoretical prediction that a supercurrent can flow between two pieces of superconductor separated by a thin layer of insulator. This phenomenon, now called the Josephson effect, is exploited by superconducting devices such as SQUIDs. It is used in the most accurate available measurements of the magnetic flux quantum h/2e, and thus (coupled with the quantum Hall resistivity) for Planck's constant h. Josephson was awarded the Nobel Prize in Physics for this work in 1973.
-In 1962, Josephson made the important theoretical prediction that a supercurrent can flow between two pieces of superconductor separated by a thin layer of insulator. This phenomenon, now called the Josephson effect, is exploited by superconducting devices such as SQUIDs. It is used in the most accurate available measurements of the magnetic flux quantum Φ0 = h/(2e), where h is the Planck constant. Coupled with the quantum Hall resistivity, this leads to a precise measurement of the Planck constant. Josephson was awarded the Nobel Prize for this work in 1973.In 2008, it was proposed that the same mechanism that produces superconductivity could produce a superinsulator state in some materials, with almost infinite electrical resistance. The first development and study of superconducting Bose–Einstein condensate (BEC) in 2020 suggests that there is a ""smooth transition between"" BEC and Bardeen-Cooper-Shrieffer regimes.
-In 1962, the first commercial superconducting wire, a niobium-titanium alloy, was developed by researchers at Westinghouse, allowing the construction of the first practical superconducting magnets. In the same year, Josephson made the important theoretical prediction that a supercurrent can flow between two pieces of superconductor separated by a thin layer of insulator. This phenomenon, now called the Josephson effect, is exploited by superconducting devices such as SQUIDs. It is used in the most accurate available measurements of the magnetic flux quantum  Φ0=h2e , and thus (coupled with the quantum Hall resistivity) for the Planck constant h. Josephson was awarded the Nobel Prize for this work in 1973.
-SQUIDs, or superconducting quantum interference devices, are very sensitive magnetometers that operate via the Josephson effect. They are widely used in science and engineering.
In precision metrology, the Josephson effect provides an exactly reproducible conversion between frequency and voltage. Since the frequency is already defined precisely and practically by the caesium standard, the Josephson effect is used, for most practical purposes, to give the standard representation of a volt, the Josephson voltage standard.
 A. The Josephson effect is a phenomenon exploited by superconducting devices such as SQUIDs. It is used in the most accurate available measurements of the magnetic flux quantum Φ0 = h/(2e), where h is the Planck constant.
 B. The Josephson effect is a phenomenon exploited by magnetic devices such as SQUIDs. It is used in the most accurate available measurements of the magnetic flux quantum Φ0 = h/(2e), where h is the magnetic constant.
 C. The Josephson effect is a phenomenon exploited by superconducting devices such as SQUIDs. It is used in the most accurate available measurements of the electric flux quantum Φ0 = h/(2e), where h is the Planck constant.
 D. The Josephson effect is a phenomenon exploited by superconducting devices such as SQUIDs. It is used in the most accurate available measurements of the magnetic flux quantum Φ0 = e/(2h), where h is the Planck constant.
 E. The Josephson effect is a phenomenon exploited by magnetic devices such as SQUIDs. It is used in the most accurate available measurements of the electric flux quantum Φ0 = h/(2e), where h is the magnetic constant. "
What is the SI unit of the physical quantity m/Q?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the SI unit of the physical quantity m/Q?
 Context: -Units The SI unit for pressure is the pascal (Pa), equal to one newton per square metre (N/m2, or kg·m−1·s−2). This name for the unit was added in 1971; before that, pressure in SI was expressed simply in newtons per square metre.
-Pascal (unit) – the SI unit of pressure, which uses the symbol Pa and is defined as one newton per square metre. It is also used to quantify internal pressure, stress, Young's modulus, and ultimate tensile strength.
Physics – Pinion – Piston – Pitch drop experiment – Plain bearing – Plasma processing – Plasticity – Pneumatics – Poisson's ratio – Position vector – Potential difference – Power – the amount of energy transferred or converted per unit time. Power is a scalar quantity.
-The metre per second is the unit of both speed (a scalar quantity) and velocity (a vector quantity, which has direction and magnitude) in the International System of Units (SI), equal to the speed of a body covering a distance of one metre in a time of one second.
The SI unit symbols are m/s, m·s−1, m s−1, or m/s. Sometimes it is abbreviated as ""mps"".
-The pascal can be expressed using SI derived units, or alternatively solely SI base units, as: 1Pa=1Nm2=1kgm⋅s2=1Jm3 where N is the newton, m is the metre, kg is the kilogram, s is the second, and J is the joule.One pascal is the pressure exerted by a force of magnitude one newton perpendicularly upon an area of one square metre.
-This use of the unit of pressure provides an intuitive understanding for how a body's mass can apply force to a scale's surface area i.e.kilogram-force per square (centi-)metre.  In SI units, the unit is converted to the SI derived unit pascal (Pa), which is defined as one newton per square metre (N/m2). A newton is equal to 1 kg⋅m/s2, and a kilogram-force is 9.80665 N, meaning that 1 kgf/cm2 equals 98.0665 kilopascals (kPa).
 A. Meter per second
 B. Pascal per second
 C. Kilogram per coulomb
 D. Newton per meter
 E. Joule per second "
How many crystallographic point groups are there in three-dimensional space?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: How many crystallographic point groups are there in three-dimensional space?
 Context: -Up to conjugacy the set of three-dimensional point groups consists of 7 infinite series, and 7 other individual groups. In crystallography, only those point groups are considered which preserve some crystal lattice (so their rotations may only have order 1, 2, 3, 4, or 6). This crystallographic restriction of the infinite families of general point groups results in 32 crystallographic point groups (27 individual groups from the 7 series, and 5 of the 7 other individuals).
-In the classification of crystals, each point group defines a so-called (geometric) crystal class. There are infinitely many three-dimensional point groups. However, the crystallographic restriction on the general point groups results in there being only 32 crystallographic point groups. These 32 point groups are one-and-the-same as the 32 types of morphological (external) crystalline symmetries derived in 1830 by Johann Friedrich Christian Hessel from a consideration of observed crystal forms.  The point group of a crystal determines, among other things, the directional variation of physical properties that arise from its structure, including optical properties such as birefringency, or electro-optical features such as the Pockels effect. For a periodic crystal (as opposed to a quasicrystal), the group must maintain the three-dimensional translational symmetry that defines crystallinity.
-The trigonal crystal system consists of the 5 point groups that have a single three-fold rotation axis, which includes space groups 143 to 167. These 5 point groups have 7 corresponding space groups (denoted by R) assigned to the rhombohedral lattice system and 18 corresponding space groups (denoted by P) assigned to the hexagonal lattice system. Hence, the trigonal crystal system is the only crystal system whose point groups have more than one lattice system associated with their space groups.
-The geometric symmetries of crystals are described by space groups, which allow translations and contain point groups as subgroups. Discrete point groups in more than one dimension come in infinite families, but from the crystallographic restriction theorem and one of Bieberbach's theorems, each number of dimensions has only a finite number of point groups that are symmetric over some lattice or grid with that number of dimensions. These are the crystallographic point groups.
-§ The seven remaining point groups, which have multiple 3-or-more-fold rotation axes; these groups can also be characterized as point groups having multiple 3-fold rotation axes. The possible combinations are: Four 3-fold axes (the three tetrahedral symmetries T, Th, and Td) Four 3-fold axes and three 4-fold axes (octahedral symmetries O and Oh) Ten 3-fold axes and six 5-fold axes (icosahedral symmetries I and Ih)According to the crystallographic restriction theorem, only a limited number of point groups are compatible with discrete translational symmetry: 27 from the 7 infinite series, and 5 of the 7 others. Together, these make up the 32 so-called crystallographic point groups.
 A. 7
 B. 32
 C. 14
 D. 5
 E. 27 "
What is the Liouville density?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the Liouville density?
 Context: -A classical particle has a definite position and momentum, and hence it is represented by a point in phase space. Given a collection (ensemble) of particles, the probability of finding a particle at a certain position in phase space is specified by a probability distribution, the Liouville density. This strict interpretation fails for a quantum particle, due to the uncertainty principle. Instead, the above quasiprobability Wigner distribution plays an analogous role, but does not satisfy all the properties of a conventional probability distribution; and, conversely, satisfies boundedness properties unavailable to classical distributions.
-In physics, Liouville's theorem, named after the French mathematician Joseph Liouville, is a key theorem in classical statistical and Hamiltonian mechanics. It asserts that the phase-space distribution function is constant along the trajectories of the system—that is that the density of system points in the vicinity of a given system point traveling through phase-space is constant with time. This time-independent density is in statistical mechanics known as the classical a priori probability.There are related mathematical results in symplectic topology and ergodic theory; systems obeying Liouville's theorem are examples of incompressible dynamical systems.
-The evolution of an N-particle system in absence of quantum fluctuations is given by the Liouville equation for the probability density function  fN=fN(q1…qN,p1…pN,t) in 6N-dimensional phase space (3 space and 3 momentum coordinates per particle) ∂fN∂t+∑i=1Npim∂fN∂qi+∑i=1NFi∂fN∂pi=0, where  qi,pi are the coordinates and momentum for  i -th particle with mass  m , and the net force acting on the  i -th particle is ext ∂qi, where  Φij(qi,qj) is the pair potential for interaction between particles, and  ext (qi) is the external-field potential. By integration over part of the variables, the Liouville equation can be transformed into a chain of equations where the first equation connects the evolution of one-particle probability density function with the two-particle probability density function, second equation connects the two-particle probability density function with the three-particle probability density function, and generally the s-th equation connects the s-particle probability density function fs(q1…qs,p1…ps,t)=∫fN(q1…qN,p1…pN,t)dqs+1…dqNdps+1…dpN with the (s + 1)-particle probability density function: ∂fs∂t+∑i=1spim∂fs∂qi−∑i=1s(∑j=1≠is∂Φij∂qi+∂Φiext∂qi)∂fs∂pi=(N−s)∑i=1s∫∂Φis+1∂qi∂fs+1∂pidqs+1dps+1.
-However, the meaning of Liouville’s theorem in mechanics is rather different from the theorem of conservation of étendue. Liouville’s theorem is essentially statistical in nature, and it refers to the evolution in time of an ensemble of mechanical systems of identical properties but with different initial conditions. Each system is represented by a single point in phase space, and the theorem states that the average density of points in phase space is constant in time. An example would be the molecules of a perfect classical gas in equilibrium in a container. Each point in phase space, which in this example has 2N dimensions, where N is the number of molecules, represents one of an ensemble of identical containers, an ensemble large enough to permit taking a statistical average of the density of representative points. Liouville’s theorem states that if all the containers remain in equilibrium, the average density of points remains constant.
-The starting point is the quantum mechanical version of the von Neumann equation, also known as the Liouville equation: ∂tρ=iℏ[ρ,H]=Lρ, where the Liouville operator  L is defined as  LA=iℏ[A,H] The density operator (density matrix)  ρ is split by means of a projection operator P into two parts  ρ=(P+Q)ρ ,  where  Q≡1−P . The projection operator  P selects the aforementioned relevant part from the density operator, for which an equation of motion is to be derived.
 A. The Liouville density is a probability distribution that specifies the probability of finding a particle at a certain position in phase space for a collection of particles.
 B. The Liouville density is a quasiprobability distribution that plays an analogous role to the probability distribution for a quantum particle.
 C. The Liouville density is a bounded probability distribution that is a convenient indicator of quantum-mechanical interference.
 D. The Liouville density is a probability distribution that takes on negative values for states which have no classical model.
 E. The Liouville density is a probability distribution that satisfies all the properties of a conventional probability distribution for a quantum particle. "
What are the four qualitative levels of crystallinity described by geologists?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What are the four qualitative levels of crystallinity described by geologists?
 Context: -Geologists describe four qualitative levels of crystallinity: holocrystalline rocks are completely crystalline; hypocrystalline rocks are partially crystalline, with crystals embedded in an amorphous or glassy matrix; hypohyaline rocks are partially glassy; holohyaline rocks (such as obsidian) are completely glassy.
-Examples include: Serpentine subgroup Antigorite – Mg3Si2O5(OH)4 Chrysotile – Mg3Si2O5(OH)4 Lizardite – Mg3Si2O5(OH)4 Clay minerals group 1:1 clay minerals (TO) Halloysite – Al2Si2O5(OH)4 Kaolinite – Al2Si2O5(OH)4 2:1 clay minerals (TOT) Pyrophyllite – Al2Si4O10(OH)2 Talc – Mg3Si4O10(OH)2 Illite – (K,H3O)(Al,Mg,Fe)2(Si,Al)4O10[(OH)2,(H2O)] Montmorillonite (smectite) – (Na,Ca)0.33(Al,Mg)2Si4O10(OH)2·nH2O Chlorite – (Mg,Fe)3(Si,Al)4O10(OH)2·(Mg,Fe)3(OH)6 Vermiculite – (Mg,Fe,Al)3(Al,Si)4O10(OH)2·4H2O Other clay minerals Sepiolite – Mg4Si6O15(OH)2·6H2O Palygorskite (or attapulgite) – (Mg,Al)2Si4O10(OH)·4(H2O) Mica group Biotite – K(Mg,Fe)3(AlSi3)O10(OH)2 Fuchsite – K(Al,Cr)2(AlSi3)O10(OH)2 Muscovite – KAl2(AlSi3)O10(OH)2 Phlogopite – KMg3(AlSi3)O10(OH)2 Lepidolite – K(Li,Al)2–3(AlSi3)O10(OH)2 Margarite – CaAl2(Al2Si2)O10(OH)2 Glauconite – (K,Na)(Al,Mg,Fe)2(Si,Al)4O10(OH)2 
-The crystal structure is the arrangement of atoms in a crystal. It is represented by a lattice of points which repeats a basic pattern, called a unit cell, in three dimensions. The lattice can be characterized by its symmetries and by the dimensions of the unit cell. These dimensions are represented by three Miller indices.: 91–92  The lattice remains unchanged by certain symmetry operations about any given point in the lattice: reflection, rotation, inversion, and rotary inversion, a combination of rotation and reflection. Together, they make up a mathematical object called a crystallographic point group or crystal class. There are 32 possible crystal classes. In addition, there are operations that displace all the points: translation, screw axis, and glide plane. In combination with the point symmetries, they form 230 possible space groups.: 125–126 Most geology departments have X-ray powder diffraction equipment to analyze the crystal structures of minerals.: 54–55  X-rays have wavelengths that are the same order of magnitude as the distances between atoms. Diffraction, the constructive and destructive interference between waves scattered at different atoms, leads to distinctive patterns of high and low intensity that depend on the geometry of the crystal. In a sample that is ground to a powder, the X-rays sample a random distribution of all crystal orientations. Powder diffraction can distinguish between minerals that may appear the same in a hand sample, for example quartz and its polymorphs tridymite and cristobalite.: 54 Isomorphous minerals of different compositions have similar powder diffraction patterns, the main difference being in spacing and intensity of lines. For example, the NaCl (halite) crystal structure is space group Fm3m; this structure is shared by sylvite (KCl), periclase (MgO), bunsenite (NiO), galena (PbS), alabandite (MnS), chlorargyrite (AgCl), and osbornite (TiN).: 150–151 
-Crystal structure and habit Crystal structure results from the orderly geometric spatial arrangement of atoms in the internal structure of a mineral. This crystal structure is based on regular internal atomic or ionic arrangement that is often expressed in the geometric form that the crystal takes. Even when the mineral grains are too small to see or are irregularly shaped, the underlying crystal structure is always periodic and can be determined by X-ray diffraction. Minerals are typically described by their symmetry content. Crystals are restricted to 32 point groups, which differ by their symmetry. These groups are classified in turn into more broad categories, the most encompassing of these being the six crystal families.These families can be described by the relative lengths of the three crystallographic axes, and the angles between them; these relationships correspond to the symmetry operations that define the narrower point groups. They are summarized below; a, b, and c represent the axes, and α, β, γ represent the angle opposite the respective crystallographic axis (e.g. α is the angle opposite the a-axis, viz. the angle between the b and c axes): The hexagonal crystal family is also split into two crystal systems – the trigonal, which has a three-fold axis of symmetry, and the hexagonal, which has a six-fold axis of symmetry.
-When an element has allotropes with different densities, one representative allotrope is typically selected in summary presentations, while densities for each allotrope can be stated where more detail is provided. For example, the three familiar allotropes of carbon (amorphous carbon, graphite, and diamond) have densities of 1.8–2.1, 2.267, and 3.515 g/cm3, respectively.
Crystal structures The elements studied to date as solid samples have eight kinds of crystal structures: cubic, body-centered cubic, face-centered cubic, hexagonal, monoclinic, orthorhombic, rhombohedral, and tetragonal. For some of the synthetically produced transuranic elements, available samples have been too small to determine crystal structures.
Occurrence and origin on Earth Chemical elements may also be categorized by their origin on Earth, with the first 94 considered naturally occurring, while those with atomic numbers beyond 94 have only been produced artificially as the synthetic products of human-made nuclear reactions.
 A. Holocrystalline, hypocrystalline, hypercrystalline, and holohyaline
 B. Holocrystalline, hypocrystalline, hypohyaline, and holohyaline
 C. Holocrystalline, hypohyaline, hypercrystalline, and holohyaline
 D. Holocrystalline, hypocrystalline, hypercrystalline, and hyperhyaline
 E. Holocrystalline, hypocrystalline, hypohyaline, and hyperhyaline "
What is an order parameter?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is an order parameter?
 Context: -Order parameters An order parameter is a measure of the degree of order across the boundaries in a phase transition system; it normally ranges between zero in one phase (usually above the critical point) and nonzero in the other. At the critical point, the order parameter susceptibility will usually diverge.
An example of an order parameter is the net magnetization in a ferromagnetic system undergoing a phase transition. For liquid/gas transitions, the order parameter is the difference of the densities.
-From a theoretical perspective, order parameters arise from symmetry breaking. When this happens, one needs to introduce one or more extra variables to describe the state of the system. For example, in the ferromagnetic phase, one must provide the net magnetization, whose direction was spontaneously chosen when the system cooled below the Curie point. However, note that order parameters can also be defined for non-symmetry-breaking transitions.
-A parameter (from Ancient Greek παρά (pará) 'beside, subsidiary', and μέτρον (métron) 'measure'), generally, is any characteristic that can help in defining or classifying a particular system (meaning an event, project, object, situation, etc.). That is, a parameter is an element of a system that is useful, or critical, when identifying the system, or when evaluating its performance, status, condition, etc.
-Pair distribution function – This parameter is usually used in physics to characterize the degree of spatial order in a system of particles. It also describes the density, but this measure describes the density at a distance away from a given point. Cavagna et al. found that flocks of starlings exhibited more structure than a gas but less than a liquid.
-Essential in synergetics is the order-parameter concept which was originally introduced in the Ginzburg–Landau theory in order to describe phase transitions in thermodynamics. The order parameter concept is generalized by Haken to the ""enslaving-principle"" saying that the dynamics of fast-relaxing (stable) modes is completely determined by the 'slow' dynamics of, as a rule, only a few 'order-parameters' (unstable modes). The order parameters can be interpreted as the amplitudes of the unstable modes determining the macroscopic pattern.
 A. An order parameter is a measure of the temperature of a physical system.
 B. An order parameter is a measure of the gravitational force in a physical system.
 C. An order parameter is a measure of the magnetic field strength in a physical system.
 D. An order parameter is a measure of the degree of symmetry breaking in a physical system.
 E. An order parameter is a measure of the rotational symmetry in a physical system. "
What is the significance of the discovery of the Crab pulsar?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the significance of the discovery of the Crab pulsar?
 Context: -The discovery of the Crab pulsar provided confirmation of the rotating neutron star model of pulsars. The Crab pulsar 33-millisecond pulse period was too short to be consistent with other proposed models for pulsar emission. Moreover, the Crab pulsar is so named because it is located at the center of the Crab Nebula, consistent with the 1933 prediction of Baade and Zwicky.
-In 1968, Richard V. E. Lovelace and collaborators discovered period  33 ms of the Crab pulsar using Arecibo Observatory. After this discovery, scientists concluded that pulsars were rotating neutron stars. Before that, many scientists believed that pulsars were pulsating white dwarfs.
-Neutron stars Pulsars Supernovae sometimes leave behind dense spinning neutron stars called pulsars. They emit jets of charged particles which emit synchrotron radiation in the radio spectrum. Examples include the Crab Pulsar, the first pulsar to be discovered. Pulsars and quasars (dense central cores of extremely distant galaxies) were both discovered by radio astronomers. In 2003 astronomers using the Parkes radio telescope discovered two pulsars orbiting each other, the first such system known.
-On 11 January 2017, the first results from a survey of 118 unidentified pulsar-like sources from the Fermi-LAT Catalog were published. A total of 13 new pulsars were found. Most of them are young and were formed in supernovae several tens to hundreds of thousands of years ago. The discoveries and the methods used in the survey were published in the first of two associated papers. The second paper reports faint radio pulsations from two of the 13 gamma-ray pulsars, and presents modeling of the gamma-ray and radio pulse profiles with different geometric emission models.The discovery of two millisecond pulsars discovered by Einstein@Home through their pulsed gamma radiation was published on 28 February 2018. PSR J1035−6720, spinning at 348 Hertz, has detectable radio pulsations which were found in follow-up searches. The other discovery PSR J1744−7619 is the first radio-quiet millisecond pulsar ever discovered. The project also announced that it was searching for gamma-ray pulsars in binary systems, which are more difficult to find due to the additional orbital parameters.The first Einstein@Home discovery of a gamma-ray pulsar in a binary system was published on 22 October 2020. PSR J1653-0158, a neutron star with about two solar masses and one of the highest known rotation frequencies of 508 Hertz, orbits the common center of mass with a companion of only 1% of the Sun’s mass. The orbital period is 75 minutes, shorter than that of any comparable binary systems. The discovery was made using a GPU-accelerated version of a modified gamma-ray pulsar search code, which included binary orbital parameters. No radio waves were found in follow-up searches. A search for gravitational waves from the pulsar discovered no such emission. The pulsar is from a class known as black widow pulsars. The pulsar evaporates its companion with its energetic radiation and a particle wind. The ablated material fills the binary system with a cloud of plasma absorbing radio waves, but not gamma radiation.
-The word ""pulsar"" first appeared in print in 1968: An entirely novel kind of star came to light on Aug. 6 last year and was referred to, by astronomers, as LGM (Little Green Men). Now it is thought to be a novel type between a white dwarf and a neutron [star]. The name Pulsar is likely to be given to it. Dr. A. Hewish told me yesterday: '... I am sure that today every radio telescope is looking at the Pulsars.' The existence of neutron stars was first proposed by Walter Baade and Fritz Zwicky in 1934, when they argued that a small, dense star consisting primarily of neutrons would result from a supernova. Based on the idea of magnetic flux conservation from magnetic main sequence stars, Lodewijk Woltjer proposed in 1964 that such neutron stars might contain magnetic fields as large as 1014 to 1016 gauss (=1010 to 1012 tesla). In 1967, shortly before the discovery of pulsars, Franco Pacini suggested that a rotating neutron star with a magnetic field would emit radiation, and even noted that such energy could be pumped into a supernova remnant around a neutron star, such as the Crab Nebula. After the discovery of the first pulsar, Thomas Gold independently suggested a rotating neutron star model similar to that of Pacini, and explicitly argued that this model could explain the pulsed radiation observed by Bell Burnell and Hewish.  In 1968, Richard V. E. Lovelace with collaborators discovered period  33 ms of the Crab Nebula pulsar using Arecibo Observatory.
 A. The discovery of the Crab pulsar confirmed the black hole model of pulsars.
 B. The discovery of the Crab pulsar confirmed the rotating neutron star model of pulsars.
 C. The discovery of the Crab pulsar confirmed the white dwarf model of pulsars.
 D. The discovery of the Crab pulsar disproved the rotating neutron star model of pulsars.
 E. The discovery of the Crab pulsar confirmed the red giant model of pulsars. "
What is the De Haas-Van Alphen effect?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the De Haas-Van Alphen effect?
 Context: -Several experimental techniques allow for the measurement of the electronic properties of a material. An important effect in metals under strong magnetic fields, is the oscillation of the differential susceptibility as function of 1/H. This behaviour is known as the De Haas–Van Alphen effect and relates the period of the susceptibility with the Fermi surface of the material.
An analogue non-linear relation between magnetization and magnetic field happens for antiferromagnetic materials.
-When the magnetic susceptibility is measured in response to an AC magnetic field (i.e. a magnetic field that varies sinusoidally), this is called AC susceptibility. AC susceptibility (and the closely related ""AC permeability"") are complex number quantities, and various phenomena, such as resonance, can be seen in AC susceptibility that cannot occur in constant-field (DC) susceptibility. In particular, when an AC field is applied perpendicular to the detection direction (called the ""transverse susceptibility"" regardless of the frequency), the effect has a peak at the ferromagnetic resonance frequency of the material with a given static applied field. Currently, this effect is called the microwave permeability or network ferromagnetic resonance in the literature. These results are sensitive to the domain wall configuration of the material and eddy currents.
-The Fermi gas (an ensemble of non-interacting fermions) is part of the basis for understanding of the thermodynamic properties of metals. In 1930 Landau derived an estimate for the magnetic susceptibility of a Fermi gas, known as Landau susceptibility, which is constant for small magnetic fields. Landau also noticed that the susceptibility oscillates with high frequency for large magnetic fields, this physical phenomenon is known as the De Haas–Van Alphen effect.
-The differential magnetic susceptibility of a material is defined as χ=∂M∂H where  H is the applied external magnetic field and  M the magnetization of the material. Such that  B=μ0(H+M) , where  μ0 is the vacuum permeability. For practical purposes, the applied and the measured field are approximately the same  B≈μ0H (if the material is not ferromagnetic).
-The primary measurement in magnetochemistry is magnetic susceptibility. This measures the strength of interaction on placing the substance in a magnetic field. The volume magnetic susceptibility, represented by the symbol  χv is defined by the relationship M→=χvH→ where,  M→ is the magnetization of the material (the magnetic dipole moment per unit volume), measured in amperes per meter (SI units), and  H→ is the magnetic field strength, also measured in amperes per meter. Susceptibility is a dimensionless quantity. For chemical applications the molar magnetic susceptibility (χmol) is the preferred quantity. It is measured in m3·mol−1 (SI) or cm3·mol−1 (CGS) and is defined as mol =Mχv/ρ where ρ is the density in kg·m−3 (SI) or g·cm−3 (CGS) and M is molar mass in kg·mol−1 (SI) or g·mol−1 (CGS).
 A. The measurement of the electronic properties of a material using several experimental techniques.
 B. The complex number quantity that describes AC susceptibility and AC permeability.
 C. The oscillation of the differential susceptibility as a function of 1/H in metals under strong magnetic fields, which relates the period of the susceptibility with the Fermi surface of the material.
 D. The analogue non-linear relation between magnetization and magnetic field in antiferromagnetic materials.
 E. The measurement of magnetic susceptibility in response to an AC magnetic field. "
"What is a ""coffee ring"" in physics?","Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is a ""coffee ring"" in physics?
 Context: -In physics, a ""coffee ring"" is a pattern left by a puddle of particle-laden liquid after it evaporates. The phenomenon is named for the characteristic ring-like deposit along the perimeter of a spill of coffee. It is also commonly seen after spilling red wine. The mechanism behind the formation of these and similar rings is known as the coffee ring effect or in some instances, the coffee stain effect, or simply ring stain.
-The coffee-ring pattern originates from the capillary flow induced by the evaporation of the drop: liquid evaporating from the edge is replenished by liquid from the interior. The resulting current can carry nearly all the dispersed material to the edge. As a function of time, this process exhibits a ""rush-hour"" effect, that is, a rapid acceleration of the flow towards the edge at the final stage of the drying process.Evaporation induces a Marangoni flow inside a droplet. The flow, if strong, redistributes particles back to the center of the droplet. Thus, for particles to accumulate at the edges, the liquid must have a weak Marangoni flow, or something must occur to disrupt the flow. For example, surfactants can be added to reduce the liquid's surface tension gradient, disrupting the induced flow. Water has a weak Marangoni flow to begin with, which is then reduced significantly by natural surfactants.Interaction of the particles suspended in a droplet with the free surface of the droplet is important in creating a coffee ring. ""When the drop evaporates, the free surface collapses and traps the suspended particles ... eventually all the particles are captured by the free surface and stay there for the rest of their trip towards the edge of the drop."" This result means that surfactants can be used to manipulate the motion of the solute particles by changing the surface tension of the drop, rather than trying to control the bulk flow inside the drop. A number of interesting morphologies of the deposited particles can result. For example, an enantiopure poly (isocyanate) derivative has been shown to form ordered arrays of squashed donut structures.
-Coffee preparation is the process of turning coffee beans into liquid coffee. While the particular steps vary with the type of coffee and with the raw materials, the process includes four basic steps: raw coffee beans must be roasted, the roasted coffee beans must then be ground, and the ground coffee must then be mixed with hot or cold water (depending on the method of brewing) for a specific time (brewed), the liquid coffee extraction must be separated from the used grounds, and finally, if desired, the extracted coffee is combined with other elements of the desired beverage, such as sweeteners, dairy products, dairy alternatives, or toppings (such as shaved chocolate).  Coffee is usually brewed hot, at close to the boiling point of water, immediately before drinking, yielding a hot beverage capable of scalding if splashed or spilled; if not consumed promptly, coffee is often sealed into a vacuum flask or insulated bottle to maintain its temperature. In most areas, coffee may be purchased unprocessed, or already roasted, or already roasted and ground. Whole roast coffee or ground coffee is often vacuum-packed to prevent oxidation and lengthen its shelf life. Especially in hot climates, some find cold or iced coffee more refreshing. This can be prepared well in advance as it maintains its character when stored cold better than as a hot beverage.  Even with the same roast, the character of the extraction is highly dependent on distribution of particle sizes produced by the grinding process, temperature of the grounds after grinding, freshness of the roast and grind, brewing process and equipment, temperature of the water, character of the water itself, contact time with hot water (less sensitive with cold water), and the brew ratio employed. Preferred brew ratios of water to coffee often fall into the range of 15–18:1 by mass; even within this fairly small range, differences are easily perceived by an experienced coffee drinker. Processes can range from extremely manual (e.g. hand grinding with manual pour-over in steady increments) to totally automated by a single appliance with a reservoir of roast beans which it automatically measures and grinds, and water, which it automatically heats and doses. Another common style of automated coffee maker is fed a single-serving ""pod"" of pre-measured coffee grounds for each beverage.  Characteristics which may be emphasized or deemphasized by different preparation methods include: acidity (brightness), aroma (especially more delicate floral and citrus notes), mouthfeel (body), astringency, bitterness (both positive and negative), and the duration and intensity of flavour perception in the mouth (finish). The addition of sweeteners, dairy products (e.g. milk or cream), or dairy alternatives (e.g. almond milk) also changes the perceived character of the brewed coffee. Principally, dairy products mute delicate aromas and thicken mouthfeel (particularly when frothed), while sweeteners mask astringency and bitterness.
-Coffee preparation is the process of turning coffee beans into liquid coffee. While the particular steps vary with the type of coffee and with the raw materials, the process includes four basic steps: raw coffee beans must be roasted, the roasted coffee beans must then be ground, and the ground coffee must then be mixed with hot or cold water (depending on the method of brewing) for a specific time (brewed), the liquid coffee extraction must be separated from the used grounds, and finally, if desired, the extracted coffee is combined with other elements of the desired beverage, such as sweeteners, dairy products, dairy alternatives, or toppings (such as shaved chocolate).  Coffee is usually brewed hot, at close to the boiling point of water, immediately before drinking, yielding a hot beverage capable of scalding if splashed or spilled; if not consumed promptly, coffee is often sealed into a vacuum flask or insulated bottle to maintain its temperature. In most areas, coffee may be purchased unprocessed, or already roasted, or already roasted and ground. Whole roast coffee or ground coffee is often vacuum-packed to prevent oxidation and lengthen its shelf life. Especially in hot climates, some find cold or iced coffee more refreshing. This can be prepared well in advance as it maintains its character when stored cold better than as a hot beverage.  Even with the same roast, the character of the extraction is highly dependent on distribution of particle sizes produced by the grinding process, temperature of the grounds after grinding, freshness of the roast and grind, brewing process and equipment, temperature of the water, character of the water itself, contact time with hot water (less sensitive with cold water), and the brew ratio employed. Preferred brew ratios of water to coffee often fall into the range of 15–18:1 by mass; even within this fairly small range, differences are easily perceived by an experienced coffee drinker. Processes can range from extremely manual (e.g. hand grinding with manual pour-over in steady increments) to totally automated by a single appliance with a reservoir of roast beans which it automatically measures and grinds, and water, which it automatically heats and doses. Another common style of automated coffee maker is fed a single-serving ""pod"" of pre-measured coffee grounds for each beverage.  Characteristics which may be emphasized or deemphasized by different preparation methods include: acidity (brightness), aroma (especially more delicate floral and citrus notes), mouthfeel (body), astringency, bitterness (both positive and negative), and the duration and intensity of flavour perception in the mouth (finish). The addition of sweeteners, dairy products (e.g. milk or cream), or dairy alternatives (e.g. almond milk) also changes the perceived character of the brewed coffee. Principally, dairy products mute delicate aromas and thicken mouthfeel (particularly when frothed), while sweeteners mask astringency and bitterness.
-A coffeemaker, coffee maker or coffee machine is a cooking appliance used to brew coffee. While there are many different types of coffeemakers, the two most common brewing principles use gravity or pressure to move hot water through coffee grounds. In the most common devices, coffee grounds are placed into a paper or metal filter inside a funnel, which is set over a glass or ceramic coffee pot, a cooking pot in the kettle family. Cold water is poured into a separate chamber, which is then boiled and directed into the funnel and allowed to drip through the grounds under gravity. This is also called automatic drip-brew. Coffee makers that use pressure to force water through the coffee grounds are called espresso makers, and they produce espresso coffee.
 A. A type of coffee that is made by boiling coffee grounds in water.
 B. A pattern left by a particle-laden liquid after it is spilled, named for the characteristic ring-like deposit along the perimeter of a spill of coffee or red wine.
 C. A type of coffee that is made by mixing instant coffee with hot water.
 D. A type of coffee that is made by pouring hot water over coffee grounds in a filter.
 E. A pattern left by a particle-laden liquid after it evaporates, named for the characteristic ring-like deposit along the perimeter of a spill of coffee or red wine. "
What is the significance of probability amplitudes in quantum mechanics?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the significance of probability amplitudes in quantum mechanics?
 Context: -In quantum mechanics, a probability amplitude is a complex number used for describing the behaviour of systems. The modulus squared of this quantity represents a probability density.
-Probability amplitudes have special significance because they act in quantum mechanics as the equivalent of conventional probabilities, with many analogous laws, as described above. For example, in the classic double-slit experiment, electrons are fired randomly at two slits, and the probability distribution of detecting electrons at all parts on a large screen placed behind the slits, is questioned. An intuitive answer is that P(through either slit) = P(through first slit) + P(through second slit), where P(event) is the probability of that event. This is obvious if one assumes that an electron passes through either slit. When no measurement apparatus that determines through which slit the electrons travel is installed, the observed probability distribution on the screen reflects the interference pattern that is common with light waves. If one assumes the above law to be true, then this pattern cannot be explained. The particles cannot be said to go through either slit and the simple explanation does not work. The correct explanation is, however, by the association of probability amplitudes to each event. The complex amplitudes which represent the electron passing each slit (ψfirst and ψsecond) follow the law of precisely the form expected: ψtotal = ψfirst + ψsecond. This is the principle of quantum superposition. The probability, which is the modulus squared of the probability amplitude, then, follows the interference pattern under the requirement that amplitudes are complex: Here,  φ1 and  φ2 are the arguments of ψfirst and ψsecond respectively. A purely real formulation has too few dimensions to describe the system's state when superposition is taken into account. That is, without the arguments of the amplitudes, we cannot describe the phase-dependent interference. The crucial term  first second cos {\textstyle 2\left|\psi _{\text{first}}\right|\left|\psi _{\text{second}}\right|\cos(\varphi _{1}-\varphi _{2})} is called the ""interference term"", and this would be missing if we had added the probabilities.
-That basic scaffolding remains when one moves to a quantum description, but some conceptual changes are needed. One is that whereas we might expect in our everyday life that there would be some constraints on the points to which a particle can move, that is not true in full quantum electrodynamics. There is a nonzero probability amplitude of an electron at A, or a photon at B, moving as a basic action to any other place and time in the universe. That includes places that could only be reached at speeds greater than that of light and also earlier times. (An electron moving backwards in time can be viewed as a positron moving forward in time.): 89, 98–99 Probability amplitudes Quantum mechanics introduces an important change in the way probabilities are computed. Probabilities are still represented by the usual real numbers we use for probabilities in our everyday world, but probabilities are computed as the square modulus of probability amplitudes, which are complex numbers.
-Probability amplitudes The probability for a photon to be in a particular polarization state depends on the probability distribution over the fields as calculated by the classical Maxwell's equations (in the Glauber-Sudarshan P-representation of a one-photon Fock state.) The expectation value of the photon number in a coherent state in a limited region of space is quadratic in the fields. In quantum mechanics, by analogy, the state or probability amplitude of a single particle contains the basic probability information. In general, the rules for combining probability amplitudes look very much like the classical rules for composition of probabilities: The probability amplitude for two successive probabilities is the product of amplitudes for the individual possibilities. ...
-The nature of probability in quantum mechanics Probability for a single photon There are two ways in which probability can be applied to the behavior of photons; probability can be used to calculate the probable number of photons in a particular state, or probability can be used to calculate the likelihood of a single photon to be in a particular state. The former interpretation violates energy conservation. The latter interpretation is the viable, if nonintuitive, option. Dirac explains this in the context of the double-slit experiment: Some time before the discovery of quantum mechanics people realized that the connection between light waves and photons must be of a statistical character. What they did not clearly realize, however, was that the wave function gives information about the probability of one photon being in a particular place and not the probable number of photons in that place. The importance of the distinction can be made clear in the following way. Suppose we have a beam of light consisting of a large number of photons split up into two components of equal intensity. On the assumption that the beam is connected with the probable number of photons in it, we should have half the total number going into each component. If the two components are now made to interfere, we should require a photon in one component to be able to interfere with one in the other. Sometimes these two photons would have to annihilate one another and other times they would have to produce four photons. This would contradict the conservation of energy. The new theory, which connects the wave function with probabilities for one photon gets over the difficulty by making each photon go partly into each of the two components. Each photon then interferes only with itself. Interference between two different photons never occurs.—Paul Dirac, The Principles of Quantum Mechanics, 1930, Chapter 1 Probability amplitudes The probability for a photon to be in a particular polarization state depends on the fields as calculated by the classical Maxwell's equations. The polarization state of the photon is proportional to the field. The probability itself is quadratic in the fields and consequently is also quadratic in the quantum state of polarization. In quantum mechanics, therefore, the state or probability amplitude contains the basic probability information. In general, the rules for combining probability amplitudes look very much like the classical rules for composition of probabilities: [The following quote is from Baym, Chapter 1] The probability amplitude for two successive probabilities is the product of amplitudes for the individual possibilities. For example, the amplitude for the x polarized photon to be right circularly polarized and for the right circularly polarized photon to pass through the y-polaroid is  ⟨R|x⟩⟨y|R⟩, the product of the individual amplitudes.
 A. Probability amplitudes are used to determine the mass of particles in quantum mechanics.
 B. Probability amplitudes have no significance in quantum mechanics.
 C. Probability amplitudes are used to determine the velocity of particles in quantum mechanics.
 D. Probability amplitudes act as the equivalent of conventional probabilities in classical mechanics, with many analogous laws.
 E. Probability amplitudes act as the equivalent of conventional probabilities in quantum mechanics, with many analogous laws. "
What is the relationship between the amplitude of a sound wave and its loudness?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the relationship between the amplitude of a sound wave and its loudness?
 Context: -The amplitude of sound waves and audio signals (which relates to the volume) conventionally refers to the amplitude of the air pressure in the wave, but sometimes the amplitude of the displacement (movements of the air or the diaphragm of a speaker) is described. The logarithm of the amplitude squared is usually quoted in dB, so a null amplitude corresponds to −∞ dB. Loudness is related to amplitude and intensity and is one of the most salient qualities of a sound, although in general sounds it can be recognized independently of amplitude. The square of the amplitude is proportional to the intensity of the wave.
-Important basic characteristics of waves are wavelength, amplitude, period, and frequency. Wavelength is the length of the repeating wave shape. Amplitude is the maximum displacement of the particles of the medium, which is determined by the energy of the wave. A period (measured in seconds) is the time for one wave to pass a given point. Frequency of the wave is the number of waves passing a given point in a unit of time. Frequency is measured in hertz (hz); (Hz cycles per second) and is perceived as pitch. Each complete vibration of a sound wave is called a cycle. Two other physical properties of sound are intensity and duration. Intensity is measured in decibels (dB) and is perceived as loudness.
-The ordinary frequency (f) of the wave is given by f=ω2π.
The wavelength can be calculated as the relation between a wave's speed and ordinary frequency.
λ=cf.
For sound waves, the amplitude of the wave is the difference between the pressure of the undisturbed air and the maximum pressure caused by the wave.
Sound's propagation speed depends on the type, temperature, and composition of the medium through which it propagates.
-The perception of loudness is related to sound pressure level (SPL), frequency content and duration of a sound. The relationship between SPL and loudness of a single tone can be approximated by Stevens's power law in which SPL has an exponent of 0.67. A more precise model known as the Inflected Exponential function, indicates that loudness increases with a higher exponent at low and high levels and with a lower exponent at moderate levels.The sensitivity of the human ear changes as a function of frequency, as shown in the equal-loudness graph. Each line on this graph shows the SPL required for frequencies to be perceived as equally loud, and different curves pertain to different sound pressure levels. It also shows that humans with normal hearing are most sensitive to sounds around 2–4 kHz, with sensitivity declining to either side of this region. A complete model of the perception of loudness will include the integration of SPL by frequency.Historically, loudness was measured using an ""ear-balance"" audiometer in which the amplitude of a sine wave was adjusted by the user to equal the perceived loudness of the sound being evaluated. Contemporary standards for measurement of loudness are based on the summation of energy in critical bands.
-Sound power is related to sound intensity: P=AI, where A stands for the area; I stands for the sound intensity.Sound power is related sound energy density: P=Acw, where c stands for the speed of sound; w stands for the sound energy density.
 A. The amplitude of a sound wave is related to its loudness.
 B. The amplitude of a sound wave is directly proportional to its frequency.
 C. The amplitude of a sound wave is not related to its loudness.
 D. The amplitude of a sound wave is not related to its frequency.
 E. The amplitude of a sound wave is inversely related to its loudness. "
What are coherent turbulent structures?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What are coherent turbulent structures?
 Context: -Turbulent flows are complex multi-scale and chaotic motions that need to be classified into more elementary components, referred to coherent turbulent structures. Such a structure must have temporal coherence, i.e. it must persist in its form for long enough periods that the methods of time-averaged statistics can be applied. Coherent structures are typically studied on very large scales, but can be broken down into more elementary structures with coherent properties of their own, such examples include hairpin vortices. Hairpins and coherent structures have been studied and noticed in data since the 1930s, and have been since cited in thousands of scientific papers and reviews.
-A turbulent flow is a flow regime in fluid dynamics where fluid velocity varies significantly and irregularly in both position and time. Furthermore, a coherent structure is defined as a turbulent flow whose vorticity expression, which is usually stochastic, contains orderly components that can be described as being instantaneously coherent over the spatial extent of the flow structure. In other words, underlying the three-dimensional chaotic vorticity expressions typical of turbulent flows, there is an organized component of that vorticity which is phase-correlated over the entire space of the structure. The instantaneously space and phase correlated vorticity found within the coherent structure expressions can be defined as coherent vorticity, hence making coherent vorticity the main characteristic identifier for coherent structures. Another characteristic inherent in turbulent flows is their intermittency, but intermittency is a very poor identifier of the boundaries of a coherent structure, hence it is generally accepted that the best way to characterize the boundary of a structure is by identifying and defining the boundary of the coherent vorticity.By defining and identifying coherent structure in this manner, turbulent flows can be decomposed into coherent structures and incoherent structures depending on their coherence, particularly their correlations with their vorticity. Hence, similarly organized events in an ensemble average of organized events can be defined as a coherent structure, and whatever events not identified as similar or phase and space aligned in the ensemble average is an incoherent turbulent structure.  Other attempts at defining a coherent structure can be done through examining the correlation between their momenta or pressure and their turbulent flows. However, it often leads to false indications of turbulence, since pressure and velocity fluctuations over a fluid could be well correlated in the absence of any turbulence or vorticity. Some coherent structures, such as vortex rings, etc. can be large-scale motions comparable to the extent of the shear flow. There are also coherent motions at much smaller scales such as hairpin vortices and typical eddies, which are typically known as coherent substructures, as in coherent structures which can be broken up into smaller more elementary substructures.
-Although a coherent structure is by definition characterized by high levels of coherent vorticity, Reynolds stress, production, and heat and mass transportation, it does not necessarily require a high level of kinetic energy. In fact, one of the main roles of coherent structures is the large-scale transport of mass, heat, and momentum without requiring the high amounts of energy normally needed. Consequently, this implies that coherent structures are not the main production and cause of Reynolds stress, and incoherent turbulence can be similarly significant.Coherent structures cannot superimpose, i.e. they cannot overlap and each coherent structure has its own independent domain and boundary. Since eddies coexist as spatial superpositions, a coherent structure is not an eddy. For example, eddies dissipate energy by obtaining energy from the mean flow at large scales, and eventually dissipating it at the smallest scales. There is no such analogous exchange of energy between coherent structures, and any interaction such as tearing between coherent structures simply results in a new structure. However, two coherent structures can interact and influence each other. The mass of a structure change with time, with the typical case being that structures increase in volume via the diffusion of vorticity.  One of the most fundamental quantities of coherent structures is characterized by coherent vorticity,  Ωc . Perhaps the next most critical measures of coherent structures are the coherent vs. incoherent Reynold's stresses,  −ucνc and  −⟨urνr⟩ . These represent the transports of momentum, and their relative strength indicates how much momentum is being transported by coherent structures as compared to incoherent structures. The next most significant measures include contoured depictions of coherent strain rate and shear production. A useful property of such contours is that they are invariant under Galilean transformations, hence the contours of coherent vorticity constitute an excellent identifier to the structure's boundaries. The contours of these properties not only locate where exactly coherent structure quantities have their peaks and saddles, but also identify where the incoherent turbulent structures are when overlaid on their directional gradients. In addition, spatial contours can be drawn describe the shape, size, and strength of the coherent structures, depicting not only the mechanics but also the dynamical evolution of coherent structures. For example, in order for a structure to be evolving, and hence dominant, its coherent vorticity, coherent Reynolds stress, and production terms should be larger than the time averaged values of the flow structures.
-In many geophysical flows (rivers, atmospheric boundary layer), the flow turbulence is dominated by the coherent structures and turbulent events. A turbulent event is a series of turbulent fluctuations that contain more energy than the average flow turbulence. The turbulent events are associated with coherent flow structures such as eddies and turbulent bursting, and they play a critical role in terms of sediment scour, accretion and transport in rivers as well as contaminant mixing and dispersion in rivers and estuaries, and in the atmosphere.
-Flow visualization experiments, using smoke and dye as tracers, have been historically used to simulate coherent structures and verify theories, but computer models are now the dominant tools widely used in the field to verify and understand the formation, evolution, and other properties of such structures. The kinematic properties of these motions include size, scale, shape, vorticity, energy, and the dynamic properties govern the way coherent structures grow, evolve, and decay. Most coherent structures are studied only within the confined forms of simple wall turbulence, which approximates the coherence to be steady, fully developed, incompressible, and with a zero pressure gradient in the boundary layer. Although such approximations depart from reality, they contain sufficient parameters needed to understand turbulent coherent structures in a highly conceptual degree.
 A. Coherent turbulent structures are the most elementary components of complex multi-scale and chaotic motions in turbulent flows, which do not have temporal coherence and persist in their form for long enough periods that the methods of time-averaged statistics can be applied.
 B. Coherent turbulent structures are the most elementary components of complex multi-scale and chaotic motions in turbulent flows, which have temporal coherence and persist in their form for very short periods that the methods of time-averaged statistics cannot be applied.
 C. Coherent turbulent structures are more elementary components of complex multi-scale and chaotic motions in turbulent flows, which have temporal coherence and persist in their form for long enough periods that the methods of time-averaged statistics can be applied.
 D. Coherent turbulent structures are the most complex and chaotic motions in turbulent flows, which have temporal coherence and persist in their form for long enough periods that the methods of time-averaged statistics can be applied.
 E. Coherent turbulent structures are the most complex and chaotic motions in turbulent flows, which do not have temporal coherence and persist in their form for very short periods that the methods of time-averaged statistics cannot be applied. "
What is the main factor that determines the occurrence of each type of supernova?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the main factor that determines the occurrence of each type of supernova?
 Context: -Progenitor The supernova classification type is closely tied to the type of star at the time of the collapse. The occurrence of each type of supernova depends on the progenitor star's metallicity, since this affects the strength of the stellar wind and thereby the rate at which the star loses mass.Type Ia supernovae are produced from white dwarf stars in binary star systems and occur in all galaxy types. Core collapse supernovae are only found in galaxies undergoing current or very recent star formation, since they result from short-lived massive stars. They are most commonly found in type Sc spirals, but also in the arms of other spiral galaxies and in irregular galaxies, especially starburst galaxies.Type Ib and Ic supernovae are hypothesized to have been produced by core collapse of massive stars that have lost their outer layer of hydrogen and helium, either via strong stellar winds or mass transfer to a companion. They normally occur in regions of new star formation, and are extremely rare in elliptical galaxies. The progenitors of type IIn supernovae also have high rates of mass loss in the period just prior to their explosions. Type Ic supernovae have been observed to occur in regions that are more metal-rich and have higher star-formation rates than average for their host galaxies. The table shows the progenitor for the main types of core collapse supernova, and the approximate proportions that have been observed in the local neighbourhood.
-All known life requires the complex chemistry of metallic elements. The absorption spectrum of a star reveals the presence of metals within, and studies of stellar spectra reveal that many, perhaps most, stars are poor in metals. Because heavy metals originate in supernova explosions, metallicity increases in the universe over time. Low metallicity characterizes the early universe: globular clusters and other stars that formed when the universe was young, stars in most galaxies other than large spirals, and stars in the outer regions of all galaxies. Metal-rich central stars capable of supporting complex life are therefore believed to be most common in the less dense regions of the larger spiral galaxies—where radiation also happens to be weak.
-In the current system of stellar classification, stars are grouped according to temperature, with the massive, very young and energetic Class O stars boasting temperatures in excess of 30,000 K while the less massive, typically older Class M stars exhibit temperatures less than 3,500 K. Because luminosity is proportional to temperature to the fourth power, the large variation in stellar temperatures produces an even vaster variation in stellar luminosity. Because the luminosity depends on a high power of the stellar mass, high mass luminous stars have much shorter lifetimes. The most luminous stars are always young stars, no more than a few million years for the most extreme. In the Hertzsprung–Russell diagram, the x-axis represents temperature or spectral type while the y-axis represents luminosity or magnitude. The vast majority of stars are found along the main sequence with blue Class O stars found at the top left of the chart while red Class M stars fall to the bottom right. Certain stars like Deneb and Betelgeuse are found above and to the right of the main sequence, more luminous or cooler than their equivalents on the main sequence. Increased luminosity at the same temperature, or alternatively cooler temperature at the same luminosity, indicates that these stars are larger than those on the main sequence and they are called giants or supergiants.
-Unlike the other types of supernovae, Type Ia supernovae generally occur in all types of galaxies, including ellipticals. They show no preference for regions of current stellar formation. As white dwarf stars form at the end of a star's main sequence evolutionary period, such a long-lived star system may have wandered far from the region where it originally formed. Thereafter a close binary system may spend another million years in the mass transfer stage (possibly forming persistent nova outbursts) before the conditions are ripe for a Type Ia supernova to occur.A long-standing problem in astronomy has been the identification of supernova progenitors. Direct observation of a progenitor would provide useful constraints on supernova models. As of 2006, the search for such a progenitor had been ongoing for longer than a century. Observation of the supernova SN 2011fe has provided useful constraints. Previous observations with the Hubble Space Telescope did not show a star at the position of the event, thereby excluding a red giant as the source. The expanding plasma from the explosion was found to contain carbon and oxygen, making it likely the progenitor was a white dwarf primarily composed of these elements.
-Supergiant luminosity classes are easy to determine and apply to large numbers of stars, but they group several very different types of stars into a single category. An evolutionary definition restricts the term supergiant to those massive stars which start core helium fusion without developing a degenerate helium core and without undergoing a helium flash. They will universally go on to burn heavier elements and undergo core-collapse resulting in a supernova.Less massive stars may develop a supergiant spectral luminosity class at relatively low luminosity, around 1,000 L☉ when they are on the asymptotic giant branch (AGB) undergoing helium shell burning. Researchers now prefer to categorize these as AGB stars distinct from supergiants because they are less massive, have different chemical compositions at the surface, undergo different types of pulsation and variability, and will evolve differently, usually producing a planetary nebula and white dwarf. Most AGB stars will not become supernovae although there is interest in a class of super-AGB stars, those almost massive enough to undergo full carbon fusion, which may produce peculiar supernovae although without ever developing an iron core. One notable group of low mass high luminosity stars are the RV Tauri variables, AGB or post-AGB stars lying on the instability strip and showing distinctive semi-regular variations.
 A. The star's distance from Earth
 B. The star's age
 C. The star's temperature
 D. The star's luminosity
 E. The progenitor star's metallicity "
What is the Erlangen program?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the Erlangen program?
 Context: -In mathematics, the Erlangen program is a method of characterizing geometries based on group theory and projective geometry. It was published by Felix Klein in 1872 as Vergleichende Betrachtungen über neuere geometrische Forschungen. It is named after the University Erlangen-Nürnberg, where Klein worked.
-When topology is routinely described in terms of properties invariant under homeomorphism, one can see the underlying idea in operation. The groups involved will be infinite-dimensional in almost all cases – and not Lie groups – but the philosophy is the same. Of course this mostly speaks to the pedagogical influence of Klein. Books such as those by H.S.M. Coxeter routinely used the Erlangen program approach to help 'place' geometries. In pedagogic terms, the program became transformation geometry, a mixed blessing in the sense that it builds on stronger intuitions than the style of Euclid, but is less easily converted into a logical system.
-In 1872, Felix Klein noted that the many branches of geometry which had been developed during the 19th century (affine geometry, projective geometry, hyperbolic geometry, etc.) could all be treated in a uniform way. He did this by considering the groups under which the geometric objects were invariant. This unification of geometry goes by the name of the Erlangen programme.The general theory of angle can be unified with invariant measure of area. The hyperbolic angle is defined in terms of area, very nearly the area associated with natural logarithm. The circular angle also has area interpretation when referred to a circle with radius equal to the square root of two. These areas are invariant with respect to hyperbolic rotation and circular rotation respectively. These affine transformations are effected by elements of the special linear group SL(2,R). Inspection of that group reveals shear mappings which increase or decrease slopes but differences of slope do not change. A third type of angle, also interpreted as an area dependent on slope differences, is invariant because of area preservation of a shear mapping.
-In the seminal paper which introduced categories, Saunders Mac Lane and Samuel Eilenberg stated: ""This may be regarded as a continuation of the Klein Erlanger Program, in the sense that a geometrical space with its group of transformations is generalized to a category with its algebra of mappings.""Relations of the Erlangen program with work of Charles Ehresmann on groupoids in geometry is considered in the article below by Pradines.In mathematical logic, the Erlangen program also served as an inspiration for Alfred Tarski in his analysis of logical notions.
-The long-term effects of the Erlangen program can be seen all over pure mathematics (see tacit use at congruence (geometry), for example); and the idea of transformations and of synthesis using groups of symmetry has become standard in physics.
 A. The Erlangen program is a method of characterizing geometries based on statistics and probability, published by Felix Klein in 1872 as Vergleichende Betrachtungen über neuere geometrische Forschungen.
 B. The Erlangen program is a method of characterizing geometries based on group theory and projective geometry, published by Felix Klein in 1872 as Vergleichende Betrachtungen über neuere geometrische Forschungen.
 C. The Erlangen program is a method of characterizing geometries based on algebra and trigonometry, published by Felix Klein in 1872 as Vergleichende Betrachtungen über neuere geometrische Forschungen.
 D. The Erlangen program is a method of characterizing geometries based on geometry and topology, published by Felix Klein in 1872 as Vergleichende Betrachtungen über neuere geometrische Forschungen.
 E. The Erlangen program is a method of characterizing geometries based on calculus and differential equations, published by Felix Klein in 1872 as Vergleichende Betrachtungen über neuere geometrische Forschungen. "
What is emissivity?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is emissivity?
 Context: -Emissivity is a term that is often misunderstood and misused. It represents a material's ability to emit thermal radiation and is an optical property of matter.
-Emissivity is the value given to materials based on the ratio of heat emitted compared to a perfect black body, on a scale from zero to one. A black body would have an emissivity of 1 and a perfect reflector would have a value of 0.
-Thermal emittance or thermal emissivity ( ε ) is the ratio of the radiant emittance of heat of a specific object or surface to that of a standard black body. Emissivity and emittivity are both dimensionless quantities given in the range of 0 to 1, representing the comparative/relative emittance with respect to a blackbody operating in similar conditions, but emissivity refers to a material property (of a homogeneous material), while emittivity refers to specific samples or objects.For building products, thermal emittance measurements are taken for wavelengths in the infrared. Determining the thermal emittance and solar reflectance of building materials, especially roofing materials, can be very useful for reducing heating and cooling energy costs in buildings. Combined index Solar Reflectance Index (SRI) is often used to determine the overall ability to reflect solar heat and release thermal heat. A roofing surface with high solar reflectance and high thermal emittance will reflect solar heat and release absorbed heat readily. High thermal emittance material radiates thermal heat back into the atmosphere more readily than one with a low thermal emittance. In common construction applications, the thermal emittance of a surface is usually higher than 0.8–0.85.High thermal emittance materials are essential to passive daytime radiative cooling, which uses surfaces high in thermal emittance and solar reflectance to lower surface temperatures by dissipating heat to outer space. It has been proposed as a solution to energy crises and global warming.
-The emissivity of a material (usually written ε or e) is the relative ability of its surface to emit energy by radiation. A black body has an emissivity of 1 and a perfect reflector has an emissivity of 0.In radiative heat transfer, a view factor quantifies the relative importance of the radiation that leaves an object (person or surface) and strikes another one, considering the other surrounding objects. In enclosures, radiation leaving a surface is conserved, therefore, the sum of all view factors associated with a given object is equal to 1.
-All materials in existence give off, or emit, energy by thermal radiation as a result of their temperature. The amount of energy radiated depends on the surface temperature and a property called emissivity (also called ""emittance""). Emissivity is expressed as a number between zero and one at a given wavelength. The higher the emissivity, the greater the emitted radiation at that wavelength. A related material property is reflectivity (also called ""reflectance""). This is a measure of how much energy is reflected by a material at a given wavelength. Reflectivity is also expressed as a number between 0 and 1 (or a percentage between 0 and 100). At a given wavelength and angle of incidence the emissivity and reflectivity values sum to 1 by Kirchhoff's law.Radiant barrier materials must have low emissivity (usually 0.1 or less) at the wavelengths at which they are expected to function. For typical building materials, the wavelengths are in the mid- and long-infrared spectrum, in the range of 3-15 micrometres.Radiant barriers may or may not exhibit high visual reflectivity. While reflectivity and emissivity must sum to 1 at a given wavelength, reflectivity at one set of wavelengths (visible) and emissivity at a different set of wavelengths (thermal) do not necessarily sum to 1. Therefore, it is possible to create visibly dark colored surfaces with low thermal emissivity.To perform properly, radiant barriers need to face open space (e.g., air or vacuum) through which there would otherwise be radiation.
 A. Emissivity is a measure of how well a surface resists deformation under stress.
 B. Emissivity is a measure of how well a surface conducts heat.
 C. Emissivity is a measure of how well a surface absorbs and emits thermal radiation.
 D. Emissivity is a measure of how well a surface reflects visible light.
 E. Emissivity is a measure of how well a surface absorbs and emits sound waves. "
Who was the first person to describe the pulmonary circulation system?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: Who was the first person to describe the pulmonary circulation system?
 Context: -Pre-modern The earliest descriptions of the coronary and pulmonary circulation systems can be found in the Commentary on Anatomy in Avicenna's Canon, published in 1242 by Ibn al-Nafis. In his manuscript, al-Nafis wrote that blood passes through the pulmonary circulation instead of moving from the right to the left ventricle as previously believed by Galen. His work was later translated into Latin by Andrea Alpago.In Europe, the teachings of Galen continued to dominate the academic community and his doctrines were adopted as the official canon of the Church. Andreas Vesalius questioned some of Galen's beliefs of the heart in De humani corporis fabrica (1543), but his magnum opus was interpreted as a challenge to the authorities and he was subjected to a number of attacks. Michael Servetus wrote in Christianismi Restitutio (1553) that blood flows from one side of the heart to the other via the lungs.
-The pulmonary circulation is archaically known as the ""lesser circulation"" which is still used in non-English literature.The discovery of the pulmonary circulation has been attributed to many scientists with credit distributed in varying ratios by varying sources. In much of modern medical literature, the discovery is credited to English physician William Harvey (1578 – 1657 CE) based on the comprehensive completeness and correctness of his model, despite its relative recency. Other sources credit Greek philosopher Hippocrates (460 – 370 BCE), Spanish physician Michael Servetus (c. 1509 – 1553 CE), Arab physician Ibn al-Nafis (1213 – 1288 CE), and Syrian physician Qusta ibn Luqa. Several figures such as Hippocrates and al-Nafis receive credit for accurately predicting or developing specific elements of the modern model of pulmonary circulation: Hippocrates for being the first to describe pulmonary circulation as a discrete system separable from systemic circulation as a whole and al-Nafis for making great strides over the understanding of those before him and towards a rigorous model. There is a great deal of subjectivity involved in deciding at which point a complex system is ""discovered"", as it is typically elucidated in piecemeal form so that the very first description, most complete or accurate description, and the most significant forward leaps in understanding are all considered acts of discovery of varying significance.Primitive descriptions of the cardiovascular system are found throughout several ancient cultures. The earliest known description of the role of air in circulation was produced in Egypt in 3500 BCE. At the time, the Egyptians believed that the heart was the origin of many channels that connected different parts of the body to each other and transported air – as well as urine, blood, and the soul – between them. The Edwin Smith Papyrus (1700 BCE), named for American Egyptologist Edwin Smith (1822 – 1906 CE) who purchased the scroll in 1862, provided evidence that Egyptians believed that the heartbeat created a pulse that transported the above substances throughout the body. A second scroll, the Ebers Papyrus (c. 1550 BCE), also emphasized the importance of the heart and its connection to vessels throughout the body and described methods to detect cardiac disease through pulse abnormalities. Although they had knowledge of the heartbeat, vessels, and pulse, the Egyptians attributed the movement of substances through the vessels to air that resided in these channels, rather than to the heart's exertion of pressure. The Egyptians knew that air played an important role in circulation but did not yet have a conception of the role of the lungs.
-One of the first major discoveries relevant to the field of pulmonology was the discovery of pulmonary circulation. Originally, it was thought that blood reaching the right side of the heart passed through small 'pores' in the septum into the left side to be oxygenated, as theorized by Galen; however, the discovery of pulmonary circulation disproves this theory, which had previously been accepted since the 2nd century. Thirteenth-century anatomist and physiologist Ibn Al-Nafis accurately theorized that there was no 'direct' passage between the two sides (ventricles) of the heart. He believed that the blood must have passed through the pulmonary artery, through the lungs, and back into the heart to be pumped around the body. This is believed by many to be the first scientific description of pulmonary circulation.Although pulmonary medicine only began to evolve as a medical specialty in the 1950s, William Welch and William Osler founded the 'parent' organization of the American Thoracic Society, the National Association for the Study and Prevention of Tuberculosis. The care, treatment, and study of tuberculosis of the lung is recognised as a discipline in its own right, phthisiology. When the specialty did begin to evolve, several discoveries were being made linking the respiratory system and the measurement of arterial blood gases, attracting more and more physicians and researchers to the developing field.
-The next addition to the historical understanding of pulmonary circulation arrived with the Ancient Greeks. Physician Alcmaeon (520 – 450 BCE) proposed that the brain, not the heart, was the connection point for all of the vessels in the body. He believed that the function of these vessels was to bring the ""spirit"" (""pneuma"") and air to the brain. Empedocles (492 – 432 BCE), a philosopher, proposed a series of pipes, impermeable to blood but continuous with blood vessels, that carried the pneuma throughout the body. He proposed that this spirit was internalized by pulmonary respiration.Hippocrates was the first to describe pulmonary circulation as a discrete system, separable from systemic circulation, in his Corpus Hippocraticum, which is often regarded as the foundational text of modern medicine. Hippocrates developed the view that the liver and spleen produced blood, and that this traveled to the heart to be cooled by the lungs that surrounded it. He described the heart as having two ventricles connected by an interventricular septum, and depicted the heart as the nexus point of all of the vessels of the body. He proposed that some vessels carried only blood and that others carried only air. He hypothesized that these air-carrying vessels were divisible into the pulmonary veins, which carried in air to the left ventricle, and the pulmonary artery, which carried in air to the right ventricle and blood to the lungs. He also proposed the existence of two atria of the heart functioning to capture air. He was one of the first to begin to accurately describe the anatomy of the heart and to describe the involvement of the lungs in circulation. His descriptions built substantially on previous and contemporaneous efforts but, by modern standards, his conceptions of pulmonary circulation and of the functions of the parts of the heart were still largely inaccurate.Greek philosopher and scientist Aristotle (384 – 322 BCE) followed Hippocrates and proposed that the heart had three ventricles, rather than two, that all connected to the lungs. Greek physician Erasistratus (315 – 240 BCE) agreed with Hippocrates and Aristotle that the heart was the origin of all of the vessels in the body but proposed a system in which air was drawn into the lungs and traveled to the left ventricle via pulmonary veins. It was transformed there into the pneuma and distributed throughout the body by arteries, which contained only air. In this system, veins distributed blood throughout the body, and thus blood did not circulate, but rather was consumed by the organs.The Greek physician Galen (129 – c. 210 CE) provided the next insights into pulmonary circulation. Though many of his theories, like those of his predecessors, were marginally or completely incorrect, his theory of pulmonary circulation dominated the medical community's understanding for hundreds of years after his death. Galen contradicted Erasistratus before him by proposing that arteries carried both air and blood, rather than air alone (which was essentially correct, leaving aside that blood vessels carry constituents of air and not air itself). He proposed that the liver was the originating point of all blood vessels. He also theorized that the heart was not a pumping muscle but rather an organ through which blood passed. Galen's theory included a new description of pulmonary circulation: air was inhaled into the lungs where it became the pneuma. Pulmonary veins transmitted this pneuma to the left ventricle of the heart to cool the blood simultaneously arriving there. This mixture of pneuma, blood, and cooling produced the vital spirits that could then be transported throughout the body via arteries. Galen further proposed that the heat of the blood arriving in the heart produced noxious vapors that were expelled through the same pulmonary veins that first brought the pneuma. He wrote that the right ventricle played a different role to the left: it transported blood to the lungs where the impurities were vented out so that clean blood could be distributed throughout the body. Though Galen's description of the anatomy of the heart was more complete than those of his predecessors, it included several mistakes. Most notably, Galen believed that blood flowed between the two ventricles of the heart through small, invisible pores in the interventricular septum.The next significant developments in the understanding of pulmonary circulation did not arrive until centuries later. Persian polymath Avicenna (c. 980 – 1037 CE) wrote a medical encyclopedia entitled The Canon of Medicine. In it, he translated and compiled contemporary medical knowledge and added some new information of his own. However, Avicenna's description of pulmonary circulation reflected the incorrect views of Galen.The Arab physician, Ibn al-Nafis, wrote the Commentary on Anatomy in Avicenna's Canon in 1242 in which he provided possibly the first known description of the system that remains substantially congruent with modern understandings, in spite of its flaws. Ibn al-Nafis made two key improvements on Galen's ideas. First, he disproved the existence of the pores in the interventricular septum that Galen had believed allowed blood to flow between the left and right ventricles. Second, he surmised that the only way for blood to get from the right to the left ventricle in the absence of interventricular pores was a system like pulmonary circulation. He also described the anatomy of the lungs in clear and basically correct detail, which his predecessors had not. However, like Aristotle and Galen, al-Nafis still believed in the quasi-mythical concept of vital spirit and that it was formed in the left ventricle from a mixture of blood and air. Despite the enormity of Ibn al-Nafis's improvements on the theories that preceded him, his commentary on The Canon was not widely known to Western scholars until the manuscript was discovered in Berlin, Germany, in 1924. As a result, the ongoing debate among Western scholars as to how credit for the discovery should be apportioned failed to include Ibn al-Nafis until, at earliest, the mid-20th century (shortly after which he came to enjoy a share of this credit). In 2021, several researchers described a text predating the work of al-Nafis, fargh- beyn-roh va nafs, in which there is a comparable report on pulmonary circulation. The researchers argue that its author, Qusta ibn Luqa, is the best candidate for the discoverer of pulmonary circulation on a similar basis to arguments in favour of al-Nafis generally.It took centuries for other scientists and physicians to reach conclusions that were similar to and then more accurate than those of al-Nafis and ibn Luqa. This later progress, constituting the gap between medieval and modern understanding, occurred throughout Europe. Italian polymath Leonardo da Vinci (1452 – 1519 CE) was one of the first to propose that the heart was just a muscle, rather than a vessel of spirits and air, but he still subscribed to Galen's ideas of circulation and defended the existence of interventricular pores. The Flemish physician Andreas Vesalius (1514 – 1564 CE) published corrections to Galen's view of circulatory anatomy, questioning the existence of interventricular pores, in his book De humani corporis fabrica libri septem in 1543. Spanish Michael Servetus, after him, was the first European physician to accurately describe pulmonary circulation. His assertions largely matched those of al-Nafis. In subsequent centuries, he has frequently been credited with the discovery, but some historians have propounded the idea that he potentially had access to Ibn al-Nafis's work while writing his own texts. Servetus published his findings in Christianismi Restituto (1553): a theological work that was considered heretical by Catholics and Calvinists alike. As a result, both book and author were burned at the stake and only a few copies of his work survived. Italian physician Realdo Colombo (c. 1515 – 1559 CE) published a book, De re anatomica libri XV, in 1559 that accurately described pulmonary circulation. It is still a matter of debate among historians as to whether Colombo reached his conclusions alone or based them to an unknown degree on the works of al-Nafis and Servetus. Finally, in 1628, the influential British physician William Harvey (1578 – 1657 AD) provided at the time the most complete and accurate description of pulmonary circulation of any scholar worldwide in his treatise Exercitatio Anatomica de Motu Cordis et Sanguinis in Animalibus. At the macroscopic level, his model is still recognizable in and reconcilable with modern understandings of pulmonary circulation.
-The earliest known writings on the circulatory system are found in the Ebers Papyrus (16th century BCE), an ancient Egyptian medical papyrus containing over 700 prescriptions and remedies, both physical and spiritual. In the papyrus, it acknowledges the connection of the heart to the arteries. The Egyptians thought air came in through the mouth and into the lungs and heart. From the heart, the air travelled to every member through the arteries. Although this concept of the circulatory system is only partially correct, it represents one of the earliest accounts of scientific thought.
 A. Galen
 B. Avicenna
 C. Hippocrates
 D. Aristotle
 E. Ibn al-Nafis "
What is the fate of a carbocation formed in crystalline naphthalene?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the fate of a carbocation formed in crystalline naphthalene?
 Context: -Radical formation In a condensed phase, the carbocation can also gain an electron from surrounding molecules, thus becoming an electrically neutral radical. For example, in crystalline naphthalene, a molecule with tritium substituted for hydrogen in the 1 (or 2) position will be turned by decay into a cation with a positive charge at that position. That charge will however be quickly neutralized by an electron transported through the lattice, turning the molecule into the 1-naphthyl (or 2-naphthyl) radical; which are stable, trapped in the solid, below 170 K (−103 °C).
-In the basic method, a molecule (R,R′,R″)C−T is prepared where the vacant bond of the desired radical or ion is satisfied by an atom of tritium 3H, the radioactive isotope of hydrogen with mass number 3. As the tritium undergoes beta decay (with a half-life of 12.32 years), it is transformed into an ion of helium-3, creating the cation (R,R′,R″)C−[3He]+.In the decay, an electron and an antineutrino are ejected at great speed from the tritium nucleus, changing one of the neutrons into a proton with the release of 18,600 electronvolts (eV) of energy. The neutrino escapes the system; the electron is generally captured within a short distance, but far enough away from the site of the decay that it can be considered lost from the molecule. Those two particles carry away most of the released energy, but their departure causes the nucleus to recoil, with about 1.6 eV of energy. This recoil energy is larger than the bond strength of the carbon–helium bond (about 1 eV), so this bond breaks. The helium atom almost always leaves as a neutral 3He, leaving behind the carbocation [(R,R′,R″)C]+.These events happen very quickly compared to typical molecular relaxation times, so the carbocation is usually created in the same conformation and electronic configuration as the original neutral molecule. For example, decay of tritiated methane, CH3T (R = R′ = R″ = H) produces the carbenium ion H3C+ in a tetrahedral conformation, with one of the orbitals having a single unpaired electron and the other three forming a trigonal pyramid. The ion then relaxes to its more favorable trigonal planar form, with release of about 30 kcal/mol of energy—that goes into vibrations and rotation of the ion.The carbocation then can interact with surrounding molecules in many reactions that cannot be achieved by other means. When formed within a rarefied gas, the carbocation and its reactions can be studied by mass spectrometry techniques. However the technique can be used also in condensed matter (liquids and solids). In liquid phase, the carbocation is initially formed in the same solvation state as the parent molecule, and some reactions may happen before the solvent shells around it have time to rearrange. In a crystalline solid, the cation is formed in the same crystalline site; and the nature, position, and orientation of the other reagent(s) are strictly constrained.
-Whereas the carbon–helium-ion bond breaks spontaneously and immediately to yield a carbocation, bonds of other elements to helium are more stable. For example, molecular tritium T2 or tritium-hydrogen HT. On decay, these form a stable helium hydride ion [HeH]+ (respectively [3HeT]+ or [3HeH]+), which is stable enough to persist. This cation is claimed to be the strongest acid known, and will protonate any other molecule it comes in contact with. This is another route to creating cations that are not obtainable in other ways. In particular [HeH]+ (or [HeT]+) will protonate methane CH4 to the carbonium ion [CH5]+ (or [CH4T]+).Other structures that are expected to be stable when formed by beta-decay of tritium precursors include 3HeLi+, B2H53He+, and BeH3He+ according to theoretical calculations.
-The effect of hyperconjugation is strongly stabilizing for carbocations: hyperconjugation with alkyl substituents is often as stabilizing or even more so than conjugation with a π system. Although conjugation to unsaturated groups results in significant stabilization by the mesomeric effect (resonance), the benefit is partially offset by the presence of a more electronegative sp2 or sp carbon next to the carbocationic center. Thus, as reflected by hydride ion affinities, a secondary carbocation is more stabilized than the allyl cation, while a tertiary carbocation is more stabilized than the benzyl cation — results that may seem counterintuitive on first glance.
-A carbocation may be stabilized by resonance by a carbon–carbon double bond or by the lone pair of a heteroatom adjacent to the ionized carbon. In order for a carbocation to be resonance-stabilized, the molecular orbital of the donating group must have the proper symmetry, orientation, and energy level to interact with the empty 2p orbital of the carbocation. Such cations as allyl cation CH2=CH−CH+2 and benzyl cation C6H5−CH+2 are more stable than most other carbocations due to donation of electron density from π systems to the cationic center. Furthermore, carbocations present in aromatic molecules are especially stabilized, largely due to the delocalized π electrons characteristic of aromatic rings. Molecules that can form allyl or benzyl carbocations are especially reactive. These carbocations where the C+ is adjacent to another carbon atom that has a double or triple bond have extra stability because of the overlap of the empty p orbital of the carbocation with the p orbitals of the π bond. This overlap of the orbitals allows the positive charge to be dispersed and electron density from the π system to be shared with the electron-deficient center, resulting in stabilization. The doubly- and triply-benzylic carbocations, diphenylcarbenium and triphenylcarbenium (trityl) cation, are particularly stable. For the same reasons, the partial p character of strained C–C bonds in cyclopropyl groups also allows for donation of electron density and stabilizes the cyclopropylmethyl (cyclopropylcarbinyl) cation.
 A. The carbocation remains positively charged, trapped in the solid.
 B. The carbocation undergoes spontaneous bond breaking, yielding a carbon-helium ion.
 C. The carbocation forms a bond with helium, becoming a stable compound.
 D. The carbocation undergoes decay, forming a negatively charged ion.
 E. The carbocation gains an electron from surrounding molecules, becoming an electrically neutral radical. "
What is the main focus of the Environmental Science Center at Qatar University?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the main focus of the Environmental Science Center at Qatar University?
 Context: -Environmental science is an interdisciplinary academic field that integrates physics, biology, and geography (including ecology, chemistry, plant science, zoology, mineralogy, oceanography, limnology, soil science, geology and physical geography, and atmospheric science) to the study of the environment, and the solution of environmental problems. Environmental science emerged from the fields of natural history and medicine during the Enlightenment. Today it provides an integrated, quantitative, and interdisciplinary approach to the study of environmental systems.Environmental studies incorporates more of the social sciences for understanding human relationships, perceptions and policies towards the environment. Environmental engineering focuses on design and technology for improving environmental quality in every aspect.
-This is a glossary of environmental science.
Environmental science is the study of interactions among physical, chemical, and biological components of the environment. Environmental science provides an integrated, quantitative, and interdisciplinary approach to the study of environmental systems.
-Atmospheric sciences Atmospheric sciences focus on the Earth's atmosphere, with an emphasis upon its interrelation to other systems. Atmospheric sciences can include studies of meteorology, greenhouse gas phenomena, atmospheric dispersion modeling of airborne contaminants, sound propagation phenomena related to noise pollution, and even light pollution.
Taking the example of the global warming phenomena, physicists create computer models of atmospheric circulation and infrared radiation transmission, chemists examine the inventory of atmospheric chemicals and their reactions, biologists analyze the plant and animal contributions to carbon dioxide fluxes, and specialists such as meteorologists and oceanographers add additional breadth in understanding the atmospheric dynamics.
-Environmental studies is a broader academic discipline that is the systematic study of the interaction of humans with their environment. It is a broad field of study that includes: The natural environment Built environments Social environmentsEnvironmentalism is a broad social and philosophical movement that, in a large part, seeks to minimize and compensate for the negative effect of human activity on the biophysical environment. The issues of concern for environmentalists usually relate to the natural environment with the more important ones being climate change, species extinction, pollution, and old growth forest loss.
-Environmental science is the study of the interactions within the biophysical environment. Part of this scientific discipline is the investigation of the effect of human activity on the environment.
Ecology, a sub-discipline of biology and a part of environmental sciences, is often mistaken as a study of human-induced effects on the environment.
 A. Environmental studies, with a main focus on marine science, atmospheric and political sciences.
 B. Environmental studies, with a main focus on marine science, atmospheric and physical sciences.
 C. Environmental studies, with a main focus on marine science, atmospheric and social sciences.
 D. Environmental studies, with a main focus on marine science, atmospheric and biological sciences.
 E. Environmental studies, with a main focus on space science, atmospheric and biological sciences. "
What is the purpose of obtaining surgical resection specimens?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the purpose of obtaining surgical resection specimens?
 Context: -Surgical resection specimens are obtained by the therapeutic surgical removal of an entire diseased area or organ (and occasionally multiple organs). These procedures are often intended as definitive surgical treatment of a disease in which the diagnosis is already known or strongly suspected. However, pathological analysis of these specimens is critically important in confirming the previous diagnosis, staging the extent of malignant disease, establishing whether or not the entire diseased area was removed (a process called ""determination of the surgical margin"", often using frozen section), identifying the presence of unsuspected concurrent diseases, and providing information for postoperative treatment, such as adjuvant chemotherapy in the case of cancer.
-== Need == In medicine, a laboratory specimen is a biological specimen of a medical patient's tissue, fluids, or other material used for laboratory analysis to assist in differential diagnosis or staging of a disease process. For example, to detect breast cancer, the breast tissue is biopsied, and the extracted specimen is sent to a lab for analysis and testing. This method of testing often yields extremely high levels of accuracy, with a reported 1-2% of cases having incorrect biopsy resultsGeneral areas for cellular tissue extraction:  Bone marrow aspiration Cardiac Core Endometrial biopsy Endoscopic biopsy Excisional and incisional Fine-needle aspiration Lymph node 
-The practice of surgical pathology allows for definitive diagnosis of disease (or lack thereof) in any case where tissue is surgically removed from a patient. This is usually performed by a combination of gross (i.e., macroscopic) and histologic (i.e., microscopic) examination of the tissue, and may involve evaluations of molecular properties of the tissue by immunohistochemistry or other laboratory tests.
-Surgical pathology Surgical pathology is one of the primary areas of practice for most anatomical pathologists. Surgical pathology involves the gross and microscopic examination of surgical specimens, as well as biopsies submitted by surgeons and non-surgeons such as general internists, medical subspecialists, dermatologists, and interventional radiologists. Often an excised tissue sample is the best and most definitive evidence of disease (or lack thereof) in cases where tissue is surgically removed from a patient. These determinations are usually accomplished by a combination of gross (i.e., macroscopic) and histologic (i.e., microscopic) examination of the tissue, and may involve evaluations of molecular properties of the tissue by immunohistochemistry or other laboratory tests.There are two major types of specimens submitted for surgical pathology analysis: biopsies and surgical resections. A biopsy is a small piece of tissue removed primarily for surgical pathology analysis, most often in order to render a definitive diagnosis. Types of biopsies include core biopsies, which are obtained through the use of large-bore needles, sometimes under the guidance of radiological techniques such as ultrasound, CT scan, or magnetic resonance imaging. Incisional biopsies are obtained through diagnostic surgical procedures that remove part of a suspicious lesion, whereas excisional biopsies remove the entire lesion, and are similar to therapeutic surgical resections. Excisional biopsies of skin lesions and gastrointestinal polyps are very common. The pathologist's interpretation of a biopsy is critical to establishing the diagnosis of a benign or malignant tumor, and can differentiate between different types and grades of cancer, as well as determining the activity of specific molecular pathways in the tumor. Surgical resection specimens are obtained by the therapeutic surgical removal of an entire diseased area or organ (and occasionally multiple organs). These procedures are often intended as definitive surgical treatment of a disease in which the diagnosis is already known or strongly suspected, but pathological analysis of these specimens remains important in confirming the previous diagnosis.
-After the biopsy is performed, the sample of tissue that was removed from the patient is sent to the pathology laboratory. A pathologist specializes in diagnosing diseases (such as cancer) by examining tissue under a microscope. When the laboratory (see Histology) receives the biopsy sample, the tissue is processed and an extremely thin slice of tissue is removed from the sample and attached to a glass slide. Any remaining tissue is saved for use in later studies, if required.The slide with the tissue attached is treated with dyes that stain the tissue, which allows the individual cells in the tissue to be seen more clearly. The slide is then given to the pathologist, who examines the tissue under a microscope, looking for any abnormal findings. The pathologist then prepares a report that lists any abnormal or important findings from the biopsy. This report is sent to the surgeon who originally performed the biopsy on the patient.
 A. To remove an entire diseased area or organ for definitive surgical treatment of a disease, with pathological analysis of the specimen used to confirm the diagnosis.
 B. To perform visual and microscopic tests on tissue samples using automated analysers and cultures.
 C. To work in close collaboration with medical technologists and hospital administrations.
 D. To administer a variety of tests of the biophysical properties of tissue samples.
 E. To obtain bodily fluids such as blood and urine for laboratory analysis of disease diagnosis. "
What is the function of mammary glands in mammals?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the function of mammary glands in mammals?
 Context: -A mammary gland is an exocrine gland in humans and other mammals that produces milk to feed young offspring. Mammals get their name from the Latin word mamma, ""breast"". The mammary glands are arranged in organs such as the breasts in primates (for example, humans and chimpanzees), the udder in ruminants (for example, cows, goats, sheep, and deer), and the dugs of other animals (for example, dogs and cats). Lactorrhea, the occasional production of milk by the glands, can occur in any mammal, but in most mammals, lactation, the production of enough milk for nursing, occurs only in phenotypic females who have gestated in recent months or years. It is directed by hormonal guidance from sex steroids. In a few mammalian species, male lactation can occur. With humans, male lactation can occur only under specific circumstances.
-The primary function of the breasts, as mammary glands, is the nourishing of an infant with breast milk. Milk is produced in milk-secreting cells in the alveoli. When the breasts are stimulated by the suckling of her baby, the mother's brain secretes oxytocin. High levels of oxytocin trigger the contraction of muscle cells surrounding the alveoli, causing milk to flow along the ducts that connect the alveoli to the nipple.Full-term newborns have an instinct and a need to suck on a nipple, and breastfed babies nurse for both nutrition and for comfort. Breast milk provides all necessary nutrients for the first six months of life, and then remains an important source of nutrition, alongside solid foods, until at least one or two years of age.
-In females, it serves as the mammary gland, which produces and secretes milk to feed infants. Subcutaneous fat covers and envelops a network of ducts that converge on the nipple, and these tissues give the breast its size and shape. At the ends of the ducts are lobules, or clusters of alveoli, where milk is produced and stored in response to hormonal signals. During pregnancy, the breast responds to a complex interaction of hormones, including estrogens, progesterone, and prolactin, that mediate the completion of its development, namely lobuloalveolar maturation, in preparation of lactation and breastfeeding.
-Mammals are divided into 3 groups: prototherians, metatherians, and eutherians. In the case of prototherians, both males and females have functional mammary glands, but their mammary glands are without nipples. These mammary glands are modified sebaceous glands. Concerning metatherians and eutherians, only females have functional mammary glands. Their mammary glands can be termed as breasts or udders. In the case of breasts, each mammary gland has its own nipple (e.g., human mammary glands). In the case of udders, pairs of mammary glands comprise a single mass, with more than one nipple (or teat) hanging from it. For instance, cows and buffalo each have one udder with four teats, whereas sheep and goats each have two teats protruding from the udder. These mammary glands are modified sweat glands.
-The chief function of a lactation is to provide nutrition and immune protection to the young after birth. Due to lactation, the mother-young pair can survive even if food is scarce or too hard for the young to attain, expanding the environmental conditions the species can withstand. The costly investment of energy and resources into milk is outweighed by the benefit to offspring survival. In almost all mammals, lactation induces a period of infertility (in humans, lactational amenorrhea), which serves to provide the optimal birth spacing for survival of the offspring.
 A. Mammary glands produce milk to feed the young.
 B. Mammary glands help mammals draw air into the lungs.
 C. Mammary glands help mammals breathe with lungs.
 D. Mammary glands excrete nitrogenous waste as urea.
 E. Mammary glands separate oxygenated and deoxygenated blood in the mammalian heart. "
What is the relationship between interstellar and cometary chemistry?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the relationship between interstellar and cometary chemistry?
 Context: -Such IR observations have determined that in dense clouds (where there are enough particles to attenuate the destructive UV radiation) thin ice layers coat the microscopic particles, permitting some low-temperature chemistry to occur. Since dihydrogen is by far the most abundant molecule in the universe, the initial chemistry of these ices is determined by the chemistry of the hydrogen. If the hydrogen is atomic, then the H atoms react with available O, C and N atoms, producing ""reduced"" species like H2O, CH4, and NH3. However, if the hydrogen is molecular and thus not reactive, this permits the heavier atoms to react or remain bonded together, producing CO, CO2, CN, etc. These mixed-molecular ices are exposed to ultraviolet radiation and cosmic rays, which results in complex radiation-driven chemistry. Lab experiments on the photochemistry of simple interstellar ices have produced amino acids. The similarity between interstellar and cometary ices (as well as comparisons of gas phase compounds) have been invoked as indicators of a connection between interstellar and cometary chemistry. This is somewhat supported by the results of the analysis of the organics from the comet samples returned by the Stardust mission but the minerals also indicated a surprising contribution from high-temperature chemistry in the solar nebula.
-Chemistry in cometary comae The chemical composition of comets should reflect both the conditions in the outer solar nebula some 4.5 × 109 ayr, and the nature of the natal interstellar cloud from which the Solar System was formed. While comets retain a strong signature of their ultimate interstellar origins, significant processing must have occurred in the protosolar nebula. Early models of coma chemistry showed that reactions can occur rapidly in the inner coma, where the most important reactions are proton transfer reactions. Such reactions can potentially cycle deuterium between the different coma molecules, altering the initial D/H ratios released from the nuclear ice, and necessitating the construction of accurate models of cometary deuterium chemistry, so that gas-phase coma observations can be safely extrapolated to give nuclear D/H ratios.
-Research is progressing on the way in which interstellar and circumstellar molecules form and interact, e.g. by including non-trivial quantum mechanical phenomena for synthesis pathways on interstellar particles. This research could have a profound impact on our understanding of the suite of molecules that were present in the molecular cloud when our solar system formed, which contributed to the rich carbon chemistry of comets and asteroids and hence the meteorites and interstellar dust particles which fall to the Earth by the ton every day.
-Interstellar molecules are formed by chemical reactions within very sparse interstellar or circumstellar clouds of dust and gas. Usually this occurs when a molecule becomes ionised, often as the result of an interaction with cosmic rays. This positively charged molecule then draws in a nearby reactant by electrostatic attraction of the neutral molecule's electrons. Molecules can also be generated by reactions between neutral atoms and molecules, although this process is generally slower. The dust plays a critical role of shielding the molecules from the ionizing effect of ultraviolet radiation emitted by stars. The Murchison meteorite contains the organic molecules uracil and xanthine, which must therefore already have been present in the early Solar System, where they could have played a role in the origin of life.Nitriles, key molecular precursors of the RNA World scenario, are among the most abundant chemical families in the universe and have been found in molecular clouds in the center of the Milky Way, protostars of different masses, meteorites and comets, and also in the atmosphere of Titan, the largest moon of Saturn.Evidence for the extraterrestrial creation of organic molecules includes both their discovery in various contexts in space, and their laboratory synthesis under extraterrestrial conditions: 
-PAHs, subjected to interstellar medium (ISM) conditions, are transformed, through hydrogenation, oxygenation, and hydroxylation, to more complex organic compounds—""a step along the path toward amino acids and nucleotides, the raw materials of proteins and DNA, respectively"". Further, as a result of these transformations, the PAHs lose their spectroscopic signature which could be one of the reasons ""for the lack of PAH detection in interstellar ice grains, particularly the outer regions of cold, dense clouds or the upper molecular layers of protoplanetary disks.""Low-temperature chemical pathways from simple organic compounds to complex PAHs are of interest. Such chemical pathways may help explain the presence of PAHs in the low-temperature atmosphere of Saturn's moon Titan, and may be significant pathways, in terms of the PAH world hypothesis, in producing precursors to biochemicals related to life as we know it.
 A. Cometary chemistry is responsible for the formation of interstellar molecules, but there is no direct connection between the two.
 B. Interstellar and cometary chemistry are the same thing, just with different names.
 C. There is a possible connection between interstellar and cometary chemistry, as indicated by the similarity between interstellar and cometary ices and the analysis of organics from comet samples returned by the Stardust mission.
 D. There is no relationship between interstellar and cometary chemistry, as they are two completely different phenomena.
 E. Interstellar chemistry is responsible for the formation of comets, but there is no direct connection between the two. "
What is the reason for recycling rare metals according to the United Nations?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the reason for recycling rare metals according to the United Nations?
 Context: -Recycling Demand for metals is closely linked to economic growth given their use in infrastructure, construction, manufacturing, and consumer goods. During the 20th century, the variety of metals used in society grew rapidly. Today, the development of major nations, such as China and India, and technological advances, are fuelling ever more demand. The result is that mining activities are expanding, and more and more of the world's metal stocks are above ground in use, rather than below ground as unused reserves. An example is the in-use stock of copper. Between 1932 and 1999, copper in use in the U.S. rose from 73 g to 238 g per person.Metals are inherently recyclable, so in principle, can be used over and over again, minimizing these negative environmental impacts and saving energy. For example, 95% of the energy used to make aluminum from bauxite ore is saved by using recycled material.Globally, metal recycling is generally low. In 2010, the International Resource Panel, hosted by the United Nations Environment Programme published reports on metal stocks that exist within society and their recycling rates. The authors of the report observed that the metal stocks in society can serve as huge mines above ground. They warned that the recycling rates of some rare metals used in applications such as mobile phones, battery packs for hybrid cars and fuel cells are so low that unless future end-of-life recycling rates are dramatically stepped up these critical metals will become unavailable for use in modern technology.
-During the 20th century, the variety of metals used in society grew rapidly. Today, the development of major nations such as China and India and advances in technologies are fueling an ever-greater demand. The result is that metal mining activities are expanding and more and more of the world's metal stocks are above ground in use rather than below ground as unused reserves. An example is the in-use stock of copper. Between 1932 and 1999, copper in use in the US rose from 73 kilograms (161 lb) to 238 kilograms (525 lb) per person.95% of the energy used to make aluminium from bauxite ore is saved by using recycled material. However, levels of metals recycling are generally low. In 2010, the International Resource Panel, hosted by the United Nations Environment Programme (UNEP), published reports on metal stocks that exist within society and their recycling rates.The report's authors observed that the metal stocks in society can serve as huge mines above ground. However, they warned that the recycling rates of some rare metals used in applications such as mobile phones, battery packs for hybrid cars, and fuel cells are so low that unless future end-of-life recycling rates are dramatically stepped up these critical metals will become unavailable for use in modern technology.As recycling rates are low and so much metal has already been extracted, some landfills now contain higher concentrations of metal than mines themselves. This is especially true of aluminum, used in cans, and precious metals, found in discarded electronics. Furthermore, waste after 15 years has still not broken down, so less processing would be required when compared to mining ores. A study undertaken by Cranfield University has found £360 million of metals could be mined from just four landfill sites. There is also up to 20 MJ/kg of energy in waste, potentially making the re-extraction more profitable. However, although the first landfill mine opened in Tel Aviv, Israel in 1953, little work has followed due to the abundance of accessible ores.
-The USGS reported a current total reserve base of copper in potentially recoverable ores of 1.6 billion tonnes as of 2005, of which 950 million tonnes were considered economically recoverable. A 2013 global assessment identified ""455 known deposits (with well-defined identified resources) that contain about 1.8 billion metric tons of copper"", and predicted ""a mean of 812 undiscovered deposits within the uppermost kilometer of the earth's surface"" containing another 3.1 billion metric tons of copper ""which represents about 180 times 2012 global copper production from all types of copper deposits."" Known resources Recycling In the US, more copper is recovered and put back into service from recycled material than is derived from newly mined ore. Copper's recycle value is so great that premium-grade scrap normally has at least 95% of the value of primary metal from newly mined ore. In Europe, about 50% of copper demand comes from recycling (as of 2016).As of 2011, recycled copper provided 35% of total worldwide copper usage.
-In 2007, the OECD estimated 670 years of economically recoverable uranium in total conventional resources and phosphate ores assuming the then-current use rate.Light water reactors make relatively inefficient use of nuclear fuel, mostly using only the very rare uranium-235 isotope. Nuclear reprocessing can make this waste reusable, and newer reactors also achieve a more efficient use of the available resources than older ones. With a pure fast reactor fuel cycle with a burn up of all the uranium and actinides (which presently make up the most hazardous substances in nuclear waste), there is an estimated 160,000 years worth of uranium in total conventional resources and phosphate ore at the price of 60–100 US$/kg. However, reprocessing is expensive, possibly dangerous and can be used to manufacture nuclear weapons. One analysis found that for uranium prices could increase by two orders of magnitudes between 2035 and 2100 and that there could be a shortage near the end of the century. A 2017 study by researchers from MIT and WHOI found that ""at the current consumption rate, global conventional reserves of terrestrial uranium (approximately 7.6 million tonnes) could be depleted in a little over a century"". Limited uranium-235 supply may inhibit substantial expansion with the current nuclear technology. While various ways to reduce dependence on such resources are being explored, new nuclear technologies are considered to not be available in time for climate change mitigation purposes or competition with alternatives of renewables in addition to being more expensive and require costly research and development. A study found it to be uncertain whether identified resources will be developed quickly enough to provide uninterrupted fuel supply to expanded nuclear facilities and various forms of mining may be challenged by ecological barriers, costs, and land requirements. Researchers also report considerable import dependence of nuclear energy.Unconventional uranium resources also exist. Uranium is naturally present in seawater at a concentration of about 3 micrograms per liter, with 4.4 billion tons of uranium considered present in seawater at any time.
-Recycling steel saves energy and natural resources. The steel industry saves enough energy to power about 18 million households for a year, on a yearly basis. Recycling metal also uses about 74 percent less energy than making metal. Thus, recyclers of end-of-life vehicles save an estimated 85 million barrels of oil annually that would have been used in the manufacturing of other parts. Likewise, car recycling keeps 11 million tons of steel and 800,000 non-ferrous metals out of landfills and back in consumer use.
 A. The demand for rare metals will quickly exceed the consumed tonnage in 2013, but recycling rare metals with a worldwide production higher than 100 000 t/year is a good way to conserve natural resources and energy.
 B. The demand for rare metals will decrease in 2013, and recycling rare metals with a worldwide production lower than 100 000 t/year is a good way to conserve natural resources and energy.
 C. The demand for rare metals will quickly exceed the consumed tonnage in 2013, but recycling rare metals with a worldwide production higher than 100 000 t/year is not a good way to conserve natural resources and energy.
 D. The demand for rare metals will quickly exceed the consumed tonnage in 2013, but recycling rare metals with a worldwide production lower than 100 000 t/year is not a good way to conserve natural resources and energy.
 E. The demand for rare metals will quickly exceed the consumed tonnage in 2013, and recycling rare metals with a worldwide production lower than 100 000 t/year is urgent and priority should be placed on it in order to conserve natural resources and energy. "
What is radiometric dating?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is radiometric dating?
 Context: -Radiometric dating measures the steady decay of radioactive elements in an object to determine its age. It is used to calculate dates for the older part of the planet's geological record. The theory is very complicated but, in essence, the radioactive elements within an object decay to form isotopes of each chemical element. Isotopes are atoms of the element that differ in mass but share the same general properties. Geologists are most interested in the decay of isotopes carbon-14 (into nitrogen-14) and potassium-40 (into argon-40). Carbon-14 aka radiocarbon dating works for organic materials that are less than about 50,000 years old. For older periods, the potassium-argon dating process is more accurate.
-Radiometric dating, radioactive dating or radioisotope dating is a technique which is used to date materials such as rocks or carbon, in which trace radioactive impurities were selectively incorporated when they were formed. The method compares the abundance of a naturally occurring radioactive isotope within the material to the abundance of its decay products, which form at a known constant rate of decay. The use of radiometric dating was first published in 1907 by Bertram Boltwood and is now the principal source of information about the absolute age of rocks and other geological features, including the age of fossilized life forms or the age of Earth itself, and can also be used to date a wide range of natural and man-made materials.
-Radiometric dating is how geologist determine the age of a rock. In a closed system, the amount of radiogenic isotopes present in a sample is a direct function of time and the decay rate of the mineral. Therefore, to find the age of a sample, geologists find the ratio of daughter isotopes to remaining parent isotopes present in the mineral through different methods, such as mass spectrometry. From the known parent isotopes and the decay constant, we can then determine the age. Different ions can be analyzed for this and are called different dating.
-Together with stratigraphic principles, radiometric dating methods are used in geochronology to establish the geologic time scale. Among the best-known techniques are radiocarbon dating, potassium–argon dating and uranium–lead dating. By allowing the establishment of geological timescales, it provides a significant source of information about the ages of fossils and the deduced rates of evolutionary change. Radiometric dating is also used to date archaeological materials, including ancient artifacts.
-Radiometric dating is based on the known and constant rate of decay of radioactive isotopes into their radiogenic daughter isotopes. Particular isotopes are suitable for different applications due to the types of atoms present in the mineral or other material and its approximate age. For example, techniques based on isotopes with half-lives in the thousands of years, such as carbon-14, cannot be used to date materials that have ages on the order of billions of years, as the detectable amounts of the radioactive atoms and their decayed daughter isotopes will be too small to measure within the uncertainty of the instruments.
 A. Radiometric dating is a method of measuring geological time using geological sedimentation, discovered in the early 20th century.
 B. Radiometric dating is a method of measuring geological time using radioactive decay, discovered in the early 20th century.
 C. Radiometric dating is a method of measuring geological time using the position of rocks, discovered in the early 20th century.
 D. Radiometric dating is a method of measuring geological time using the age of fossils, discovered in the early 20th century.
 E. Radiometric dating is a method of measuring geological time using the cooling of the earth, discovered in the early 20th century. "
What is the role of methane in Fischer-Tropsch processes?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the role of methane in Fischer-Tropsch processes?
 Context: -The Fischer–Tropsch process is a collection of chemical reactions that converts a mixture of carbon monoxide and hydrogen, known as syngas, into liquid hydrocarbons. These reactions occur in the presence of metal catalysts, typically at temperatures of 150–300 °C (302–572 °F) and pressures of one to several tens of atmospheres. The Fischer–Tropsch process is an important reaction in both coal liquefaction and gas to liquids technology for producing liquid hydrocarbons.In the usual implementation, carbon monoxide and hydrogen, the feedstocks for FT, are produced from coal, natural gas, or biomass in a process known as gasification. The process then converts these gases into synthetic lubrication oil and synthetic fuel. This process has received intermittent attention as a source of low-sulfur diesel fuel and to address the supply or cost of petroleum-derived hydrocarbons. Fischer-Tropsch process is discussed as a step of producing carbon-neutral liquid hydrocarbon fuels from CO2 and hydrogen.The process was first developed by Franz Fischer and Hans Tropsch at the Kaiser Wilhelm Institute for Coal Research in Mülheim an der Ruhr, Germany, in 1925.
-Carbon monoxide and methanol are important chemical feedstocks. CO is utilized by myriad carbonylation reactions. Together with hydrogen, it is the feed for the Fischer–Tropsch process, which affords liquid fuels. Methanol is the precursor to acetic acid, dimethyl ether, formaldehyde, and many methyl compounds (esters, amines, halides). A larger scale application is methanol to olefins, which produces ethylene and propylene.In contrast to the situation for carbon monoxide and methanol, methane and carbon dioxide have limited uses as feedstocks to chemicals and fuels. This disparity contrasts with the relative abundance of methane and carbon dioxide. Methane is often partially converted to carbon monoxide for utilization in Fischer-Tropsch processes. Of interest for upgrading methane is its oxidative coupling: 2CH4 + O2 → C2H4 + 2H2OConversion of carbon dioxide to unsaturated hydrocarbons via electrochemical reduction is a hopeful avenue of research, but no stable and economic technology yet has been developed.
-Fischer–Tropsch process The Fischer–Tropsch process is used to produce synfuels from gasified biomass. Carbonaceous material is gasified and the gas is processed to make purified syngas (a mixture of carbon monoxide and hydrogen). The Fischer–Tropsch polymerizes syngas into diesel-range hydrocarbons. While biodiesel and bio-ethanol production so far only use parts of a plant, i.e. oil, sugar, starch or cellulose, BtL production can gasify and utilize the entire plant.
-The Fischer–Tropsch process involves a series of chemical reactions that produce a variety of hydrocarbons, ideally having the formula (CnH2n+2). The more useful reactions produce alkanes as follows: (2n + 1) H2 + n CO → CnH2n+2 + n H2Owhere n is typically 10–20. The formation of methane (n = 1) is unwanted. Most of the alkanes produced tend to be straight-chain, suitable as diesel fuel. In addition to alkane formation, competing reactions give small amounts of alkenes, as well as alcohols and other oxygenated hydrocarbons.The reaction is a highly exothermic reaction due to a standard reaction enthalpy (ΔH) of −165 kJ/mol CO combined.
-In order to obtain the mixture of CO and H2 required for the Fischer–Tropsch process, methane (main component of natural gas) may be subjected to partial oxidation which yields a raw synthesis gas mixture of mostly carbon dioxide, carbon monoxide, hydrogen gas (and sometimes water and nitrogen). The ratio of carbon monoxide to hydrogen in the raw synthesis gas mixture can be adjusted e.g. using the water gas shift reaction. Removing impurities, particularly nitrogen, carbon dioxide and water, from the raw synthesis gas mixture yields pure synthesis gas (syngas).
 A. Methane is partially converted to carbon monoxide for utilization in Fischer-Tropsch processes.
 B. Methane is used as a catalyst in Fischer-Tropsch processes.
 C. Methane is not used in Fischer-Tropsch processes.
 D. Methane is fully converted to carbon monoxide for utilization in Fischer-Tropsch processes.
 E. Methane is a byproduct of Fischer-Tropsch processes. "
What is a phageome?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is a phageome?
 Context: -A phageome is a community of bacteriophages and their metagenomes localized in a particular environment, similar to a microbiome. The term was first used in an article by Modi et al in 2013 and has continued to be used in scientific articles that relate to bacteriophages and their metagenomes. A bacteriophage, or phage for short, is a virus that has the ability to infect bacteria and archaea, and can replicate inside of them. Phageome is a subcategory of virome, which is all of the viruses that are associated with a host or environment. Phages make up the majority of most viromes and are currently understood as being the most abundant organism. Oftentimes scientists will look only at a phageome instead of a virome while conducting research.
-Bacteriophages, often just called phages, are viruses that parasite bacteria and archaea. Marine phages parasite marine bacteria and archaea, such as cyanobacteria. They are a common and diverse group of viruses and are the most abundant biological entity in marine environments, because their hosts, bacteria, are typically the numerically dominant cellular life in the sea. Generally there are about 1 million to 10 million viruses in each mL of seawater, or about ten times more double-stranded DNA viruses than there are cellular organisms, although estimates of viral abundance in seawater can vary over a wide range. Tailed bacteriophages appear to dominate marine ecosystems in number and diversity of organisms. Bacteriophages belonging to the families Corticoviridae, Inoviridae and Microviridae are also known to infect diverse marine bacteria.
-Definitions Microbial communities have commonly been defined as the collection of microorganisms living together. More specifically, microbial communities are defined as multi-species assemblages, in which (micro) organisms interact with each other in a contiguous environment. In 1988, Whipps and colleagues working on the ecology of rhizosphere microorganisms provided the first definition of the term microbiome. They described the microbiome as a combination of the words micro and biome, naming a ""characteristic microbial community"" in a ""reasonably well-defined habitat which has distinct physio-chemical properties"" as their ""theatre of activity"". This definition represents a substantial advancement of the definition of a microbial community, as it defines a microbial community with distinct properties and functions and its interactions with its environment, resulting in the formation of specific ecological niches.However, many other microbiome definitions have been published in recent decades. By 2020 the most cited definition was by Lederberg, and described microbiomes within an ecological context as a community of commensal, symbiotic, and pathogenic microorganisms within a body space or other environment. Marchesi and Ravel focused in their definition on the genomes and microbial (and viral) gene expression patterns and proteomes in a given environment and its prevailing biotic and abiotic conditions. All these definitions imply that general concepts of macro-ecology could be easily applied to microbe-microbe as well as to microbe-host interactions. However, the extent to which these concepts, developed for macro-eukaryotes, can be applied to prokaryotes with their different lifestyles regarding dormancy, variation of phenotype, and horizontal gene transfer as well as to micro-eukaryotes that is not quite clear. This raises the challenge of considering an entirely novel body of conceptual ecology models and theory for microbiome ecology, particularly in relation to the diverse hierarchies of interactions of microbes with one another and with the host biotic and abiotic environments. Many current definitions fail to capture this complexity and describe the term microbiome as encompassing the genomes of microorganisms only.
-Viruses Metagenomic sequencing is particularly useful in the study of viral communities. As viruses lack a shared universal phylogenetic marker (as 16S RNA for bacteria and archaea, and 18S RNA for eukarya), the only way to access the genetic diversity of the viral community from an environmental sample is through metagenomics. Viral metagenomes (also called viromes) should thus provide more and more information about viral diversity and evolution. For example, a metagenomic pipeline called Giant Virus Finder showed the first evidence of existence of giant viruses in a saline desert and in Antarctic dry valleys.
-Bacteriophages are among the most common and diverse entities in the biosphere. Bacteriophages are ubiquitous viruses, found wherever bacteria exist. It is estimated there are more than 1031 bacteriophages on the planet, more than every other organism on Earth, including bacteria, combined. Viruses are the most abundant biological entity in the water column of the world's oceans, and the second largest component of biomass after prokaryotes, where up to 9x108 virions per millilitre have been found in microbial mats at the surface, and up to 70% of marine bacteria may be infected by phages.Phages have been used since the late 20th century as an alternative to antibiotics in the former Soviet Union and Central Europe, as well as in France. They are seen as a possible therapy against multi-drug-resistant strains of many bacteria (see phage therapy).Phages are known to interact with the immune system both indirectly via bacterial expression of phage-encoded proteins and directly by influencing innate immunity and bacterial clearance. Phage–host interactions are becoming increasingly important areas of research.
 A. A community of viruses and their metagenomes localized in a particular environment, similar to a microbiome.
 B. A community of bacteria and their metagenomes localized in a particular environment, similar to a microbiome.
 C. A community of bacteriophages and their metagenomes localized in a particular environment, similar to a microbiome.
 D. A community of fungi and their metagenomes localized in a particular environment, similar to a microbiome.
 E. A community of archaea and their metagenomes localized in a particular environment, similar to a microbiome. "
What is organography?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is organography?
 Context: -Organography (from Greek όργανο, organo, ""organ""; and -γραφή, -graphy) is the scientific description of the structure and function of the organs of living things.
-Organography as a scientific study starts with Aristotle, who considered the parts of plants as ""organs"" and began to consider the relationship between different organs and different functions. In the 17th century Joachim Jung, clearly articulated that plants are composed of different organ types such as root, stem and leaf, and he went on to define these organ types on the basis of form and position.
-Organ patterns Growth and division of plant cells together result in the growth of tissue, and specific tissue growth contributes to the development of plant organs.
-Plant morphology treats both the vegetative structures of plants, as well as the reproductive structures.
The vegetative (somatic) structures of vascular plants include two major organ systems: (1) a shoot system, composed of stems and leaves, and (2) a root system. These two systems are common to nearly all vascular plants, and provide a unifying theme for the study of plant morphology.
-The study of plant organs is covered in plant morphology. Organs of plants can be divided into vegetative and reproductive. Vegetative plant organs include roots, stems, and leaves. The reproductive organs are variable. In flowering plants, they are represented by the flower, seed and fruit. In conifers, the organ that bears the reproductive structures is called a cone. In other divisions (phyla) of plants, the reproductive organs are called strobili, in Lycopodiophyta, or simply gametophores in mosses. Common organ system designations in plants include the differentiation of shoot and root. All parts of the plant above ground (in non-epiphytes), including the functionally distinct leaf and flower organs, may be classified together as the shoot organ system.The vegetative organs are essential for maintaining the life of a plant. While there can be 11 organ systems in animals, there are far fewer in plants, where some perform the vital functions, such as photosynthesis, while the reproductive organs are essential in reproduction. However, if there is asexual vegetative reproduction, the vegetative organs are those that create the new generation of plants (see clonal colony).
 A. Organography is the study of the stem and root of plants.
 B. Organography is the scientific description of the structure and function of the organs of living things.
 C. Organography is the study of the development of organs from the ""growing points"" or apical meristems.
 D. Organography is the study of the commonality of development between foliage leaves and floral leaves.
 E. Organography is the study of the relationship between different organs and different functions in plants. "
What is the definition of anatomy?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the definition of anatomy?
 Context: -Anatomy (from Ancient Greek ἀνατομή (anatomḗ) 'dissection') is the branch of biology concerned with the study of the structure of organisms and their parts. Anatomy is a branch of natural science that deals with the structural organization of living things. It is an old science, having its beginnings in prehistoric times. Anatomy is inherently tied to developmental biology, embryology, comparative anatomy, evolutionary biology, and phylogeny, as these are the processes by which anatomy is generated, both over immediate and long-term timescales. Anatomy and physiology, which study the structure and function of organisms and their parts respectively, make a natural pair of related disciplines, and are often studied together. Human anatomy is one of the essential basic sciences that are applied in medicine.Anatomy is a complex and dynamic field that is constantly evolving as new discoveries are made. In recent years, there has been a significant increase in the use of advanced imaging techniques, such as MRI and CT scans, which allow for more detailed and accurate visualizations of the body's structures.
-The branch of biology dealing with the study of the bodies and their specific structural features called morphology. Anatomy is a branch of morphology that deals with the structure of the body at a level higher than tissue. Anatomy is closely related to histology, which studies the structure of tissues, as well as cytology, which studies the structure and function of the individual cells, from which the tissues and organs of the studied macroorganism are built. Taken together, anatomy, histology, cytology and embryology represent a morphology The study of functions and mechanisms in a body is physiology.
-Derived from the Greek ἀνατομή anatomē ""dissection"" (from ἀνατέμνω anatémnō ""I cut up, cut open"" from ἀνά aná ""up"", and τέμνω témnō ""I cut""), anatomy is the scientific study of the structure of organisms including their systems, organs and tissues. It includes the appearance and position of the various parts, the materials from which they are composed, and their relationships with other parts. Anatomy is quite distinct from physiology and biochemistry, which deal respectively with the functions of those parts and the chemical processes involved. For example, an anatomist is concerned with the shape, size, position, structure, blood supply and innervation of an organ such as the liver; while a physiologist is interested in the production of bile, the role of the liver in nutrition and the regulation of bodily functions.The discipline of anatomy can be subdivided into a number of branches, including gross or macroscopic anatomy and microscopic anatomy. Gross anatomy is the study of structures large enough to be seen with the naked eye, and also includes superficial anatomy or surface anatomy, the study by sight of the external body features. Microscopic anatomy is the study of structures on a microscopic scale, along with histology (the study of tissues), and embryology (the study of an organism in its immature condition). Regional anatomy is the study of the interrelationships of all of the structures in a specific body region, such as the abdomen. In contrast, systemic anatomy is the study of the structures that make up a discrete body system—that is, a group of structures that work together to perform a unique body function, such as the digestive system.Anatomy can be studied using both invasive and non-invasive methods with the goal of obtaining information about the structure and organization of organs and systems. Methods used include dissection, in which a body is opened and its organs studied, and endoscopy, in which a video camera-equipped instrument is inserted through a small incision in the body wall and used to explore the internal organs and other structures. Angiography using X-rays or magnetic resonance angiography are methods to visualize blood vessels.The term ""anatomy"" is commonly taken to refer to human anatomy. However, substantially similar structures and tissues are found throughout the rest of the animal kingdom, and the term also includes the anatomy of other animals. The term zootomy is also sometimes used to specifically refer to non-human animals. The structure and tissues of plants are of a dissimilar nature and they are studied in plant anatomy.
-Morphology is a branch of biology dealing with the study of the form and structure of organisms and their specific structural features.This includes aspects of the outward appearance (shape, structure, colour, pattern, size), i.e. external morphology (or eidonomy), as well as the form and structure of the internal parts like bones and organs, i.e. internal morphology (or anatomy). This is in contrast to physiology, which deals primarily with function. Morphology is a branch of life science dealing with the study of gross structure of an organism or taxon and its component parts.
-Comparative morphology is analysis of the patterns of the locus of structures within the body plan of an organism, and forms the basis of taxonomical categorization.
Functional morphology is the study of the relationship between the structure and function of morphological features.
Experimental morphology is the study of the effects of external factors upon the morphology of organisms under experimental conditions, such as the effect of genetic mutation.
Anatomy is a ""branch of morphology that deals with the structure of organisms"".
Molecular morphology is a rarely used term, usually referring to the superstructure of polymers such as fiber formation or to larger composite assemblies. The term is commonly not applied to the spatial structure of individual molecules.
Gross morphology refers to the collective structures of an organism as a whole as a general description of the form and structure of an organism, taking into account all of its structures without specifying an individual structure.
 A. Anatomy is the rarely used term that refers to the superstructure of polymers such as fiber formation or to larger composite assemblies.
 B. Anatomy is a branch of morphology that deals with the structure of organisms.
 C. Anatomy is the study of the effects of external factors upon the morphology of organisms under experimental conditions, such as the effect of genetic mutation.
 D. Anatomy is the analysis of the patterns of the locus of structures within the body plan of an organism, and forms the basis of taxonomical categorization.
 E. Anatomy is the study of the relationship between the structure and function of morphological features. "
What is a trophic level in an ecological pyramid?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is a trophic level in an ecological pyramid?
 Context: -Trophic levels A trophic level (from Greek troph, τροφή, trophē, meaning ""food"" or ""feeding"") is ""a group of organisms acquiring a considerable majority of its energy from the lower adjacent level (according to ecological pyramids) nearer the abiotic source."": 383  Links in food webs primarily connect feeding relations or trophism among species. Biodiversity within ecosystems can be organized into trophic pyramids, in which the vertical dimension represents feeding relations that become further removed from the base of the food chain up toward top predators, and the horizontal dimension represents the abundance or biomass at each level. When the relative abundance or biomass of each species is sorted into its respective trophic level, they naturally sort into a 'pyramid of numbers'.Species are broadly categorized as autotrophs (or primary producers), heterotrophs (or consumers), and Detritivores (or decomposers). Autotrophs are organisms that produce their own food (production is greater than respiration) by photosynthesis or chemosynthesis. Heterotrophs are organisms that must feed on others for nourishment and energy (respiration exceeds production). Heterotrophs can be further sub-divided into different functional groups, including primary consumers (strict herbivores), secondary consumers (carnivorous predators that feed exclusively on herbivores), and tertiary consumers (predators that feed on a mix of herbivores and predators). Omnivores do not fit neatly into a functional category because they eat both plant and animal tissues. It has been suggested that omnivores have a greater functional influence as predators because compared to herbivores, they are relatively inefficient at grazing.Trophic levels are part of the holistic or complex systems view of ecosystems. Each trophic level contains unrelated species that are grouped together because they share common ecological functions, giving a macroscopic view of the system. While the notion of trophic levels provides insight into energy flow and top-down control within food webs, it is troubled by the prevalence of omnivory in real ecosystems. This has led some ecologists to ""reiterate that the notion that species clearly aggregate into discrete, homogeneous trophic levels is fiction."": 815  Nonetheless, recent studies have shown that real trophic levels do exist, but ""above the herbivore trophic level, food webs are better characterized as a tangled web of omnivores."": 612 Keystone species A keystone species is a species that is connected to a disproportionately large number of other species in the food-web. Keystone species have lower levels of biomass in the trophic pyramid relative to the importance of their role. The many connections that a keystone species holds means that it maintains the organization and structure of entire communities. The loss of a keystone species results in a range of dramatic cascading effects (termed trophic cascades) that alters trophic dynamics, other food web connections, and can cause the extinction of other species. The term keystone species was coined by Robert Paine in 1969 and is a reference to the keystone architectural feature as the removal of a keystone species can result in a community collapse just as the removal of the keystone in an arch can result in the arch's loss of stability.Sea otters (Enhydra lutris) are commonly cited as an example of a keystone species because they limit the density of sea urchins that feed on kelp. If sea otters are removed from the system, the urchins graze until the kelp beds disappear, and this has a dramatic effect on community structure. Hunting of sea otters, for example, is thought to have led indirectly to the extinction of the Steller's sea cow (Hydrodamalis gigas). While the keystone species concept has been used extensively as a conservation tool, it has been criticized for being poorly defined from an operational stance. It is difficult to experimentally determine what species may hold a keystone role in each ecosystem. Furthermore, food web theory suggests that keystone species may not be common, so it is unclear how generally the keystone species model can be applied.
-In above ground food webs, energy moves from producers (plants) to primary consumers (herbivores) and then to secondary consumers (predators). The phrase, trophic level, refers to the different levels or steps in the energy pathway. In other words, the producers, consumers, and decomposers are the main trophic levels. This chain of energy transferring from one species to another can continue several more times, but eventually ends. At the end of the food chain, decomposers such as bacteria and fungi break down dead plant and animal material into simple nutrients.
-A pyramid of energy shows how much energy is retained in the form of new biomass from each trophic level, while a pyramid of biomass shows how much biomass (the amount of living or organic matter present in an organism) is present in the organisms. There is also a pyramid of numbers representing the number of individual organisms at each trophic level. Pyramids of energy are normally upright, but other pyramids can be inverted(pyramid of biomass for marine region) or take other shapes.(spindle shaped pyramid) Ecological pyramids begin with producers on the bottom (such as plants) and proceed through the various trophic levels (such as herbivores that eat plants, then carnivores that eat flesh, then omnivores that eat both plants and flesh, and so on). The highest level is the top of the food chain.
-Trophic level A species’ trophic level is their position in the food chain or web. At the bottom of the food web are autotrophs, also known as primary producer. Producers provide their own energy through photosynthesis or chemosynthesis, plants are primary producers. The next level is herbivores (primary consumers), these species feed on vegetation for their energy source. Herbivores are consumed by omnivores or carnivores. These species are secondary and tertiary consumers. Additional levels to the trophic scale come when smaller omnivores or carnivores are eaten by larger ones. At the top of the food web is the apex predator, this animal species is not consumed by any other in the community. Herbivores, omnivores and carnivores are all heterotrophs.A basic example of a food chain is; grass → rabbit → fox. Food chains become more complex when more species are present, often being food webs. Energy is passed up through trophic levels. Energy is lost at each level, due to ecological inefficiencies.The trophic level of an organism can change based on the other species present. For example, tuna can be an apex predator eating the smaller fish, such as mackerel. However, in a community where a shark species is present the shark becomes the apex predator, feeding on the tuna.Decomposers play a role in the trophic pyramid. They provide energy source and nutrients to the plant species in the community. Decomposers such as fungi and bacteria recycle energy back to the base of the food web by feeding on dead organisms from all trophic levels.
-Ecological pyramids In a pyramid of numbers, the number of consumers at each level decreases significantly, so that a single top consumer, (e.g., a polar bear or a human), will be supported by a much larger number of separate producers. There is usually a maximum of four or five links in a food chain, although food chains in aquatic ecosystems are more often longer than those on land. Eventually, all the energy in a food chain is dispersed as heat.Ecological pyramids place the primary producers at the base. They can depict different numerical properties of ecosystems, including numbers of individuals per unit of area, biomass (g/m2), and energy (k cal m−2 yr−1). The emergent pyramidal arrangement of trophic levels with amounts of energy transfer decreasing as species become further removed from the source of production is one of several patterns that is repeated amongst the planets ecosystems. The size of each level in the pyramid generally represents biomass, which can be measured as the dry weight of an organism. Autotrophs may have the highest global proportion of biomass, but they are closely rivaled or surpassed by microbes.Pyramid structure can vary across ecosystems and across time. In some instances biomass pyramids can be inverted. This pattern is often identified in aquatic and coral reef ecosystems. The pattern of biomass inversion is attributed to different sizes of producers. Aquatic communities are often dominated by producers that are smaller than the consumers that have high growth rates. Aquatic producers, such as planktonic algae or aquatic plants, lack the large accumulation of secondary growth as exists in the woody trees of terrestrial ecosystems. However, they are able to reproduce quickly enough to support a larger biomass of grazers. This inverts the pyramid. Primary consumers have longer lifespans and slower growth rates that accumulates more biomass than the producers they consume. Phytoplankton live just a few days, whereas the zooplankton eating the phytoplankton live for several weeks and the fish eating the zooplankton live for several consecutive years. Aquatic predators also tend to have a lower death rate than the smaller consumers, which contributes to the inverted pyramidal pattern. Population structure, migration rates, and environmental refuge for prey are other possible causes for pyramids with biomass inverted. Energy pyramids, however, will always have an upright pyramid shape if all sources of food energy are included and this is dictated by the second law of thermodynamics.
 A. A group of organisms that acquire most of their energy from the level above them in the pyramid.
 B. A group of organisms that acquire most of their energy from the abiotic sources in the ecosystem.
 C. A group of organisms that acquire most of their energy from the level below them in the pyramid.
 D. A group of organisms that acquire most of their energy from the same level in the pyramid.
 E. A group of organisms that do not acquire any energy from the ecosystem. "
What is a crossover experiment?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is a crossover experiment?
 Context: -In chemistry, a crossover experiment is a method used to study the mechanism of a chemical reaction. In a crossover experiment, two similar but distinguishable reactants simultaneously undergo a reaction as part of the same reaction mixture. The products formed will either correspond directly to one of the two reactants (non-crossover products) or will include components of both reactants (crossover products). The aim of a crossover experiment is to determine whether or not a reaction process involves a stage where the components of each reactant have an opportunity to exchange with each other.
-In medicine, a crossover study or crossover trial is a longitudinal study in which subjects receive a sequence of different treatments (or exposures). While crossover studies can be observational studies, many important crossover studies are controlled experiments, which are discussed in this article. Crossover designs are common for experiments in many scientific disciplines, for example psychology, pharmaceutical science, and medicine.
-The results of crossover experiments are often straightforward to analyze, making them one of the most useful and most frequently applied methods of mechanistic study. In organic chemistry, crossover experiments are most often used to distinguish between intramolecular and intermolecular reactions.Inorganic and organometallic chemists rely heavily on crossover experiments, and in particular isotopic labeling experiments, for support or contradiction of proposed mechanisms. When the mechanism being investigated is more complicated than an intra- or intermolecular substitution or rearrangement, crossover experiment design can itself become a challenging question. A well-designed crossover experiment can lead to conclusions about a mechanism that would otherwise be impossible to make. Many mechanistic studies include both crossover experiments and measurements of rate and kinetic isotope effects.
-Crossover experiments allow for experimental study of a reaction mechanism. Mechanistic studies are of interest to theoretical and experimental chemists for a variety of reasons including prediction of stereochemical outcomes, optimization of reaction conditions for rate and selectivity, and design of improved catalysts for better turnover number, robustness, etc. Since a mechanism cannot be directly observed or determined solely based on the reactants or products, mechanisms are challenging to study experimentally. Only a handful of experimental methods are capable of providing information about the mechanism of a reaction, including crossover experiments, studies of the kinetic isotope effect, and rate variations by substituent. The crossover experiment has the advantage of being conceptually straightforward and relatively easy to design, carry out, and interpret. In modern mechanistic studies, crossover experiments and KIE studies are commonly used in conjunction with computational methods.
-In designing a crossover experiment the first task is to propose possible mechanisms for the reaction being studied. Based on these possible mechanisms, the goal is to determine either a traditional crossover experiment or an isotope scrambling experiment that will enable the researcher to distinguish between the two or more possible mechanisms. Often many methods of mechanistic study will have to be employed to support or discount all of the mechanisms proposed. However, in some cases a crossover experiment alone will be able to distinguish between the main possibilities, for example in the case of intramolecular vs. intermolecular organic reaction mechanisms.
 A. An experiment that involves crossing over two different types of materials to create a new material.
 B. A type of experiment used to distinguish between different mechanisms proposed for a chemical reaction, such as intermolecular vs. intramolecular mechanisms.
 C. An experiment that involves crossing over two different types of organisms to create a hybrid.
 D. An experiment that involves crossing over two different types of cells to create a new cell.
 E. An experiment that involves crossing over two different chemicals to create a new substance. "
What is the role of IL-10 in the formation of Tr1 cells and tolerogenic DCs?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the role of IL-10 in the formation of Tr1 cells and tolerogenic DCs?
 Context: -During a tolerant state potential effector cells remain but are tightly regulated by induced antigen-specific CD4+ regulatory T cells (iTregs). Many subsets of iTregs play a part in this process, but CD4+CD25+FoxP3+ Tregs play a key role, because they have the ability to convert conventional T cells into iTregs directly by secretion of the suppressive cytokines TGF-β, IL-10 or IL-35, or indirectly via dendritic cells (DCs). Production of IL-10 induces the formation of another population of regulatory T cells called Tr1. Tr1 cells are dependent on IL-10 and TGF-β as well as Tregs, but differ from them by lacking expression of Foxp3. High IL-10 production is characteristic for Tr1 cells themselves and they also produce TGF-β. In the presence of IL-10 can be also induced tolerogenic DCs from monocytes, whose production of IL-10 is also important for Tr1 formation. These interactions lead to the production of enzymes such as IDO (indolamine 2,3-dioxygenase) that catabolize essential amino acids. This microenvironment with a lack of essential amino acids together with other signals results in mTOR (mammalian target of rapamycin) inhibition which, particularly in synergy with TGF-β, direct the induction of new FoxP3 (forkhead box protein 3) expressing Tregs.
-IL-27, together with TGF-β induces IL-10–producing regulatory T cells with Tr1-like properties cells. IL-27 alone can induce IL-10-producing Tr1 cells, but in the absence of TGF-β, the cells produce large quantities of both IFN-γ and IL-10. IL-6 and IL-21 also plays a role in differentiation as they regulate expression of transcription factors necessary for IL-10 production, which is believed to start up the differentiation itself later on.
-The suppressing and tolerance-inducing effect of Tr1 cells is mediated mainly by cytokines. The other mechanism as cell to cell contact, modulation of dendritic cells, metabolic disruption and cytolysis is however also available to them. In vivo Tr1 cells need to be activated, to be able to exert their regulatory effects.
-Studies have suggested a role for tolerogenic dendritic cells in the treatment of diseases like type 1 diabetes mellitus and multiple sclerosis.In animal models of Diabetes mellitus (NOD mice), GM-CSF induces resistance by increasing the frequency of regulatory T cells which can suppress T cell proliferation through their T-cell receptors. GM-CSF treated mice were found to have a semi-mature phenotype of dendritic cells which were inefficient at inducing antigen specific cytotoxic T cells compared to controls.In multiple sclerosis research, EAE mice were completely protected from symptoms when injected with dendritic cells matured with TNF-α and antigen specific peptide compared to controls. T regulatory cells of mice treated with TNF-α produced IL-10, a cytokine which is able to inhibit the Th1 response therefore protecting against the Th1 dependent autoimmune EAE.Mouse models of autoimmune thyroiditis showed that a semi-mature phenotype of dendritic cells is maintained after mouse thyroglobulin immunization in GM-CSF treated but not control mice. IL-10 produced by T regulatory cells was important in suppressing the mouse thyroglobulin specific T cell response and therefore protecting against Experimental autoimmune thyroiditis in mice.Phase I studies into the safety and efficacy of tolerogenic DC therapy in humans have demonstrated the appropriateness of the therapy for further research. Future research will consider the effectiveness of tolerogenic therapies in a number of planned clinical trials into autoimmune diseases.
-Tolerogenic DCs are essential in maintenance of central and peripheral tolerance through induction of T cell clonal deletion, T cell anergy and generation and activation of regulatory T (Treg) cells. For that reason, tolerogenic DCs are possible candidates for specific cellular therapy for treatment of allergic diseases, autoimmune diseases (e.g. type 1 diabetes, multiple sclerosis, rheumatoid arthritis) or transplant rejections.Tolerogenic DCs often display an immature or semi-mature phenotype with characteristically low expression of costimulatory (e.g. CD80, CD86) and MHC molecules on their surface. Tolerogenic DCs also produce different cytokines as mature DCs (e.g. anti-inflammatory cytokines interleukin (IL)-10, transforming growth factor-β (TGF-β)). Moreover, tolerogenic DCs may also express various inhibitory surface molecules (e.g. programmed cell death ligand (PDL)-1, PDL-2) or can modulate metabolic parameters and change T cell response. For example, tolerogenic DCs can release or induce enzymes such as indoleamine 2,3-dioxygenase (IDO) or heme oxygenase-1 (HO-1). IDO promotes the degradation of tryptophan to N-formylkynurenin leading to reduced T cell proliferation, whereas HO- 1 catalyzes degradation of hemoglobin resulting in production of monoxide and lower DC immunogenicity. Besides that, tolerogenic DCs also may produce retinoic acid (RA), which induces Treg differentiation.Human tolerogenic DCs may be induced by various immunosuppressive drugs or biomediators. Immunosuppressive drugs, e.g. corticosteroid dexamethasone, rapamycin, cyclosporine or acetylsalicylic acid, cause low expression of costimulatory molecules, reduced expression of MHC, higher expression of inhibitory molecules (e.g. PDL-1) or higher secretion of IL-10 or IDO. In addition, incubation with inhibitory cytokines IL-10 or TGF-β leads to generation of tolerogenic phenotype. Other mediators also affect generation of tolerogenic DC, e.g. vitamin D3, vitamin D2, hepatocyte growth factor or vasoactive intestinal peptide. The oldest and mostly used cytokine cocktail for in vitro DC generation is GM-CSF/IL-4.Tolerogenic DCs may be a potential candidate for specific immunotherapy and are studied for using them for treatment of inflammatory, autoimmune and allergic diseases and also in transplant medicine. Important and interesting feature of tolerogenic DCs is also the migratory capacity toward secondary lymph organs, leading to T-cell mediated immunosuppression. The first trial to transfer tolerogenic DCs to humans was undertaken by Ralph Steinman's group in 2001. Relating to the DC administration, various application have been used in humans in last years. Tolerogenic DCs have been injected e.g. intraperitoneally in patients with Crohn's disease, intradermally in diabetes and rheumatoid arthritis patients, subcutaneously in rheumatoid arthritis patients and via arthroscopic injections in joints of patient with rheumatoid and inflammatory arthritis.Therefore, it is necessary to test tolerogenic DCs for a stable phenotype to exclude a loss of the regulatory function and a switch to an immunostimulatory activity.
 A. IL-10 inhibits the formation of Tr1 cells and tolerogenic DCs, which are dependent on TGF-β and Tregs. Tr1 cells produce low levels of IL-10 and TGF-β, while tolerogenic DCs produce TGF-β that is important for Tr1 formation.
 B. IL-10 induces the formation of Tr1 cells and tolerogenic DCs, which are dependent on IL-10 and TGF-β, but differ from Tregs by lacking expression of Foxp3. Tr1 cells produce high levels of IL-10 and TGF-β, while tolerogenic DCs produce IL-10 that is important for Tr1 formation.
 C. IL-10 has no role in the formation of Tr1 cells and tolerogenic DCs. TGF-β and Tregs are the only factors involved in the formation of Tr1 cells and tolerogenic DCs.
 D. IL-10 induces the formation of Tr1 cells and tolerogenic DCs, which are dependent on IL-10 and Tregs, but differ from Tregs by expressing Foxp3. Tr1 cells produce low levels of IL-10 and TGF-β, while tolerogenic DCs produce IL-10 that is important for Tr1 formation.
 E. IL-10 induces the formation of Tregs, which are dependent on TGF-β and Foxp3. Tr1 cells and tolerogenic DCs are not involved in this process. "
"What is the reason behind the designation of Class L dwarfs, and what is their color and composition?","Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the reason behind the designation of Class L dwarfs, and what is their color and composition?
 Context: -Class L Class L dwarfs get their designation because they are cooler than M stars and L is the remaining letter alphabetically closest to M. Some of these objects have masses large enough to support hydrogen fusion and are therefore stars, but most are of substellar mass and are therefore brown dwarfs. They are a very dark red in color and brightest in infrared. Their atmosphere is cool enough to allow metal hydrides and alkali metals to be prominent in their spectra.Due to low surface gravity in giant stars, TiO- and VO-bearing condensates never form. Thus, L-type stars larger than dwarfs can never form in an isolated environment. However, it may be possible for these L-type supergiants to form through stellar collisions, an example of which is V838 Monocerotis while in the height of its luminous red nova eruption.
-Spectral class L The defining characteristic of spectral class M, the coolest type in the long-standing classical stellar sequence, is an optical spectrum dominated by absorption bands of titanium(II) oxide (TiO) and vanadium(II) oxide (VO) molecules. However, GD 165B, the cool companion to the white dwarf GD 165, had none of the hallmark TiO features of M dwarfs. The subsequent identification of many objects like GD 165B ultimately led to the definition of a new spectral class, the L dwarfs, defined in the red optical region of the spectrum not by metal-oxide absorption bands (TiO, VO), but by metal hydride emission bands (FeH, CrH, MgH, CaH) and prominent atomic lines of alkali metals (Na, K, Rb, Cs). As of 2013, over 900 L dwarfs have been identified, most by wide-field surveys: the Two Micron All Sky Survey (2MASS), the Deep Near Infrared Survey of the Southern Sky (DENIS), and the Sloan Digital Sky Survey (SDSS). This spectral class contains not only the brown dwarfs, because the coolest main-sequence stars above brown dwarfs (> 80 MJ) have the spectral class L2 to L6.
-Brown dwarfs and sub-stellar objects Protostars with masses less than roughly 0.08 M☉ (1.6×1029 kg) never reach temperatures high enough for nuclear fusion of hydrogen to begin. These are known as brown dwarfs. The International Astronomical Union defines brown dwarfs as stars massive enough to fuse deuterium at some point in their lives (13 Jupiter masses (MJ), 2.5 × 1028 kg, or 0.0125 M☉). Objects smaller than 13 MJ are classified as sub-brown dwarfs (but if they orbit around another stellar object they are classified as planets). Both types, deuterium-burning and not, shine dimly and fade away slowly, cooling gradually over hundreds of millions of years.
-Cool red and brown dwarf classes The new spectral types L, T, and Y were created to classify infrared spectra of cool stars. This includes both red dwarfs and brown dwarfs that are very faint in the visible spectrum.Brown dwarfs, stars that do not undergo hydrogen fusion, cool as they age and so progress to later spectral types. Brown dwarfs start their lives with M-type spectra and will cool through the L, T, and Y spectral classes, faster the less massive they are; the highest-mass brown dwarfs cannot have cooled to Y or even T dwarfs within the age of the universe. Because this leads to an unresolvable overlap between spectral types' effective temperature and luminosity for some masses and ages of different L-T-Y types, no distinct temperature or luminosity values can be given.
-Classification of brown dwarfs Spectral class M These are brown dwarfs with a spectral class of M5.5 or later; they are also called late-M dwarfs. These can be considered red dwarfs in the eyes of some scientists. Many brown dwarfs with spectral type M are young objects, such as Teide 1.
 A. Class L dwarfs are hotter than M stars and are designated L because L is the remaining letter alphabetically closest to M. They are bright blue in color and are brightest in ultraviolet. Their atmosphere is hot enough to allow metal hydrides and alkali metals to be prominent in their spectra. Some of these objects have masses large enough to support hydrogen fusion and are therefore stars, but most are of substellar mass and are therefore brown dwarfs.
 B. Class L dwarfs are cooler than M stars and are designated L because L is the remaining letter alphabetically closest to M. They are dark red in color and are brightest in infrared. Their atmosphere is cool enough to allow metal hydrides and alkali metals to be prominent in their spectra. Some of these objects have masses large enough to support hydrogen fusion and are therefore stars, but most are of substellar mass and are therefore brown dwarfs.
 C. Class L dwarfs are hotter than M stars and are designated L because L is the next letter alphabetically after M. They are dark red in color and are brightest in infrared. Their atmosphere is cool enough to allow metal hydrides and alkali metals to be prominent in their spectra. Some of these objects have masses large enough to support hydrogen fusion and are therefore stars, but most are of substellar mass and are therefore brown dwarfs.
 D. Class L dwarfs are cooler than M stars and are designated L because L is the next letter alphabetically after M. They are bright yellow in color and are brightest in visible light. Their atmosphere is hot enough to allow metal hydrides and alkali metals to be prominent in their spectra. Some of these objects have masses large enough to support hydrogen fusion and are therefore stars, but most are of substellar mass and are therefore brown dwarfs.
 E. Class L dwarfs are cooler than M stars and are designated L because L is the remaining letter alphabetically closest to M. They are bright green in color and are brightest in visible light. Their atmosphere is cool enough to allow metal hydrides and alkali metals to be prominent in their spectra. Some of these objects have masses small enough to support hydrogen fusion and are therefore stars, but most are of substellar mass and are therefore brown dwarfs. "
What was Isaac Newton's explanation for rectilinear propagation of light?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What was Isaac Newton's explanation for rectilinear propagation of light?
 Context: -Isaac Newton rejected the wave explanation of rectilinear propagation, believing that if light consisted of waves, it would ""bend and spread every way"" into the shadows. His corpuscular theory of light explained rectilinear propagation more simply, and it accounted for the ordinary laws of refraction and reflection, including TIR, on the hypothesis that the corpuscles of light were subject to a force acting perpendicular to the interface. In this model, for dense-to-rare incidence, the force was an attraction back towards the denser medium, and the critical angle was the angle of incidence at which the normal velocity of the approaching corpuscle was just enough to reach the far side of the force field; at more oblique incidence, the corpuscle would be turned back. Newton gave what amounts to a formula for the critical angle, albeit in words: ""as the Sines are which measure the Refraction, so is the Sine of Incidence at which the total Reflexion begins, to the Radius of the Circle"".Newton went beyond Huygens in two ways. First, not surprisingly, Newton pointed out the relationship between TIR and dispersion: when a beam of white light approaches a glass-to-air interface at increasing obliquity, the most strongly-refracted rays (violet) are the first to be ""taken out"" by ""total Reflexion"", followed by the less-refracted rays. Second, he observed that total reflection could be frustrated (as we now say) by laying together two prisms, one plane and the other slightly convex; and he explained this simply by noting that the corpuscles would be attracted not only to the first prism, but also to the second.In two other ways, however, Newton's system was less coherent. First, his explanation of partial reflection depended not only on the supposed forces of attraction between corpuscles and media, but also on the more nebulous hypothesis of ""Fits of easy Reflexion"" and ""Fits of easy Transmission"". Second, although his corpuscles could conceivably have ""sides"" or ""poles"", whose orientations could conceivably determine whether the corpuscles suffered ordinary or extraordinary refraction in ""Island-Crystal"", his geometric description of the extraordinary refraction was theoretically unsupported and empirically inaccurate.
-Wave theory of light Beginning in 1670 and progressing over three decades, Isaac Newton developed and championed his corpuscular theory, arguing that the perfectly straight lines of reflection demonstrated light's particle nature, as at that time no wave theory demonstrated travel in straight lines.: 19  He explained refraction by positing that particles of light accelerated laterally upon entering a denser medium. Around the same time, Newton's contemporaries Robert Hooke and Christiaan Huygens, and later Augustin-Jean Fresnel, mathematically refined the wave viewpoint, showing that if light traveled at different speeds in different media, refraction could be easily explained as the medium-dependent propagation of light waves. The resulting Huygens–Fresnel principle was extremely successful at reproducing light's behaviour and was consistent with Thomas Young's discovery of wave interference of light by his double-slit experiment in 1801. The wave view did not immediately displace the ray and particle view, but began to dominate scientific thinking about light in the mid 19th century, since it could explain polarization phenomena that the alternatives could not.James Clerk Maxwell discovered that he could apply his previously discovered Maxwell's equations, along with a slight modification to describe self-propagating waves of oscillating electric and magnetic fields. It quickly became apparent that visible light, ultraviolet light, and infrared light were all electromagnetic waves of differing frequency.: 272  This theory became a critical ingredient in the beginning of quantum mechanics.
-According to Laplace's elaboration of Newton's theory of refraction, a corpuscle incident on a plane interface between two homogeneous isotropic media was subject to a force field that was symmetrical about the interface. If both media were transparent, total reflection would occur if the corpuscle were turned back before it exited the field in the second medium. But if the second medium were opaque, reflection would not be total unless the corpuscle were turned back before it left the first medium; this required a larger critical angle than the one given by Snell's law, and consequently impugned the validity of Wollaston's method for opaque media. Laplace combined the two cases into a single formula for the relative refractive index in terms of the critical angle (minimum angle of incidence for TIR). The formula contained a parameter which took one value for a transparent external medium and another value for an opaque external medium. Laplace's theory further predicted a relationship between refractive index and density for a given substance.
-Nonetheless, he continued to develop his ideas. He believed that a wave model could much better explain many aspects of light propagation than the corpuscular model: A very extensive class of phenomena leads us still more directly to the same conclusion; they consist chiefly of the production of colours by means of transparent plates, and by diffraction or inflection, none of which have been explained upon the supposition of emanation, in a manner sufficiently minute or comprehensive to satisfy the most candid even of the advocates for the projectile system; while on the other hand all of them may be at once understood, from the effect of the interference of double lights, in a manner nearly similar to that which constitutes in sound the sensation of a beat, when two strings forming an imperfect unison, are heard to vibrate together.
-During this period, many scientists proposed a wave theory of light based on experimental observations, including Robert Hooke, Christiaan Huygens and Leonhard Euler. However, Isaac Newton, who did many experimental investigations of light, had rejected the wave theory of light and developed his corpuscular theory of light according to which light is emitted from a luminous body in the form of tiny particles. This theory held sway until the beginning of the nineteenth century despite the fact that many phenomena, including diffraction effects at edges or in narrow apertures, colours in thin films and insect wings, and the apparent failure of light particles to crash into one another when two light beams crossed, could not be adequately explained by the corpuscular theory which, nonetheless, had many eminent supporters, including Pierre-Simon Laplace and Jean-Baptiste Biot.
 A. Isaac Newton rejected the wave theory of light and proposed that light consists of corpuscles that are subject to a force acting parallel to the interface. In this model, the critical angle was the angle of incidence at which the normal velocity of the approaching corpuscle was just enough to reach the far side of the force field; at more oblique incidence, the corpuscle would be turned back.
 B. Isaac Newton rejected the wave theory of light and proposed that light consists of corpuscles that are subject to a force acting perpendicular to the interface. In this model, the critical angle was the angle of incidence at which the normal velocity of the approaching corpuscle was just enough to reach the near side of the force field; at more oblique incidence, the corpuscle would be turned back.
 C. Isaac Newton accepted the wave theory of light and proposed that light consists of transverse waves that are subject to a force acting perpendicular to the interface. In this model, the critical angle was the angle of incidence at which the normal velocity of the approaching wave was just enough to reach the far side of the force field; at more oblique incidence, the wave would be turned back.
 D. Isaac Newton rejected the wave theory of light and proposed that light consists of corpuscles that are subject to a force acting perpendicular to the interface. In this model, the critical angle was the angle of incidence at which the normal velocity of the approaching corpuscle was just enough to reach the far side of the force field; at more oblique incidence, the corpuscle would be turned back.
 E. Isaac Newton accepted the wave theory of light and proposed that light consists of longitudinal waves that are subject to a force acting perpendicular to the interface. In this model, the critical angle was the angle of incidence at which the normal velocity of the approaching wave was just enough to reach the far side of the force field; at more oblique incidence, the wave would be turned back. "
What is the relationship between chemical potential and quarks/antiquarks?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the relationship between chemical potential and quarks/antiquarks?
 Context: -The phase diagram of quark matter is not well known, either experimentally or theoretically. A commonly conjectured form of the phase diagram is shown in the figure to the right. It is applicable to matter in a compact star, where the only relevant thermodynamic potentials are quark chemical potential μ and temperature T.  For guidance it also shows the typical values of μ and T in heavy-ion collisions and in the early universe. For readers who are not familiar with the concept of a chemical potential, it is helpful to think of μ as a measure of the imbalance between quarks and antiquarks in the system. Higher μ means a stronger bias favoring quarks over antiquarks. At low temperatures there are no antiquarks, and then higher μ generally means a higher density of quarks.
-It is common in electrochemistry and solid-state physics to discuss both the chemical potential and the electrochemical potential of the electrons. However, in the two fields, the definitions of these two terms are sometimes swapped. In electrochemistry, the electrochemical potential of electrons (or any other species) is the total potential, including both the (internal, nonelectrical) chemical potential and the electric potential, and is by definition constant across a device in equilibrium, whereas the chemical potential of electrons is equal to the electrochemical potential minus the local electric potential energy per electron. In solid-state physics, the definitions are normally compatible with this, but occasionally  the definitions are swapped.
-Chemical potential Assuming that the concentration of fermions does not change with temperature, then the total chemical potential µ (Fermi level) of the three-dimensional ideal Fermi gas is related to the zero temperature Fermi energy EF by a Sommerfeld expansion (assuming  kBT≪EF ): where T is the temperature.Hence, the internal chemical potential, µ-E0, is approximately equal to the Fermi energy at temperatures that are much lower than the characteristic Fermi temperature TF. This characteristic temperature is on the order of 105 K for a metal, hence at room temperature (300 K), the Fermi energy and internal chemical potential are essentially equivalent.
-Contact potentials When solids of two different materials are in contact, thermodynamic equilibrium requires that one of the solids assume a higher electrical potential than the other. This is called the contact potential. Dissimilar metals in contact produce what is known also as a contact electromotive force or Galvani potential. The magnitude of this potential difference is often expressed as a difference in Fermi levels in the two solids when they are at charge neutrality, where the Fermi level (a name for the chemical potential of an electron system) describes the energy necessary to remove an electron from the body to some common point (such as ground). If there is an energy advantage in taking an electron from one body to the other, such a transfer will occur. The transfer causes a charge separation, with one body gaining electrons and the other losing electrons. This charge transfer causes a potential difference between the bodies, which partly cancels the potential originating from the contact, and eventually equilibrium is reached. At thermodynamic equilibrium, the Fermi levels are equal (the electron removal energy is identical) and there is now a built-in electrostatic potential between the bodies.
-Chemical potential plays an especially important role in solid-state physics and is closely related to the concepts of work function, Fermi energy, and Fermi level. For example, n-type silicon has a higher internal chemical potential of electrons than p-type silicon. In a p–n junction diode at equilibrium the chemical potential (internal chemical potential) varies from the p-type to the n-type side, while the total chemical potential (electrochemical potential, or, Fermi level) is constant throughout the diode.
 A. Chemical potential, represented by μ, is a measure of the imbalance between quarks and antiquarks in a system. Higher μ indicates a stronger bias favoring quarks over antiquarks.
 B. Chemical potential, represented by μ, is a measure of the balance between quarks and antiquarks in a system. Higher μ indicates an equal number of quarks and antiquarks.
 C. Chemical potential, represented by μ, is a measure of the imbalance between quarks and antiquarks in a system. Higher μ indicates a stronger bias favoring antiquarks over quarks.
 D. Chemical potential, represented by μ, is a measure of the density of antiquarks in a system. Higher μ indicates a higher density of antiquarks.
 E. Chemical potential, represented by μ, is a measure of the density of quarks in a system. Higher μ indicates a higher density of quarks. "
What is the American Petroleum Institute (API) gravity?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the American Petroleum Institute (API) gravity?
 Context: -API gravity is thus an inverse measure of a petroleum liquid's density relative to that of water (also known as specific gravity). It is used to compare densities of petroleum liquids. For example, if one petroleum liquid is less dense than another, it has a greater API gravity. Although API gravity is mathematically a dimensionless quantity (see the formula below), it is referred to as being in 'degrees'. API gravity is graduated in degrees on a hydrometer instrument. API gravity values of most petroleum liquids fall between 10 and 70 degrees.
-The American Petroleum Institute gravity, or API gravity, is a measure of how heavy or light a petroleum liquid is compared to water: if its API gravity is greater than 10, it is lighter and floats on water; if less than 10, it is heavier and sinks.
-Light crude oil has an API gravity higher than 31.1° (i.e., less than 870 kg/m3) Medium oil has an API gravity between 22.3 and 31.1° (i.e., 870 to 920 kg/m3) Heavy crude oil has an API gravity below 22.3° (i.e., 920 to 1000 kg/m3) Extra heavy oil has an API gravity below 10.0° (i.e., greater than 1000 kg/m3)However, not all parties use the same grading. The United States Geological Survey uses slightly different ranges.Crude oil with API gravity less than 10° is referred to as extra heavy oil or bitumen. Bitumen derived from oil sands deposits in Alberta, Canada, has an API gravity of around 8°. It can be diluted with lighter hydrocarbons to produce diluted bitumen, which has an API gravity of less than 22.3°, or further ""upgraded"" to an API gravity of 31 to 33° as synthetic crude.
-1.1 API gravity Density has always been an important criterion of oils, generally an oil with low density is considered to be more valuable than an oil with higher density due to the fact that if contains more light fractions (i.e. gasoline). Thus, the API gravity or specific gravity is widely used for the classification of crude oils, based on a scheme proposed by the American Petroleum Institute (Table 1).
-The formula to calculate API gravity from specific gravity (SG) is: API gravity 141.5 SG 131.5 Conversely, the specific gravity of petroleum liquids can be derived from their API gravity value as SG at 60 141.5 API gravity 131.5 Thus, a heavy oil with a specific gravity of 1.0 (i.e., with the same density as pure water at 60 °F) has an API gravity of: 141.5 1.0 131.5 10.0 API 
 A. API gravity is a measure of how heavy or light a petroleum liquid is compared to water. It is an inverse measure of a petroleum liquid's density relative to that of water and is graduated in degrees on a hydrometer instrument.
 B. API gravity is a measure of the viscosity of a petroleum liquid. It is an inverse measure of a petroleum liquid's density relative to that of water and is graduated in degrees on a hydrometer instrument.
 C. API gravity is a measure of the temperature at which a petroleum liquid freezes. It is an inverse measure of a petroleum liquid's density relative to that of water and is graduated in degrees on a hydrometer instrument.
 D. API gravity is a measure of how much petroleum liquid is present in a given volume of water. It is an inverse measure of a petroleum liquid's density relative to that of water and is graduated in degrees on a hydrometer instrument.
 E. API gravity is a measure of the acidity or alkalinity of a petroleum liquid. It is an inverse measure of a petroleum liquid's density relative to that of water and is graduated in degrees on a hydrometer instrument. "
What are the two main factors that cause resistance in a metal?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What are the two main factors that cause resistance in a metal?
 Context: -Most metals have electrical resistance. In simpler models (non quantum mechanical models) this can be explained by replacing electrons and the crystal lattice by a wave-like structure. When the electron wave travels through the lattice, the waves interfere, which causes resistance. The more regular the lattice is, the less disturbance happens and thus the less resistance. The amount of resistance is thus mainly caused by two factors. First, it is caused by the temperature and thus amount of vibration of the crystal lattice. Higher temperatures cause bigger vibrations, which act as irregularities in the lattice. Second, the purity of the metal is relevant as a mixture of different ions is also an irregularity. The small decrease in conductivity on melting of pure metals is due to the loss of long range crystalline order. The short range order remains and strong correlation between positions of ions results in coherence between waves diffracted by adjacent ions.
-Metals In general, electrical resistivity of metals increases with temperature. Electron–phonon interactions can play a key role. At high temperatures, the resistance of a metal increases linearly with temperature. As the temperature of a metal is reduced, the temperature dependence of resistivity follows a power law function of temperature. Mathematically the temperature dependence of the resistivity ρ of a metal can be approximated through the Bloch–Grüneisen formula: where  ρ(0) is the residual resistivity due to defect scattering, A is a constant that depends on the velocity of electrons at the Fermi surface, the Debye radius and the number density of electrons in the metal.  ΘR is the Debye temperature as obtained from resistivity measurements and matches very closely with the values of Debye temperature obtained from specific heat measurements. n is an integer that depends upon the nature of interaction: n = 5 implies that the resistance is due to scattering of electrons by phonons (as it is for simple metals) n = 3 implies that the resistance is due to s-d electron scattering (as is the case for transition metals) n = 2 implies that the resistance is due to electron–electron interaction.The Bloch–Grüneisen formula is an approximation obtained assuming that the studied metal has spherical Fermi surface inscribed within the first Brillouin zone and a Debye phonon spectrum.If more than one source of scattering is simultaneously present, Matthiessen's rule (first formulated by Augustus Matthiessen in the 1860s) states that the total resistance can be approximated by adding up several different terms, each with the appropriate value of n.
-As temperatures change, the electrical resistivity of amorphous metals behaves very different than that of regular metals. While the resistivity in regular metals generally increases with temperature, following the Matthiessen's rule, the resistivity in a large number of amorphous metals is found to decrease with increasing temperature. This is effect can be observed in amorphous metals of high resistivities between 150 and 300 microohm-centimeters. In these metals, the scattering events causing the resistivity of the metal can no longer be considered statistically independent, thus explaining the breakdown of the Matthiessen's rule. The fact that the thermal change of the resistivity in amorphous metals can be negative over a large range of temperatures and correlated to their absolute resistivity values was first observed by Mooij in 1973, hence coining the term ""Mooij-rule"".The alloys of boron, silicon, phosphorus, and other glass formers with magnetic metals (iron, cobalt, nickel) have high magnetic susceptibility, with low coercivity and high electrical resistance. Usually the electrical conductivity of a metallic glass is of the same low order of magnitude as of a molten metal just above the melting point. The high resistance leads to low losses by eddy currents when subjected to alternating magnetic fields, a property useful for e.g. transformer magnetic cores. Their low coercivity also contributes to low loss.
-Metal resistance PUF The metal resistance-based PUF derives its entropy from random physical variations in the metal contacts, vias and wires that define the power grid and interconnect of an IC. There are several important advantages to leveraging random resistance variations in the metal resources of an IC including: Temperature and voltage stability: Temperature and voltage (TV) variations represent one of the most significant challenges for PUFs in applications that require re-generation of exactly the same bitstring later in time, e.g., encryption. Metal resistance (unlike transistors) varies linearly with temperature and is independent of voltage. Therefore, metal resistance provides a very high level of robustness to changing environmental conditions.
-Temperature The effect of temperature on thermal conductivity is different for metals and nonmetals. In metals, heat conductivity is primarily due to free electrons. Following the Wiedemann–Franz law, thermal conductivity of metals is approximately proportional to the absolute temperature (in kelvins) times electrical conductivity. In pure metals the electrical conductivity decreases with increasing temperature and thus the product of the two, the thermal conductivity, stays approximately constant. However, as temperatures approach absolute zero, the thermal conductivity decreases sharply. In alloys the change in electrical conductivity is usually smaller and thus thermal conductivity increases with temperature, often proportionally to temperature. Many pure metals have a peak thermal conductivity between 2 K and 10 K.
 A. The amount of resistance in a metal is mainly caused by the temperature and the pressure applied to the metal. Higher temperatures cause bigger vibrations, and pressure causes the metal to become more compact, leading to more resistance.
 B. The amount of resistance in a metal is mainly caused by the temperature and the purity of the metal. Higher temperatures cause bigger vibrations, and a mixture of different ions acts as an irregularity.
 C. The amount of resistance in a metal is mainly caused by the temperature and the thickness of the metal. Higher temperatures cause bigger vibrations, and thicker metals have more irregularities, leading to more resistance.
 D. The amount of resistance in a metal is mainly caused by the purity of the metal and the amount of pressure applied to the metal. A mixture of different ions acts as an irregularity, and pressure causes the metal to become more compact, leading to more resistance.
 E. The amount of resistance in a metal is mainly caused by the purity of the metal and the thickness of the metal. A mixture of different ions acts as an irregularity, and thicker metals have more irregularities, leading to more resistance. "
What is the significance of the redshift-distance relationship in determining the expansion history of the universe?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the significance of the redshift-distance relationship in determining the expansion history of the universe?
 Context: -Extragalactic observations The most distant objects exhibit larger redshifts corresponding to the Hubble flow of the universe. The largest-observed redshift, corresponding to the greatest distance and furthest back in time, is that of the cosmic microwave background radiation; the numerical value of its redshift is about z = 1089 (z = 0 corresponds to present time), and it shows the state of the universe about 13.8 billion years ago, and 379,000 years after the initial moments of the Big Bang.The luminous point-like cores of quasars were the first ""high-redshift"" (z > 0.1) objects discovered before the improvement of telescopes allowed for the discovery of other high-redshift galaxies.For galaxies more distant than the Local Group and the nearby Virgo Cluster, but within a thousand megaparsecs or so, the redshift is approximately proportional to the galaxy's distance. This correlation was first observed by Edwin Hubble and has come to be known as Hubble's law. Vesto Slipher was the first to discover galactic redshifts, in about the year 1912, while Hubble correlated Slipher's measurements with distances he measured by other means to formulate his Law. In the widely accepted cosmological model based on general relativity, redshift is mainly a result of the expansion of space: this means that the farther away a galaxy is from us, the more the space has expanded in the time since the light left that galaxy, so the more the light has been stretched, the more redshifted the light is, and so the faster it appears to be moving away from us. Hubble's law follows in part from the Copernican principle. Because it is usually not known how luminous objects are, measuring the redshift is easier than more direct distance measurements, so redshift is sometimes in practice converted to a crude distance measurement using Hubble's law.Gravitational interactions of galaxies with each other and clusters cause a significant scatter in the normal plot of the Hubble diagram. The peculiar velocities associated with galaxies superimpose a rough trace of the mass of virialized objects in the universe. This effect leads to such phenomena as nearby galaxies (such as the Andromeda Galaxy) exhibiting blueshifts as we fall towards a common barycenter, and redshift maps of clusters showing a fingers of god effect due to the scatter of peculiar velocities in a roughly spherical distribution. This added component gives cosmologists a chance to measure the masses of objects independent of the mass-to-light ratio (the ratio of a galaxy's mass in solar masses to its brightness in solar luminosities), an important tool for measuring dark matter.The Hubble law's linear relationship between distance and redshift assumes that the rate of expansion of the universe is constant. However, when the universe was much younger, the expansion rate, and thus the Hubble ""constant"", was larger than it is today. For more distant galaxies, then, whose light has been travelling to us for much longer times, the approximation of constant expansion rate fails, and the Hubble law becomes a non-linear integral relationship and dependent on the history of the expansion rate since the emission of the light from the galaxy in question. Observations of the redshift-distance relationship can be used, then, to determine the expansion history of the universe and thus the matter and energy content.While it was long believed that the expansion rate has been continuously decreasing since the Big Bang, observations beginning in 1988 of the redshift-distance relationship using Type Ia supernovae have suggested that in comparatively recent times the expansion rate of the universe has begun to accelerate.
-Distance measures are used in physical cosmology to give a natural notion of the distance between two objects or events in the universe. They are often used to tie some observable quantity (such as the luminosity of a distant quasar, the redshift of a distant galaxy, or the angular size of the acoustic peaks in the cosmic microwave background (CMB) power spectrum) to another quantity that is not directly observable, but is more convenient for calculations (such as the comoving coordinates of the quasar, galaxy, etc.). The distance measures discussed here all reduce to the common notion of Euclidean distance at low redshift.
-Expansion of space In the earlier part of the twentieth century, Slipher, Wirtz and others made the first measurements of the redshifts and blueshifts of galaxies beyond the Milky Way. They initially interpreted these redshifts and blueshifts as being due to random motions, but later Lemaître (1927) and Hubble (1929), using previous data, discovered a roughly linear correlation between the increasing redshifts of, and distances to, galaxies. Lemaître realized that these observations could be explained by a mechanism of producing redshifts seen in Friedmann's solutions to Einstein's equations of general relativity. The correlation between redshifts and distances is required by all such models that have a metric expansion of space. As a result, the wavelength of photons propagating through the expanding space is stretched, creating the cosmological redshift.
-The observational result of Hubble's Law, the proportional relationship between distance and the speed with which a galaxy is moving away from us (usually referred to as redshift) is a product of the cosmic distance ladder. Edwin Hubble observed that fainter galaxies are more redshifted. Finding the value of the Hubble constant was the result of decades of work by many astronomers, both in amassing the measurements of galaxy redshifts and in calibrating the steps of the distance ladder. Hubble's Law is the primary means we have for estimating the distances of quasars and distant galaxies in which individual distance indicators cannot be seen.
-The redshift observed in astronomy can be measured because the emission and absorption spectra for atoms are distinctive and well known, calibrated from spectroscopic experiments in laboratories on Earth. When the redshift of various absorption and emission lines from a single astronomical object is measured, z is found to be remarkably constant. Although distant objects may be slightly blurred and lines broadened, it is by no more than can be explained by thermal or mechanical motion of the source. For these reasons and others, the consensus among astronomers is that the redshifts they observe are due to some combination of the three established forms of Doppler-like redshifts. Alternative hypotheses and explanations for redshift such as tired light are not generally considered plausible.Spectroscopy, as a measurement, is considerably more difficult than simple photometry, which measures the brightness of astronomical objects through certain filters. When photometric data is all that is available (for example, the Hubble Deep Field and the Hubble Ultra Deep Field), astronomers rely on a technique for measuring photometric redshifts. Due to the broad wavelength ranges in photometric filters and the necessary assumptions about the nature of the spectrum at the light-source, errors for these sorts of measurements can range up to δz = 0.5, and are much less reliable than spectroscopic determinations. However, photometry does at least allow a qualitative characterization of a redshift. For example, if a Sun-like spectrum had a redshift of z = 1, it would be brightest in the infrared(1000nm) rather than at the blue-green(500nm) color associated with the peak of its blackbody spectrum, and the light intensity will be reduced in the filter by a factor of four, (1 + z)2. Both the photon count rate and the photon energy are redshifted. (See K correction for more details on the photometric consequences of redshift.) Local observations In nearby objects (within our Milky Way galaxy) observed redshifts are almost always related to the line-of-sight velocities associated with the objects being observed. Observations of such redshifts and blueshifts have enabled astronomers to measure velocities and parametrize the masses of the orbiting stars in spectroscopic binaries, a method first employed in 1868 by British astronomer William Huggins. Similarly, small redshifts and blueshifts detected in the spectroscopic measurements of individual stars are one way astronomers have been able to diagnose and measure the presence and characteristics of planetary systems around other stars and have even made very detailed differential measurements of redshifts during planetary transits to determine precise orbital parameters. Finely detailed measurements of redshifts are used in helioseismology to determine the precise movements of the photosphere of the Sun. Redshifts have also been used to make the first measurements of the rotation rates of planets, velocities of interstellar clouds, the rotation of galaxies, and the dynamics of accretion onto neutron stars and black holes which exhibit both Doppler and gravitational redshifts. Additionally, the temperatures of various emitting and absorbing objects can be obtained by measuring Doppler broadening—effectively redshifts and blueshifts over a single emission or absorption line. By measuring the broadening and shifts of the 21-centimeter hydrogen line in different directions, astronomers have been able to measure the recessional velocities of interstellar gas, which in turn reveals the rotation curve of our Milky Way. Similar measurements have been performed on other galaxies, such as Andromeda. As a diagnostic tool, redshift measurements are one of the most important spectroscopic measurements made in astronomy.
 A. Observations of the redshift-distance relationship can be used to determine the expansion history of the universe and the matter and energy content, especially for galaxies whose light has been travelling to us for much shorter times.
 B. Observations of the redshift-distance relationship can be used to determine the age of the universe and the matter and energy content, especially for nearby galaxies whose light has been travelling to us for much shorter times.
 C. Observations of the redshift-distance relationship can be used to determine the expansion history of the universe and the matter and energy content, especially for nearby galaxies whose light has been travelling to us for much longer times.
 D. Observations of the redshift-distance relationship can be used to determine the age of the universe and the matter and energy content, especially for distant galaxies whose light has been travelling to us for much shorter times.
 E. Observations of the redshift-distance relationship can be used to determine the expansion history of the universe and the matter and energy content, especially for distant galaxies whose light has been travelling to us for much longer times. "
What is the Evans balance?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the Evans balance?
 Context: -An Evans balance, also known as a Johnson's balance (after a commercial producer of the Evans balance) is a device for measuring magnetic susceptibility. Magnetic susceptibility is related to the force experienced by a substance in a magnetic field. Various practical devices are available for the measurement of susceptibility, which differ in the shape of the magnetic field and the way the force is measured.The Evans balance employs a similar sample configuration but measures the force on the magnet.
-The Evans balance. is a torsion balance which uses a sample in a fixed position and a variable secondary magnet to bring the magnets back to their initial position. It, too, is calibrated against HgCo(NCS)4.
With a Faraday balance the sample is placed in a magnetic field of constant gradient, and weighed on a torsion balance. This method can yield information on magnetic anisotropy.
SQUID is a very sensitive magnetometer.
For substances in solution NMR may be used to measure susceptibility.
-Volume magnetic susceptibility is measured by the force change felt upon a substance when a magnetic field gradient is applied. Early measurements are made using the Gouy balance where a sample is hung between the poles of an electromagnet. The change in weight when the electromagnet is turned on is proportional to the susceptibility. Today, high-end measurement systems use a superconductive magnet. An alternative is to measure the force change on a strong compact magnet upon insertion of the sample. This system, widely used today, is called the Evans balance. For liquid samples, the susceptibility can be measured from the dependence of the NMR frequency of the sample on its shape or orientation.Another method using NMR techniques measures the magnetic field distortion around a sample immersed in water inside an MR scanner. This method is highly accurate for diamagnetic materials with susceptibilities similar to water.
-Advantages vs alternative magnetic balances The main advantage of this system is that it is cheap to construct as it does not require a precision weighing device. Moreover, using a Evans balance is less time-consuming than using a Gouy or Faraday balances, although it is not sensitive and accurate in comparison to these last two systems. One reason that they were time-consuming is that the sample had to be suspended between the two poles of a very powerful magnet. The tube had to be suspended in the same place every time for the apparatus constant to be accurate. In the case of the Gouy balance, the static charge on the glass tube often caused the tube to stick to magnets. With the Evans balance, a reading could be taken in a matter of seconds with only small sacrifices in sensitivity and accuracy. A Johnson-Matthey balance has a range from 0.001 x 10−7 to 1.99 x 10−7 c.g.s. volume susceptibility units. The original Evans balance had an accuracy within 1% of literature values for diamagnetic solutions and within 2% of literature values of paramagnetic solids.The system allows for measurements of solid, liquid, and gaseous forms of a wide range of paramagnetic and diamagnetic materials. For each measurement, only around 250 mg of sample is required (50 mg can be used for a thin-bore sample tube).
-The Evans balance measures susceptibility indirectly by referring to a calibration standard of known susceptibility. The most convenient compound for this purpose is mercury cobalt thiocyanate, HgCo(NCS)4, which has a susceptibility of 16.44×10−6 (±0.5%) CGS at 20 °C. Another common calibration standard is [Ni(en)3]S2O3 which has a susceptibility of 1.104 x 10−5 erg G−2 cm−3. Three readings of the meter are needed, of an empty tube, R0 of the tube filled with calibrant and of the tube filled with the sample, Rs. Some balances have an auto-tare feature that eliminates the need for the R0 measurement. Accuracy depends somewhat on the homogeneous packing of the sample. The first two provide a calibration constant, C. The mass susceptibility in grams is calculated as 10 9m where L is the length of the sample, C is the calibration constant (usually 1 if it has been calibrated), and m is its mass in grams. The reading for the empty tube is needed because the tube glass is diamagnetic. There is a V term multiplied by an A term in the most general form of the equation. These two terms (V∗A) are collectively added to the numerator in the above equation. The V term is the volume susceptibility of air (0.029 x 10−6 erg G−2 cm−3) and A is the cross-sectional area of the sample. These two terms can be ignored for solid samples, yielding the original equation written above.To calculate the volume magnetic susceptibility (χ) instead of the weight susceptibility (χg), such as in a liquid sample, the equation would have the extra V term added to the numerator and instead of being divided by m, the equation would be divided by d for the density of the solution.
 A. The Evans balance is a system used to measure the change in weight of a sample when an electromagnet is turned on, which is proportional to the susceptibility.
 B. The Evans balance is a system used to measure the dependence of the NMR frequency of a liquid sample on its shape or orientation to determine its susceptibility.
 C. The Evans balance is a system used to measure the magnetic field distortion around a sample immersed in water inside an MR scanner to determine its susceptibility.
 D. The Evans balance is a system used to measure the susceptibility of a sample by measuring the force change on a strong compact magnet upon insertion of the sample.
 E. The Evans balance is a system used to measure the magnetic susceptibility of most crystals, which is not a scalar quantity. "
What is the definition of dimension in mathematics?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the definition of dimension in mathematics?
 Context: -In mathematics, the dimension of an object is, roughly speaking, the number of degrees of freedom of a point that moves on this object. In other words, the dimension is the number of independent parameters or coordinates that are needed for defining the position of a point that is constrained to be on the object. For example, the dimension of a point is zero; the dimension of a line is one, as a point can move on a line in only one direction (or its opposite); the dimension of a plane is two etc.
-The dimension is an intrinsic property of an object, in the sense that it is independent of the dimension of the space in which the object is or can be embedded. For example, a curve, such as a circle, is of dimension one, because the position of a point on a curve is determined by its signed distance along the curve to a fixed point on the curve. This is independent from the fact that a curve cannot be embedded in a Euclidean space of dimension lower than two, unless it is a line.
-In physics and mathematics, the dimension of a mathematical space (or object) is informally defined as the minimum number of coordinates needed to specify any point within it. Thus, a line has a dimension of one (1D) because only one coordinate is needed to specify a point on it – for example, the point at 5 on a number line. A surface, such as the boundary of a cylinder or sphere, has a dimension of two (2D) because two coordinates are needed to specify a point on it – for example, both a latitude and longitude are required to locate a point on the surface of a sphere. A two-dimensional Euclidean space is a two-dimensional space on the plane. The inside of a cube, a cylinder or a sphere is three-dimensional (3D) because three coordinates are needed to locate a point within these spaces.
-The intuitive concept of dimension of a geometric object X is the number of independent parameters one needs to pick out a unique point inside. However, any point specified by two parameters can be instead specified by one, because the cardinality of the real plane is equal to the cardinality of the real line (this can be seen by an argument involving interweaving the digits of two numbers to yield a single number encoding the same information). The example of a space-filling curve shows that one can even map the real line to the real plane surjectively (taking one real number into a pair of real numbers in a way so that all pairs of numbers are covered) and continuously, so that a one-dimensional object completely fills up a higher-dimensional object.
-Dimension/Hierarchy. Dimension is a dimension of a cube. A dimension is a primary organizer of measure and attribute information in a cube. MDX does not know of, nor does it assume any, dependencies between dimensions - they are assumed to be mutually independent. A dimension will contain some members (see below) organized in some hierarchy or hierarchies containing levels. It can be specified by its unique name, e.g. [Time] or it can be returned by an MDX function, e.g. .Dimension. Hierarchy is a dimension hierarchy of a cube. It can be specified by its unique name, e.g. [Time].[Fiscal] or it can be returned by an MDX function, e.g. .Hierarchy. Hierarchies are contained within dimensions. (OLEDB for OLAP MDX specification does not distinguish between dimension and hierarchy data types. Some implementations, such as Microsoft Analysis Services, treat them differently.) Level. Level is a [[:sjjhhikt:level|level]] in a dimension hierarchy. It can be specified by its unique name, e.g. [Time].[Fiscal].[Month] or it can be returned by an MDX function, e.g. .Level.
 A. The dimension of an object is the number of independent parameters or coordinates needed to define the position of a point constrained to be on the object, and is an extrinsic property of the object, dependent on the dimension of the space in which it is embedded.
 B. The dimension of an object is the number of degrees of freedom of a point that moves on this object, and is an extrinsic property of the object, dependent on the dimension of the space in which it is embedded.
 C. The dimension of an object is the number of independent parameters or coordinates needed to define the position of a point constrained to be on the object, and is an intrinsic property of the object, independent of the dimension of the space in which it is embedded.
 D. The dimension of an object is the number of directions in which a point can move on the object, and is an extrinsic property of the object, dependent on the dimension of the space in which it is embedded.
 E. The dimension of an object is the number of directions in which a point can move on the object, and is an intrinsic property of the object, independent of the dimension of the space in which it is embedded. "
What is accelerator-based light-ion fusion?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is accelerator-based light-ion fusion?
 Context: -Beam–beam or beam–target fusion Accelerator-based light-ion fusion is a technique using particle accelerators to achieve particle kinetic energies sufficient to induce light-ion fusion reactions.Accelerating light ions is relatively easy, and can be done in an efficient manner—requiring only a vacuum tube, a pair of electrodes, and a high-voltage transformer; fusion can be observed with as little as 10 kV between the electrodes. The system can be arranged to accelerate ions into a static fuel-infused target, known as beam–target fusion, or by accelerating two streams of ions towards each other, beam–beam fusion. The key problem with accelerator-based fusion (and with cold targets in general) is that fusion cross sections are many orders of magnitude lower than Coulomb interaction cross-sections. Therefore, the vast majority of ions expend their energy emitting bremsstrahlung radiation and the ionization of atoms of the target. Devices referred to as sealed-tube neutron generators are particularly relevant to this discussion. These small devices are miniature particle accelerators filled with deuterium and tritium gas in an arrangement that allows ions of those nuclei to be accelerated against hydride targets, also containing deuterium and tritium, where fusion takes place, releasing a flux of neutrons. Hundreds of neutron generators are produced annually for use in the petroleum industry where they are used in measurement equipment for locating and mapping oil reserves.A number of attempts to recirculate the ions that ""miss"" collisions have been made over the years. One of the better-known attempts in the 1970s was Migma, which used a unique particle storage ring to capture ions into circular orbits and return them to the reaction area. Theoretical calculations made during funding reviews pointed out that the system would have significant difficulty scaling up to contain enough fusion fuel to be relevant as a power source. In the 1990s, a new arrangement using a field-reverse configuration (FRC) as the storage system was proposed by Norman Rostoker and continues to be studied by TAE Technologies as of 2021. A closely related approach is to merge two FRC's rotating in opposite directions, which is being actively studied by Helion Energy. Because these approaches all have ion energies well beyond the Coulomb barrier, they often suggest the use of alternative fuel cycles like p-11B that are too difficult to attempt using conventional approaches.
-Beam–beam or beam–target fusion Accelerator-based light-ion fusion is a technique using particle accelerators to achieve particle kinetic energies sufficient to induce light-ion fusion reactions.Accelerating light ions is relatively easy, and can be done in an efficient manner—requiring only a vacuum tube, a pair of electrodes, and a high-voltage transformer; fusion can be observed with as little as 10 kV between the electrodes. The system can be arranged to accelerate ions into a static fuel-infused target, known as beam–target fusion, or by accelerating two streams of ions towards each other, beam–beam fusion. The key problem with accelerator-based fusion (and with cold targets in general) is that fusion cross sections are many orders of magnitude lower than Coulomb interaction cross-sections. Therefore, the vast majority of ions expend their energy emitting bremsstrahlung radiation and the ionization of atoms of the target. Devices referred to as sealed-tube neutron generators are particularly relevant to this discussion. These small devices are miniature particle accelerators filled with deuterium and tritium gas in an arrangement that allows ions of those nuclei to be accelerated against hydride targets, also containing deuterium and tritium, where fusion takes place, releasing a flux of neutrons. Hundreds of neutron generators are produced annually for use in the petroleum industry where they are used in measurement equipment for locating and mapping oil reserves.A number of attempts to recirculate the ions that ""miss"" collisions have been made over the years. One of the better-known attempts in the 1970s was Migma, which used a unique particle storage ring to capture ions into circular orbits and return them to the reaction area. Theoretical calculations made during funding reviews pointed out that the system would have significant difficulty scaling up to contain enough fusion fuel to be relevant as a power source. In the 1990s, a new arrangement using a field-reverse configuration (FRC) as the storage system was proposed by Norman Rostoker and continues to be studied by TAE Technologies as of 2021. A closely related approach is to merge two FRC's rotating in opposite directions, which is being actively studied by Helion Energy. Because these approaches all have ion energies well beyond the Coulomb barrier, they often suggest the use of alternative fuel cycles like p-11B that are too difficult to attempt using conventional approaches.
-Cyclotron particle accelerators, linear particle accelerators, and synchrotron particle accelerators can accelerate positively charged hydrogen ions (protons) until their velocity approaches the speed of light. Each ion has a kinetic energy range of 100-1000+ MeV. The resulting high energy protons can capture electrons from electron emitter electrodes, and be thus electrically neutralized. This creates an electrically neutral beam of high energy hydrogen atoms, that can proceed in a straight line at near the speed of light to smash into its target and damage it.
-A particle accelerator is a machine that uses electromagnetic fields to propel charged particles to very high speeds and energies, and to contain them in well-defined beams.Large accelerators are used for fundamental research in particle physics. The largest accelerator currently active is the Large Hadron Collider (LHC) near Geneva, Switzerland, operated by the CERN. It is a collider accelerator, which can accelerate two beams of protons to an energy of 6.5 TeV and cause them to collide head-on, creating center-of-mass energies of 13 TeV. Other powerful accelerators are, RHIC at Brookhaven National Laboratory in New York and, formerly, the Tevatron at Fermilab, Batavia, Illinois. Accelerators are also used as synchrotron light sources for the study of condensed matter physics. Smaller particle accelerators are used in a wide variety of applications, including particle therapy for oncological purposes, radioisotope production for medical diagnostics, ion implanters for the manufacture of semiconductors, and accelerator mass spectrometers for measurements of rare isotopes such as radiocarbon. There are currently more than 30,000 accelerators in operation around the world.There are two basic classes of accelerators: electrostatic and electrodynamic (or electromagnetic) accelerators. Electrostatic particle accelerators use static electric fields to accelerate particles. The most common types are the Cockcroft–Walton generator and the Van de Graaff generator. A small-scale example of this class is the cathode ray tube in an ordinary old television set. The achievable kinetic energy for particles in these devices is determined by the accelerating voltage, which is limited by electrical breakdown. Electrodynamic or electromagnetic accelerators, on the other hand, use changing electromagnetic fields (either magnetic induction or oscillating radio frequency fields) to accelerate particles. Since in these types the particles can pass through the same accelerating field multiple times, the output energy is not limited by the strength of the accelerating field. This class, which was first developed in the 1920s, is the basis for most modern large-scale accelerators.
-Heavy ion fusion is a fusion energy concept that uses a stream of high-energy ions from a particle accelerator to rapidly heat and compress a small pellet of fusion fuel. It is a subclass of the larger inertial confinement fusion (ICF) approach, replacing the more typical laser systems with an accelerator.
 A. Accelerator-based light-ion fusion is a technique that uses particle accelerators to achieve particle kinetic energies sufficient to induce light-ion fusion reactions. This method is relatively easy to implement and can be done in an efficient manner, requiring only a vacuum tube, a pair of electrodes, and a high-voltage transformer. Fusion can be observed with as little as 10 kV between the electrodes.
 B. Accelerator-based light-ion fusion is a technique that uses particle accelerators to achieve particle kinetic energies sufficient to induce heavy-ion fusion reactions. This method is relatively difficult to implement and requires a complex system of vacuum tubes, electrodes, and transformers. Fusion can be observed with as little as 10 kV between the electrodes.
 C. Accelerator-based light-ion fusion is a technique that uses particle accelerators to achieve particle kinetic energies sufficient to induce light-ion fusion reactions. This method is relatively difficult to implement and requires a complex system of vacuum tubes, electrodes, and transformers. Fusion can be observed with as little as 100 kV between the electrodes.
 D. Accelerator-based light-ion fusion is a technique that uses particle accelerators to achieve particle kinetic energies sufficient to induce heavy-ion fusion reactions. This method is relatively easy to implement and can be done in an efficient manner, requiring only a vacuum tube, a pair of electrodes, and a high-voltage transformer. Fusion can be observed with as little as 100 kV between the electrodes.
 E. Accelerator-based light-ion fusion is a technique that uses particle accelerators to achieve particle kinetic energies sufficient to induce light-ion fission reactions. This method is relatively easy to implement and can be done in an efficient manner, requiring only a vacuum tube, a pair of electrodes, and a high-voltage transformer. Fission can be observed with as little as 10 kV between the electrodes. "
What is the interstellar medium (ISM)?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the interstellar medium (ISM)?
 Context: -In astronomy, the interstellar medium (ISM) is the matter and radiation that exist in the space between the star systems in a galaxy. This matter includes gas in ionic, atomic, and molecular form, as well as dust and cosmic rays. It fills interstellar space and blends smoothly into the surrounding intergalactic space. The energy that occupies the same volume, in the form of electromagnetic radiation, is the interstellar radiation field. Although the density of atoms in the ISM is usually far below that in the best laboratory vacuums, the mean free path between collisions is short compared to typical interstellar lengths, so on these scales the ISM behaves as a gas (more precisely, as a plasma: it is everywhere at least slightly ionized), responding to pressure forces, and not as a collection of non-interacting particles.
-X-ray Quantum Calorimeter (XQC) project In astronomy, the interstellar medium (or ISM) is the gas and cosmic dust that pervade interstellar space: the matter that exists between the star systems within a galaxy. It fills interstellar space and blends smoothly into the surrounding intergalactic medium. The interstellar medium consists of an extremely dilute (by terrestrial standards) mixture of ions, atoms, molecules, larger dust grains, cosmic rays, and (galactic) magnetic fields. The energy that occupies the same volume, in the form of electromagnetic radiation, is the interstellar radiation field.
-The interstellar medium is matter that occupies the space between star systems in a galaxy. 99% of this matter is gaseous – hydrogen, helium, and smaller quantities of other ionized elements such as oxygen. The other 1% is dust particles, thought to be mainly graphite, silicates, and ices. Clouds of the dust and gas are referred to as nebulae.
-Interstellar space Interstellar space is the physical space within a galaxy beyond the influence each star has upon the encompassed plasma. The contents of interstellar space are called the interstellar medium. Approximately 70% of the mass of the interstellar medium consists of lone hydrogen atoms; most of the remainder consists of helium atoms. This is enriched with trace amounts of heavier atoms formed through stellar nucleosynthesis. These atoms are ejected into the interstellar medium by stellar winds or when evolved stars begin to shed their outer envelopes such as during the formation of a planetary nebula. The cataclysmic explosion of a supernova generates an expanding shock wave consisting of ejected materials that further enrich the medium. The density of matter in the interstellar medium can vary considerably: the average is around 106 particles per m3, but cold molecular clouds can hold 108–1012 per m3.A number of molecules exist in interstellar space, as can tiny 0.1 μm dust particles. The tally of molecules discovered through radio astronomy is steadily increasing at the rate of about four new species per year. Large regions of higher density matter known as molecular clouds allow chemical reactions to occur, including the formation of organic polyatomic species. Much of this chemistry is driven by collisions. Energetic cosmic rays penetrate the cold, dense clouds and ionize hydrogen and helium, resulting, for example, in the trihydrogen cation. An ionized helium atom can then split relatively abundant carbon monoxide to produce ionized carbon, which in turn can lead to organic chemical reactions.The local interstellar medium is a region of space within 100 parsecs (pc) of the Sun, which is of interest both for its proximity and for its interaction with the Solar System. This volume nearly coincides with a region of space known as the Local Bubble, which is characterized by a lack of dense, cold clouds. It forms a cavity in the Orion Arm of the Milky Way galaxy, with dense molecular clouds lying along the borders, such as those in the constellations of Ophiuchus and Taurus. (The actual distance to the border of this cavity varies from 60 to 250 pc or more.) This volume contains about 104–105 stars and the local interstellar gas counterbalances the astrospheres that surround these stars, with the volume of each sphere varying depending on the local density of the interstellar medium. The Local Bubble contains dozens of warm interstellar clouds with temperatures of up to 7,000 K and radii of 0.5–5 pc.When stars are moving at sufficiently high peculiar velocities, their astrospheres can generate bow shocks as they collide with the interstellar medium. For decades it was assumed that the Sun had a bow shock. In 2012, data from Interstellar Boundary Explorer (IBEX) and NASA's Voyager probes showed that the Sun's bow shock does not exist. Instead, these authors argue that a subsonic bow wave defines the transition from the solar wind flow to the interstellar medium. A bow shock is the third boundary of an astrosphere after the termination shock and the astropause (called the heliopause in the Solar System).
-The interstellar medium is composed of multiple phases distinguished by whether matter is ionic, atomic, or molecular, and the temperature and density of the matter. The interstellar medium is composed primarily of hydrogen, followed by helium with trace amounts of carbon, oxygen, and nitrogen. The thermal pressures of these phases are in rough equilibrium with one another. Magnetic fields and turbulent motions also provide pressure in the ISM, and are typically more important, dynamically, than the thermal pressure. In the interstellar medium, matter is primarily in molecular form and reaches number densities of 1012 molecules per m3 (1 trillion molecules per m3). In hot, diffuse regions, gas is highly ionized, and the density may be as low as 100 ions per m3. Compare this with a number density of roughly 1025 molecules per m3 for air at sea level, and 1016 molecules per m3 (10 quadrillion molecules per m3) for a laboratory high-vacuum chamber. By mass, 99% of the ISM is gas in any form, and 1% is dust. Of the gas in the ISM, by number 91% of atoms are hydrogen and 8.9% are helium, with 0.1% being atoms of elements heavier than hydrogen or helium, known as ""metals"" in astronomical parlance. By mass this amounts to 70% hydrogen, 28% helium, and 1.5% heavier elements. The hydrogen and helium are primarily a result of primordial nucleosynthesis, while the heavier elements in the ISM are mostly a result of enrichment (due to stellar nucleosynthesis) in the process of stellar evolution.
 A. The matter and radiation that exist in the space between the star systems in a galaxy, including gas in ionic, atomic, and molecular form, as well as dust and cosmic rays. It fills interstellar space and blends smoothly into the surrounding intergalactic space.
 B. The matter and radiation that exist in the space between stars in a galaxy, including gas in ionic, atomic, and molecular form, as well as dust and cosmic rays. It fills interstellar space and blends smoothly into the surrounding interplanetary space.
 C. The matter and radiation that exist in the space between galaxies, including gas in ionic, atomic, and molecular form, as well as dust and cosmic rays. It fills intergalactic space and blends smoothly into the surrounding interstellar space.
 D. The matter and radiation that exist in the space between planets in a solar system, including gas in ionic, atomic, and molecular form, as well as dust and cosmic rays. It fills interplanetary space and blends smoothly into the surrounding interstellar space.
 E. The matter and radiation that exist within a star, including gas in ionic, atomic, and molecular form, as well as dust and cosmic rays. It fills the star and blends smoothly into the surrounding interstellar space. "
What is the significance of the change in slope of the pinched hysteresis curves in ReRAM and other forms of two-terminal resistance memory?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the significance of the change in slope of the pinched hysteresis curves in ReRAM and other forms of two-terminal resistance memory?
 Context: -Pinched hysteresis One of the resulting properties of memristors and memristive systems is the existence of a pinched hysteresis effect. For a current-controlled memristive system, the input u(t) is the current i(t), the output y(t) is the voltage v(t), and the slope of the curve represents the electrical resistance. The change in slope of the pinched hysteresis curves demonstrates switching between different resistance states which is a phenomenon central to ReRAM and other forms of two-terminal resistance memory. At high frequencies, memristive theory predicts the pinched hysteresis effect will degenerate, resulting in a straight line representative of a linear resistor. It has been proven that some types of non-crossing pinched hysteresis curves (denoted Type-II) cannot be described by memristors.
-As the frequency tends to infinity, the hysteresis loop degenerates to a straight line through the origin, whose slope depends on the amplitude and shape of the forcing signal.According to Chua all resistive switching memories including ReRAM, MRAM and phase-change memory meet these criteria and are memristors. However, the lack of data for the Lissajous curves over a range of initial conditions or over a range of frequencies complicates assessments of this claim.
-Experimental evidence shows that redox-based resistance memory (ReRAM) includes a nanobattery effect that is contrary to Chua's memristor model. This indicates that the memristor theory needs to be extended or corrected to enable accurate ReRAM modeling.
-The above-mentioned thermodynamic principle furthermore implies that the operation of two-terminal non-volatile memory devices (e.g. ""resistance-switching"" memory devices (ReRAM)) cannot be associated with the memristor concept, i.e., such devices cannot by itself remember their current or voltage history. Transitions between distinct internal memory or resistance states are of probabilistic nature. The probability for a transition from state {i} to state {j} depends on the height of the free-energy barrier between both states. The transition probability can thus be influenced by suitably driving the memory device, i.e., by ""lowering"" the free-energy barrier for the transition {i} → {j} by means of, for example, an externally applied bias.
-Let HM = y, AM = x, NH = u, and HI = w = dx. Let the tangent at each point on the curve,  p=−dydx=uw . The reduction of the resistance of the sloping ring NI compared to the vertical ring NH rotated about AB is  r=yp31+p2dx=yu3u2+w2 (2) Let the minimum resistance solid be replaced by an identical one, except that the arc between points I and K is shifted by a small distance to the right  IJ=KL=o>0 , or to the left  Ij=Kl=o<0 , as shown in more detail in Fig. 5. In either case, HI becomes  HJ,Hj=w+o The resistance of the arcs of the curve DN and SG are unchanged. Also, the resistance of the arc IK is not changed by being shifted, since the slope remains the same along its length. The only change to the overall resistance of DNSG is due to the change to the gradient of arcs NI and KS. The 2 displacements have to be equal for the slope of the arc IK to be unaffected, and the new curve to end at G.
 A. The change in slope of the pinched hysteresis curves demonstrates switching between different resistance states, which is a phenomenon central to ReRAM and other forms of two-terminal resistance memory.
 B. The change in slope of the pinched hysteresis curves indicates the presence of a Type-II non-crossing curve, which is a phenomenon central to ReRAM and other forms of two-terminal resistance memory.
 C. The change in slope of the pinched hysteresis curves demonstrates the presence of a memristor, which is a phenomenon central to ReRAM and other forms of two-terminal resistance memory.
 D. The change in slope of the pinched hysteresis curves demonstrates the presence of a memristive network, which is a phenomenon central to ReRAM and other forms of two-terminal resistance memory.
 E. The change in slope of the pinched hysteresis curves indicates the presence of a linear resistor, which is a phenomenon central to ReRAM and other forms of two-terminal resistance memory. "
What is geometric quantization in mathematical physics?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is geometric quantization in mathematical physics?
 Context: -In mathematical physics, geometric quantization is a mathematical approach to defining a quantum theory corresponding to a given classical theory. It attempts to carry out quantization, for which there is in general no exact recipe, in such a way that certain analogies between the classical theory and the quantum theory remain manifest. For example, the similarity between the Heisenberg equation in the Heisenberg picture of quantum mechanics and the Hamilton equation in classical physics should be built in.
-In mathematical physics, geometric quantization is a mathematical approach to defining a quantum theory corresponding to a given classical theory. It attempts to carry out quantization, for which there is in general no exact recipe, in such a way that certain analogies between the classical theory and the quantum theory remain manifest. For example, the similarity between the Heisenberg equation in the Heisenberg picture of quantum mechanics and the Hamilton equation in classical physics should be built in.
-Quantization is the process of constraining an input from a continuous or otherwise large set of values (such as the real numbers) to a discrete set (such as the integers). The term quantization may refer to: 
-In physics, quantisation (in American English quantization) is the systematic transition procedure from a classical understanding of physical phenomena to a newer understanding known as quantum mechanics. It is a procedure for constructing quantum mechanics from classical mechanics. A generalization involving infinite degrees of freedom is field quantization, as in the ""quantization of the electromagnetic field"", referring to photons as field ""quanta"" (for instance as light quanta). This procedure is basic to theories of atomic physics, chemistry, particle physics, nuclear physics, condensed matter physics, and quantum optics.
-Quantization (physics) Canonical quantization Geometric quantization Discrete spectrum, or otherwise discrete quantity Spatial quantization Charge quantization 
 A. Geometric quantization is a mathematical approach to defining a classical theory corresponding to a given quantum theory. It attempts to carry out quantization in such a way that certain analogies between the quantum theory and the classical theory are lost.
 B. Geometric quantization is a mathematical approach to defining a quantum theory corresponding to a given classical theory. It attempts to carry out quantization in such a way that certain analogies between the classical theory and the quantum theory are lost.
 C. Geometric quantization is a mathematical approach to defining a classical theory corresponding to a given quantum theory. It attempts to carry out quantization in such a way that certain analogies between the quantum theory and the classical theory remain manifest.
 D. Geometric quantization is a mathematical approach to defining a quantum theory corresponding to a given classical theory. It attempts to carry out quantization in such a way that certain analogies between the classical theory and the quantum theory are not important.
 E. Geometric quantization is a mathematical approach to defining a quantum theory corresponding to a given classical theory. It attempts to carry out quantization in such a way that certain analogies between the classical theory and the quantum theory remain manifest. "
What is the definition of an improper rotation?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the definition of an improper rotation?
 Context: -An improper rotation is the composition of a rotation about an axis, and reflection in a plane perpendicular to that axis. The order in which the rotation and reflection are performed does not matter (that is, these operations commute). Improper rotation is also defined as the composition of a rotation about an axis, and inversion about a point on the axis. These definitions are equivalent because inversion about a point is equivalent to rotation by 180° about any axis, followed by mirroring about a plane perpendicular to that axis. The symmetry elements for improper rotation are the rotation axis, and either the mirror plane, the inversion point, or both. The improper rotation group of order 2n is denoted S2n.
-In geometry, an improper rotation (also called rotation-reflection, rotoreflection, rotary reflection, or rotoinversion) is an isometry in Euclidean space that is a combination of a rotation about an axis and a reflection in a plane perpendicular to that axis. Reflection and inversion are each special case of improper rotation. Any improper rotation is an affine transformation and, in cases that keep the coordinate origin fixed, a linear transformation.
-In 3 dimensions, improper rotation is equivalently defined as a combination of rotation about an axis and inversion in a point on the axis. For this reason it is also called a rotoinversion or rotary inversion. The two definitions are equivalent because rotation by an angle θ followed by reflection is the same transformation as rotation by θ + 180° followed by inversion (taking the point of inversion to be in the plane of reflection). In both definitions, the operations commute.
-Improper rotation, also called rotation-reflection: rotation about an axis by an angle θ, combined with reflection in the plane through the origin perpendicular to the axis. Rotation-reflection by θ = 360°/n for any positive integer n is denoted Sn (from the Schoenflies notation for the group Sn that it generates if n is even).Inversion is a special case of rotation-reflection (i = S2), as is reflection (σ = S1), so these operations are often considered to be improper rotations.
-Improper rotation operations An improper rotation involves two operation steps: a proper rotation followed by reflection through a plane perpendicular to the rotation axis. The improper rotation is represented by the symbol Sn where n is the order. Since the improper rotation is the combination of a proper rotation and a reflection, Sn will always exist whenever Cn and a perpendicular plane exist separately. S1 is usually denoted as σ, a reflection operation about a mirror plane. S2 is usually denoted as i, an inversion operation about an inversion center. When n is an even number  Snn=E, but when n is odd  Sn2n=E.
 A. An improper rotation is the combination of a rotation about an axis and reflection in a plane perpendicular to that axis, or inversion about a point on the axis. The order of the rotation and reflection does not matter, and the symmetry elements for improper rotation are the rotation axis, and either the mirror plane, the inversion point, or both, and a third plane.
 B. An improper rotation is the combination of a rotation about an axis and reflection in a plane perpendicular to that axis, or inversion about a point on the axis. The order of the rotation and reflection does not matter, and the symmetry elements for improper rotation are the rotation axis, and either the mirror plane, the inversion point, or both.
 C. An improper rotation is the combination of a rotation about an axis and reflection in a plane parallel to that axis, or inversion about a point on the axis. The order of the rotation and reflection does not matter, and the symmetry elements for improper rotation are the rotation axis, and either the mirror plane, the inversion point, or neither.
 D. An improper rotation is the combination of a rotation about an axis and reflection in a plane perpendicular to that axis, or inversion about a point on the axis. The order of the rotation and reflection does not matter, and the symmetry elements for improper rotation are the rotation axis, and either the mirror plane, the inversion point, or neither.
 E. An improper rotation is the combination of a rotation about an axis and reflection in a plane parallel to that axis, or inversion about a point on the axis. The order of the rotation and reflection does not matter, and the symmetry elements for improper rotation are the rotation axis, and either the mirror plane, the inversion point, or both. "
"What is power density in the context of energy systems, and how does it differ between renewable and non-renewable energy sources?","Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is power density in the context of energy systems, and how does it differ between renewable and non-renewable energy sources?
 Context: -Measured in W/m2 it describes the amount of power obtained per unit of Earth surface area used by a specific energy system, including all supporting infrastructure, manufacturing, mining of fuel (if applicable) and decommissioning., Fossil fuels and nuclear power are characterized by high power density which means large power can be drawn from power plants occupying relatively small area. Renewable energy sources have power density at least three orders of magnitude smaller and for the same energy output they need to occupy accordingly larger area, which has been already highlighted as a limiting factor of renewable energy in German Energiewende.The following table shows median surface power density of renewable and non-renewable energy sources.
-Power density is the amount of power (time rate of energy transfer) per unit volume.In energy transformers including batteries, fuel cells, motors, power supply units etc., power density refers to a volume, where it is often called volume power density, expressed as W/m3.
In reciprocating internal combustion engines, power density (power per swept volume or brake horsepower per cubic centimeter) is an important metric, based on the internal capacity of the engine, not its external size.
-Surface power density is an important factor in comparison of industrial energy sources. The concept was popularised by geographer Vaclav Smil. The term is usually shortened to ""power density"" in the relevant literature, which can lead to confusion with homonymous or related terms.
-In physics and engineering, surface power density is power per unit area.
-Wind Power Density (WPD) is a quantitative measure of wind energy available at any location. It is the mean annual power available per square meter of swept area of a turbine, and is calculated for different heights above ground. Calculation of wind power density includes the effect of wind velocity and air density.Wind turbines are classified by the wind speed they are designed for, from class I to class III, with A to C referring to the turbulence intensity of the wind.
 A. Power density is a measure of the amount of power obtained per unit of Earth surface area used by a specific energy system, including all supporting infrastructure, manufacturing, mining of fuel (if applicable) and decommissioning. Both renewable and non-renewable energy sources have similar power density, which means that the same amount of power can be obtained from power plants occupying similar areas.
 B. Power density is a measure of the amount of power obtained per unit of Earth surface area used by a specific energy system, including all supporting infrastructure, manufacturing, mining of fuel (if applicable) and decommissioning. Fossil fuels and nuclear power have high power density, which means large power can be drawn from power plants occupying relatively small areas. Renewable energy sources have power density at least three orders of magnitude smaller and, for the same energy output, they need to occupy accordingly larger areas.
 C. Power density is a measure of the amount of power obtained per unit of Earth surface area used by a specific energy system, including all supporting infrastructure, manufacturing, mining of fuel (if applicable) and decommissioning. Renewable energy sources have higher power density than non-renewable energy sources, which means that they can produce more power from power plants occupying smaller areas.
 D. Power density is a measure of the amount of power obtained per unit of Earth surface area used by a specific energy system, including all supporting infrastructure, manufacturing, mining of fuel (if applicable) and decommissioning. Fossil fuels and nuclear power have low power density, which means that they need to occupy larger areas to produce the same amount of power as renewable energy sources.
 E. Power density is a measure of the amount of power obtained per unit of Earth surface area used by a specific energy system, including all supporting infrastructure, manufacturing, mining of fuel (if applicable) and decommissioning. Both renewable and non-renewable energy sources have low power density, which means that they need to occupy larger areas to produce the same amount of power. "
What is Modified Newtonian Dynamics (MOND)?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is Modified Newtonian Dynamics (MOND)?
 Context: -Modified Newtonian dynamics (MOND) is a hypothesis that proposes a modification of Newton's law of universal gravitation to account for observed properties of galaxies. It is an alternative to the hypothesis of dark matter in terms of explaining why galaxies do not appear to obey the currently understood laws of physics.
-MOND Modified Newtonian Dynamics (MOND) is a relatively modern proposal to explain the galaxy rotation problem based on a variation of Newton's Second Law of Dynamics at low accelerations. This would produce a large-scale variation of Newton's universal theory of gravity. A modification of Newton's theory would also imply a modification of general relativistic cosmology in as much as Newtonian cosmology is the limit of Friedman cosmology. While almost all astrophysicists today reject MOND in favor of dark matter, a small number of researchers continue to enhance it, recently incorporating Brans–Dicke theories into treatments that attempt to account for cosmological observations.
-MOND is a phenomenological modification of the Newtonian acceleration law. In Newtonian gravity theory, the gravitational acceleration in the spherically symmetric, static field of a point mass  M at distance  r from the source can be written as a=−GMr2, where  G is Newton's constant of gravitation. The corresponding force acting on a test mass  m is F=ma.
To account for the anomalous rotation curves of spiral galaxies, Milgrom proposed a modification of this force law in the form F=μ(aa0)ma, where  μ(x) is an arbitrary function subject to the following conditions: μ(x)={1|x|≫1x|x|≪1 In this form, MOND is not a complete theory: for instance, it violates the law of momentum conservation.
However, such conservation laws are automatically satisfied for physical theories that are derived using an action principle. This led Bekenstein to a first, nonrelativistic generalization of MOND. This theory, called AQUAL (for A QUAdratic Lagrangian) is based on the Lagrangian L=−a028πGf(|∇Φ|2a02)−ρΦ, where  Φ is the Newtonian gravitational potential,  ρ is the mass density, and  f(y) is a dimensionless function.
In the case of a spherically symmetric, static gravitational field, this Lagrangian reproduces the MOND acceleration law after the substitutions  a=−∇Φ and  μ(y)=df(y)/dy are made.
-MOND is an example of a class of theories known as modified gravity, and is an alternative to the hypothesis that the dynamics of galaxies are determined by massive, invisible dark matter halos. Since Milgrom's original proposal, proponents of MOND have claimed to successfully predict a variety of galactic phenomena that they state are difficult to understand as consequences of dark matter.Though MOND explains the anomalously great rotational velocities of galaxies at their perimeters, it does not fully explain the velocity dispersions of individual galaxies within galaxy clusters. MOND reduces the discrepancy between the velocity dispersions and clusters' observed missing baryonic mass from a factor of around 10 to a factor of about 2. However, the residual discrepancy cannot be accounted for by MOND, requiring that other explanations close the gap such as the presence of as-yet undetected missing baryonic matter.The accurate measurement of the speed of gravitational waves compared to the speed of light in 2017 ruled out a certain class of modified gravity theories but concluded that other MOND theories that dispense with the need for dark matter remained viable. Two years later, theories put forth by Constantinos Skordis and Tom Zlosnik were consistent with gravitational waves that always travel at the speed of light. Later still in 2021, Skordis and Zlosnik developed a subclass of their theory called ""RMOND"", for ""relativistic MOND"", which had ""been shown to reproduce in great detail the main observations in cosmology, including the cosmic-microwave-background power spectrum, and the matter structure power spectrum."" 
-Relativistic MOND The original theory of MOND by Milgrom was developed in 1983 as an alternative to ""dark matter"". Departures from Newton's law of gravitation are governed by an acceleration scale, not a distance scale. MOND successfully explains the Tully–Fisher observation that the luminosity of a galaxy should scale as the fourth power of the rotation speed. It also explains why the rotation discrepancy in dwarf galaxies is particularly large.
 A. MOND is a theory that explains the behavior of light in the presence of strong gravitational fields. It is an alternative to the hypothesis of dark matter in terms of explaining why galaxies do not appear to obey the currently understood laws of physics.
 B. MOND is a hypothesis that proposes a modification of Einstein's theory of general relativity to account for observed properties of galaxies. It is an alternative to the hypothesis of dark matter in terms of explaining why galaxies do not appear to obey the currently understood laws of physics.
 C. MOND is a hypothesis that proposes a modification of Newton's law of universal gravitation to account for observed properties of galaxies. It is an alternative to the hypothesis of dark matter in terms of explaining why galaxies do not appear to obey the currently understood laws of physics.
 D. MOND is a hypothesis that proposes a modification of Coulomb's law to account for observed properties of galaxies. It is an alternative to the hypothesis of dark matter in terms of explaining why galaxies do not appear to obey the currently understood laws of physics.
 E. MOND is a theory that explains the behavior of subatomic particles in the presence of strong magnetic fields. It is an alternative to the hypothesis of dark energy in terms of explaining why subatomic particles do not appear to obey the currently understood laws of physics. "
What is linear frame dragging?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is linear frame dragging?
 Context: -Linear frame dragging is the similarly inevitable result of the general principle of relativity, applied to linear momentum. Although it arguably has equal theoretical legitimacy to the ""rotational"" effect, the difficulty of obtaining an experimental verification of the effect means that it receives much less discussion and is often omitted from articles on frame-dragging (but see Einstein, 1921).Static mass increase is a third effect noted by Einstein in the same paper. The effect is an increase in inertia of a body when other masses are placed nearby. While not strictly a frame dragging effect (the term frame dragging is not used by Einstein), it is demonstrated by Einstein that it derives from the same equation of general relativity. It is also a tiny effect that is difficult to confirm experimentally.
-This ""mutual"" effect, and the ability of an accelerated mass to warp lightbeam geometry and lightbeam-based coordinate systems, is referred to as frame-dragging.
Frame-dragging removes the usual distinction between accelerated frames (which show gravitational effects) and inertial frames (where the geometry is supposedly free from gravitational fields). When a forcibly-accelerated body physically ""drags"" a coordinate system, the problem becomes an exercise in warped spacetime for all observers.
-Rotational frame-dragging (the Lense–Thirring effect) appears in the general principle of relativity and similar theories in the vicinity of rotating massive objects. Under the Lense–Thirring effect, the frame of reference in which a clock ticks the fastest is one which is revolving around the object as viewed by a distant observer. This also means that light traveling in the direction of rotation of the object will move past the massive object faster than light moving against the rotation, as seen by a distant observer. It is now the best known frame-dragging effect, partly thanks to the Gravity Probe B experiment. Qualitatively, frame-dragging can be viewed as the gravitational analog of electromagnetic induction.
-Frame-dragging is an effect on spacetime, predicted by Albert Einstein's general theory of relativity, that is due to non-static stationary distributions of mass–energy. A stationary field is one that is in a steady state, but the masses causing that field may be non-static ⁠— rotating, for instance. More generally, the subject that deals with the effects caused by mass–energy currents is known as gravitoelectromagnetism, which is analogous to the magnetism of classical electromagnetism.
-Frame-dragging may be illustrated most readily using the Kerr metric, which describes the geometry of spacetime in the vicinity of a mass M rotating with angular momentum J, and Boyer–Lindquist coordinates (see the link for the transformation): sin sin sin 2⁡θρ2dϕdt where rs is the Schwarzschild radius rs=2GMc2 and where the following shorthand variables have been introduced for brevity α=JMc cos 2⁡θ Λ2=r2−rsr+α2 In the non-relativistic limit where M (or, equivalently, rs) goes to zero, the Kerr metric becomes the orthogonal metric for the oblate spheroidal coordinates sin 2⁡θdϕ2 We may rewrite the Kerr metric in the following form c2dτ2=(gtt−gtϕ2gϕϕ)dt2+grrdr2+gθθdθ2+gϕϕ(dϕ+gtϕgϕϕdt)2 This metric is equivalent to a co-rotating reference frame that is rotating with angular speed Ω that depends on both the radius r and the colatitude θ sin 2⁡θ In the plane of the equator this simplifies to: Ω=rsαcr3+α2r+rsα2 Thus, an inertial reference frame is entrained by the rotating central mass to participate in the latter's rotation; this is frame-dragging.
 A. Linear frame dragging is the effect of the general principle of relativity applied to the mass of a body when other masses are placed nearby. It is a tiny effect that is difficult to confirm experimentally and often omitted from articles on frame-dragging.
 B. Linear frame dragging is the effect of the general principle of relativity applied to rotational momentum, which is a large effect that is easily confirmed experimentally and often discussed in articles on frame-dragging.
 C. Linear frame dragging is the effect of the general principle of relativity applied to rotational momentum, which is similarly inevitable to the linear effect. It is a tiny effect that is difficult to confirm experimentally and often omitted from articles on frame-dragging.
 D. Linear frame dragging is the effect of the general principle of relativity applied to linear momentum, which is similarly inevitable to the rotational effect. It is a tiny effect that is difficult to confirm experimentally and often omitted from articles on frame-dragging.
 E. Linear frame dragging is the effect of the general principle of relativity applied to linear momentum, which is a large effect that is easily confirmed experimentally and often discussed in articles on frame-dragging. "
What is explicit symmetry breaking in theoretical physics?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is explicit symmetry breaking in theoretical physics?
 Context: -In theoretical physics, explicit symmetry breaking is the breaking of a symmetry of a theory by terms in its defining equations of motion (most typically, to the Lagrangian or the Hamiltonian) that do not respect the symmetry. Usually this term is used in situations where these symmetry-breaking terms are small, so that the symmetry is approximately respected by the theory. An example is the spectral line splitting in the Zeeman effect, due to a magnetic interaction perturbation in the Hamiltonian of the atoms involved.
-In explicit symmetry breaking (ESB), the equations of motion describing a system are variant under the broken symmetry. In Hamiltonian mechanics or Lagrangian Mechanics, this happens when there is at least one term in the Hamiltonian (or Lagrangian) that explicitly breaks the given symmetry.
-Explicit symmetry breaking differs from spontaneous symmetry breaking. In the latter, the defining equations respect the symmetry but the ground state (vacuum) of the theory breaks it.Explicit symmetry breaking is also associated with electromagnetic radiation. A system of accelerated charges results in electromagnetic radiation when the geometric symmetry of the electric field in free space is explicitly broken by the associated electrodynamic structure under time varying excitation of the given system. This is quite evident in an antenna where the electric lines of field curl around or have rotational geometry around the radiating terminals in contrast to linear geometric orientation within a pair of transmission lines which does not radiate even under time varying excitation.
-A common setting for explicit symmetry breaking is perturbation theory in quantum mechanics. The symmetry is evident in a base Hamiltonian  H0 . This  H0 is often an integrable Hamiltonian, admitting symmetries which in some sense make the Hamiltonian integrable. The base Hamiltonian might be chosen to provide a starting point close to the system being modelled.
-Vector symmetry description If all quarks had non-zero but equal masses, then this chiral symmetry is broken to the vector symmetry of the ""diagonal flavour group"" SU(Nf), which applies the same transformation to both helicities of the quarks. This reduction of symmetry is a form of explicit symmetry breaking. The strength of explicit symmetry breaking is controlled by the current quark masses in QCD.
 A. Explicit symmetry breaking is the breaking of a symmetry of a theory by terms in its defining equations of motion that do not respect the symmetry, always in situations where these symmetry-breaking terms are large, so that the symmetry is not respected by the theory.
 B. Explicit symmetry breaking is the breaking of a symmetry of a theory by terms in its defining equations of motion that do not respect the symmetry, usually in situations where these symmetry-breaking terms are small, so that the symmetry is approximately respected by the theory.
 C. Explicit symmetry breaking is the breaking of a symmetry of a theory by terms in its defining equations of motion that respect the symmetry, always in situations where these symmetry-breaking terms are small, so that the symmetry is approximately respected by the theory.
 D. Explicit symmetry breaking is the breaking of a symmetry of a theory by terms in its defining equations of motion that respect the symmetry, always in situations where these symmetry-breaking terms are large, so that the symmetry is not respected by the theory.
 E. Explicit symmetry breaking is the breaking of a symmetry of a theory by terms in its defining equations of motion that respect the symmetry, usually in situations where these symmetry-breaking terms are large, so that the symmetry is not respected by the theory. "
What is the role of the Higgs boson in the Standard Model?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the role of the Higgs boson in the Standard Model?
 Context: -The Higgs boson plays a unique role in the Standard Model, by explaining why the other elementary particles, except the photon and gluon, are massive. In particular, the Higgs boson explains why the photon has no mass, while the W and Z bosons are very heavy. Elementary-particle masses and the differences between electromagnetism (mediated by the photon) and the weak force (mediated by the W and Z bosons) are critical to many aspects of the structure of microscopic (and hence macroscopic) matter. In electroweak theory, the Higgs boson generates the masses of the leptons (electron, muon, and tau) and quarks. As the Higgs boson is massive, it must interact with itself.
-Higgs boson One of the most important goals of ATLAS was to investigate a missing piece of the Standard Model, the Higgs boson. The Higgs mechanism, which includes the Higgs boson, gives mass to elementary particles, leading to differences between the weak force and electromagnetism by giving the W and Z bosons mass while leaving the photon massless.
-The Higgs boson The Higgs boson, sometimes called the Higgs particle, is an elementary particle in the Standard Model of particle physics produced by the quantum excitation of the Higgs field, one of the fields in particle physics theory. In the Standard Model, the Higgs particle is a massive scalar boson with zero spin, even (positive) parity, no electric charge, and no colour charge, that couples to (interacts with) mass. It is also very unstable, decaying into other particles almost immediately.
-The Higgs boson, sometimes called the Higgs particle, is an elementary particle in the Standard Model of particle physics produced by the quantum excitation of the Higgs field, one of the fields in particle physics theory. In the Standard Model, the Higgs particle is a massive scalar boson with zero spin, even (positive) parity, no electric charge, and no colour charge that couples to (interacts with) mass. It is also very unstable, decaying into other particles almost immediately upon generation.
-Higgs boson As of 2011, the Higgs boson, the quantum of a field that is thought to provide particles with rest masses, remained the only particle of the Standard Model to be verified.
 A. The Higgs boson is responsible for giving mass to the photon and gluon in the Standard Model.
 B. The Higgs boson has no role in the Standard Model.
 C. The Higgs boson is responsible for giving mass to all the elementary particles in the Standard Model.
 D. The Higgs boson is responsible for giving mass to all the elementary particles, except the photon and gluon, in the Standard Model.
 E. The Higgs boson is responsible for giving mass to all the composite particles in the Standard Model. "
What is Lorentz symmetry or Lorentz invariance in relativistic physics?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is Lorentz symmetry or Lorentz invariance in relativistic physics?
 Context: -In relativistic physics, Lorentz symmetry or Lorentz invariance, named after the Dutch physicist Hendrik Lorentz, is an equivalence of observation or observational symmetry due to special relativity implying that the laws of physics stay the same for all observers that are moving with respect to one another within an inertial frame. It has also been described as ""the feature of nature that says experimental results are independent of the orientation or the boost velocity of the laboratory through space"".Lorentz covariance, a related concept, is a property of the underlying spacetime manifold. Lorentz covariance has two distinct, but closely related meanings: A physical quantity is said to be Lorentz covariant if it transforms under a given representation of the Lorentz group. According to the representation theory of the Lorentz group, these quantities are built out of scalars, four-vectors, four-tensors, and spinors. In particular, a Lorentz covariant scalar (e.g., the space-time interval) remains the same under Lorentz transformations and is said to be a Lorentz invariant (i.e., they transform under the trivial representation).
-For example, the following laws, equations, and theories respect Lorentz symmetry: The kinematical laws of special relativity Maxwell's field equations in the theory of electromagnetism The Dirac equation in the theory of the electron The Standard Model of particle physicsThe Lorentz group expresses the fundamental symmetry of space and time of all known fundamental laws of nature. In small enough regions of spacetime where gravitational variances are negligible, physical laws are Lorentz invariant in the same manner as special relativity.
-Lorentz invariance measures the universal features in hypothetical loop quantum gravity universes. The various hypothetical multiverse loop quantum gravity universe design models could have various general covariant principle results.
Because loop quantum gravity models universes, space gravity theories are contenders to build and answer unification theory; the Lorentz invariance helps grade the spread of universal features throughout a proposed multiverse in time.
-In physics, Lorentz transformations became known at the beginning of the 20th century, when it was discovered that they exhibit the symmetry of Maxwell's equations. Subsequently, they became fundamental to all of physics, because they formed the basis of special relativity in which they exhibit the symmetry of Minkowski spacetime, making the speed of light invariant between different inertial frames. They relate the spacetime coordinates of two arbitrary inertial frames of reference with constant relative speed v. In one frame, the position of an event is given by x,y,z and time t, while in the other frame the same event has coordinates x′,y′,z′ and t′.
-Similar to the approximate Lorentz symmetry of phonons in a lattice (where the speed of sound plays the role of the critical speed), the Lorentz symmetry of special relativity (with the speed of light as the critical speed in vacuum) is only a low-energy limit of the laws of physics, which involve new phenomena at some fundamental scale. Bare conventional ""elementary"" particles are not point-like field-theoretical objects at very small distance scales, and a nonzero fundamental length must be taken into account. Lorentz symmetry violation is governed by an energy-dependent parameter which tends to zero as momentum decreases. Such patterns require the existence of a privileged local inertial frame (the ""vacuum rest frame""). They can be tested, at least partially, by ultra-high energy cosmic ray experiments like the Pierre Auger Observatory.
 A. Lorentz symmetry or Lorentz invariance is a property of the underlying spacetime manifold that describes the feature of nature that says experimental results are independent of the orientation or the boost velocity of the laboratory through space.
 B. Lorentz symmetry or Lorentz invariance is a measure of the curvature of spacetime caused by the presence of massive objects, which describes the feature of nature that says experimental results are independent of the orientation or the boost velocity of the laboratory through space.
 C. Lorentz symmetry or Lorentz invariance is a physical quantity that transforms under a given representation of the Lorentz group, built out of scalars, four-vectors, four-tensors, and spinors.
 D. Lorentz symmetry or Lorentz invariance is a measure of the time dilation and length contraction effects predicted by special relativity, which states that the laws of physics stay the same for all observers that are moving with respect to one another within an inertial frame.
 E. Lorentz symmetry or Lorentz invariance is an equivalence of observation or observational symmetry due to special relativity implying that the laws of physics stay the same for all observers that are moving with respect to one another within an inertial frame. "
What is the significance of Baryon Acoustic Oscillations (BAOs) in the study of the universe?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the significance of Baryon Acoustic Oscillations (BAOs) in the study of the universe?
 Context: -Sky surveys and baryon acoustic oscillations Baryon acoustic oscillations (BAO) are fluctuations in the density of the visible baryonic matter (normal matter) of the universe on large scales. These are predicted to arise in the Lambda-CDM model due to acoustic oscillations in the photon–baryon fluid of the early universe, and can be observed in the cosmic microwave background angular power spectrum. BAOs set up a preferred length scale for baryons. As the dark matter and baryons clumped together after recombination, the effect is much weaker in the galaxy distribution in the nearby universe, but is detectable as a subtle (≈1 percent) preference for pairs of galaxies to be separated by 147 Mpc, compared to those separated by 130–160 Mpc. This feature was predicted theoretically in the 1990s and then discovered in 2005, in two large galaxy redshift surveys, the Sloan Digital Sky Survey and the 2dF Galaxy Redshift Survey. Combining the CMB observations with BAO measurements from galaxy redshift surveys provides a precise estimate of the Hubble constant and the average matter density in the Universe. The results support the Lambda-CDM model.
-In cosmology, baryon acoustic oscillations (BAO) are fluctuations in the density of the visible baryonic matter (normal matter) of the universe, caused by acoustic density waves in the primordial plasma of the early universe. In the same way that supernovae provide a ""standard candle"" for astronomical observations, BAO matter clustering provides a ""standard ruler"" for length scale in cosmology.  The length of this standard ruler is given by the maximum distance the acoustic waves could travel in the primordial plasma before the plasma cooled to the point where it became neutral atoms (the epoch of recombination), which stopped the expansion of the plasma density waves, ""freezing"" them into place. The length of this standard ruler (≈490 million light years in today's universe) can be measured by looking at the large scale structure of matter using astronomical surveys. BAO measurements help cosmologists understand more about the nature of dark energy (which causes the accelerating expansion of the universe) by constraining cosmological parameters.
-Another class of physical distance indicator is the standard ruler. In 2008, galaxy diameters have been proposed as a possible standard ruler for cosmological parameter determination. More recently the physical scale imprinted by baryon acoustic oscillations (BAO) in the early universe has been used.  In the early universe (before recombination) the baryons and photons scatter off each other, and form a tightly-coupled fluid that can support sound waves. The waves are sourced by primordial density perturbations, and travel at speed that can be predicted from the baryon density and other cosmological parameters. The total distance that these sound waves can travel before recombination determines a fixed scale, which simply expands with the universe after recombination. BAO therefore provide a standard ruler that can be measured in galaxy surveys from the effect of baryons on the clustering of galaxies. The method requires an extensive galaxy survey in order to make this scale visible, but has been measured with percent-level precision (see baryon acoustic oscillations). The scale does depend on cosmological parameters like the baryon and matter densities, and the number of neutrinos, so distances based on BAO are more dependent on cosmological model than those based on local measurements.
-Baryon acoustic oscillations The signature of baryon acoustic oscillations (BAO) can be observed in the distribution of tracers of the matter density field and used to measure the expansion history of the Universe. BAO can also be measured using purely photometric data, though at less significance. DES team observation samples consists of 7 million galaxies distributed over a footprint of 4100 deg2 with 0.6 < zphoto < 1.1 and a typical redshift uncertainty of 0.03(1+z). From their statistics, they combine the likelihoods derived from angular correlations and spherical harmonics to constrain the ratio of comoving angular diameter distance  0.835 18.92 0.51 at the effective redshift of our sample to the sound horizon scale at the drag epoch.
-DESI will measure the expansion history of the universe using the baryon acoustic oscillations (BAO) imprinted in the clustering of galaxies, quasars, and the intergalactic medium. The BAO technique is a robust way to extract cosmological distance information from the clustering of matter and galaxies. It relies only on very large-scale structure and it does so in a manner that enables scientists to separate the acoustic peak of the BAO signature from uncertainties in most systematic errors in the data. BAO was identified in the 2006 Dark Energy Task Force report as one of the key methods for studying dark energy. In May 2014, the High-Energy Physics Advisory Panel, a federal advisory committee, commissioned by the US Department of Energy (DOE) and the National Science Foundation (NSF) endorsed DESI.
 A. BAOs establish a preferred length scale for baryons, which can be used to detect a subtle preference for pairs of galaxies to be separated by 147 Mpc, compared to those separated by 130-160 Mpc.
 B. BAOs help to determine the average temperature of the Universe by measuring the temperature of the cosmic microwave background radiation.
 C. BAOs provide a way to measure the time it takes for a signal to reach its destination compared to the time it takes for background noise to dissipate.
 D. BAOs can be used to make a two-dimensional map of the galaxy distribution in the Universe.
 E. BAOs are used to measure the speed of light in the Universe. "
What can be inferred about the electronic entropy of insulators and metals based on their densities of states at the Fermi level?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What can be inferred about the electronic entropy of insulators and metals based on their densities of states at the Fermi level?
 Context: -Insulators have zero density of states at the Fermi level due to their band gaps. Thus, the density of states-based electronic entropy is essentially zero in these systems.
-Metals have non-zero density of states at the Fermi level. Metals with free-electron-like band structures (e.g. alkali metals, alkaline earth metals, Cu, and Al) generally exhibit relatively low density of states at the Fermi level, and therefore exhibit fairly low electronic entropies. Transition metals, wherein the flat d-bands lie close to the Fermi level, generally exhibit much larger electronic entropies than the free-electron like metals.
-Useful approximation It is useful to recognize that the only states within ~±kBT of the Fermi level contribute significantly to the entropy. Other states are either fully occupied, f = 1, or completely unoccupied, f = 0. In either case, these states do not contribute to the entropy. If one assumes that the density of states is constant within ±kBT of the Fermi level, one can derive that the electron heat capacity, equal to: CV=T(∂S∂T)T,V=π23kB2Tn(EF) where n(EF) is the density of states (number of levels per unit energy) at the Fermi level. Several other approximations can be made, but they all indicate that the electronic entropy should, to first order, be proportional to the temperature and the density of states at the Fermi level. As the density of states at the Fermi level varies widely between systems, this approximation is a reasonable heuristic for inferring when it may be necessary to include electronic entropy in the thermodynamic description of a system; only systems with large densities of states at the Fermi level should exhibit non-negligible electronic entropy (where large may be approximately defined as n(EF) ≥ (k2BT)−1).
-Typical values Metals Under the free electron model, the electrons in a metal can be considered to form a uniform Fermi gas. The number density  N/V of conduction electrons in metals ranges between approximately 1028 and 1029 electrons per m3, which is also the typical density of atoms in ordinary solid matter. This number density produces a Fermi energy of the order: where me is the electron rest mass. This Fermi energy corresponds to a Fermi temperature of the order of 106 kelvins, much higher than the temperature of the sun surface. Any metal will boil before reaching this temperature under atmospheric pressure. Thus for any practical purpose, a metal can be considered as a Fermi gas at zero temperature as a first approximation (normal temperatures are small compared to TF).
-Electronic entropy is the entropy of a system attributable to electrons' probabilistic occupation of states. This entropy can take a number of forms. The first form can be termed a density of states based entropy. The Fermi–Dirac distribution implies that each eigenstate of a system, i, is occupied with a certain probability, pi. As the entropy is given by a sum over the probabilities of occupation of those states, there is an entropy associated with the occupation of the various electronic states. In most molecular systems, the energy spacing between the highest occupied molecular orbital and the lowest unoccupied molecular orbital is usually large, and thus the probabilities associated with the occupation of the excited states are small. Therefore, the electronic entropy in molecular systems can safely be neglected. Electronic entropy is thus most relevant for the thermodynamics of condensed phases, where the density of states at the Fermi level can be quite large, and the electronic entropy can thus contribute substantially to thermodynamic behavior. A second form of electronic entropy can be attributed to the configurational entropy associated with localized electrons and holes. This entropy is similar in form to the configurational entropy associated with the mixing of atoms on a lattice.
 A. Insulators and metals have zero density of states at the Fermi level, and therefore, their density of states-based electronic entropy is essentially zero.
 B. Insulators have zero density of states at the Fermi level, and therefore, their density of states-based electronic entropy is essentially zero. Metals have non-zero density of states at the Fermi level, and thus, their electronic entropy should be proportional to the temperature and density of states at the Fermi level.
 C. Insulators have non-zero density of states at the Fermi level, and therefore, their density of states-based electronic entropy is proportional to the temperature and density of states at the Fermi level. Metals have zero density of states at the Fermi level, and thus, their electronic entropy is essentially zero.
 D. Insulators and metals have varying densities of states at the Fermi level, and thus, their electronic entropy may or may not be proportional to the temperature and density of states at the Fermi level.
 E. Insulators and metals have non-zero density of states at the Fermi level, and thus, their electronic entropy should be proportional to the temperature and density of states at the Fermi level. "
What are permutation-inversion groups?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What are permutation-inversion groups?
 Context: -Symmetry operations, point groups and permutation-inversion groups A molecule at equilibrium in a certain electronic state usually has some geometrical symmetry. This symmetry is described by a certain point group which consists of operations (called symmetry operations) that produce a spatial orientation of the molecule that is indistinguishable from the starting configuration. There are five types of point group symmetry operation: identity, rotation, reflection, inversion and improper rotation or rotation-reflection. Common to all symmetry operations is that the geometrical center-point of the molecule does not change its position; hence the name point group. One can determine the elements of the point group for a particular molecule by considering the geometrical symmetry of its molecular model. However, when one uses a point group, the elements are not to be interpreted in the same way. Instead the elements rotate and/or reflect the vibronic (vibration-electronic) coordinates and these elements commute with the vibronic Hamiltonian. The point group is used to classify by symmetry the vibronic eigenstates. The symmetry classification of the rotational levels, the eigenstates of the full (rovibronic nuclear spin) Hamiltonian, requires the use of the appropriate permutation-inversion group as introduced by Longuet-Higgins. See the Section Inversion symmetry and nuclear permutation symmetry below. The elements of permutation-inversion groups commute with the full molecular Hamiltonian. In addition to point groups, there exists another kind of group important in crystallography, where translation in 3-D also needs to be taken care of. They are known as space groups.
-As discussed above in the section Point groups and permutation-inversion groups, point groups are useful for classifying the vibrational and electronic states of rigid molecules (sometimes called semi-rigid molecules) which undergo only small oscillations about a single equilibrium geometry. Longuet-Higgins introduced a more general type of symmetry group suitable not only for classifying the vibrational and electronic states of rigid molecules but also for classifying their rotational and nuclear spin states. Further, such groups can be used to classify the states of non-rigid (or fluxional) molecules that tunnel between equivalent geometries (called versions) and to allow for the distorting effects of molecular rotation. These groups are known as permutation-inversion groups, because the symmetry operations in them are energetically feasible permutations of identical nuclei, or inversion with respect to the center of mass (the parity operation), or a combination of the two.
-In mathematics, a permutation group is a group G whose elements are permutations of a given set M and whose group operation is the composition of permutations in G (which are thought of as bijective functions from the set M to itself). The group of all permutations of a set M is the symmetric group of M, often written as Sym(M). The term permutation group thus means a subgroup of the symmetric group. If M = {1, 2, ..., n} then Sym(M) is usually denoted by Sn, and may be called the symmetric group on n letters.
-A subgroup of a symmetric group is called a permutation group.
-One can determine the symmetry operations of the point group for a particular molecule by considering the geometrical symmetry of its molecular model. However, when one uses a point group to classify molecular states, the operations in it are not to be interpreted in the same way. Instead the operations are interpreted as rotating and/or reflecting the vibronic (vibration-electronic) coordinates and these operations commute with the vibronic Hamiltonian. They are ""symmetry operations"" for that vibronic Hamiltonian. The point group is used to classify by symmetry the vibronic eigenstates of a rigid molecule. The symmetry classification of the rotational levels, the eigenstates of the full (rotation-vibration-electronic) Hamiltonian, requires the use of the appropriate permutation-inversion group as introduced by Longuet-Higgins. Point groups describe the geometrical symmetry of a molecule whereas permutation-inversion groups describe the energy-invariant symmetry.
 A. Permutation-inversion groups are groups of symmetry operations that are energetically feasible inversions of identical nuclei or rotation with respect to the center of mass.
 B. Permutation-inversion groups are groups of symmetry operations that are energetically feasible inversions of identical nuclei or rotation with respect to the center of mass, or a combination of both.
 C. Permutation-inversion groups are groups of symmetry operations that are energetically feasible rotations of the entire molecule about the C3 axis.
 D. Permutation-inversion groups are groups of symmetry operations that are energetically feasible inversions of the entire molecule about the C3 axis.
 E. Permutation-inversion groups are groups of symmetry operations that are energetically feasible permutations of identical nuclei or inversion with respect to the center of mass, or a combination of both. "
What is the relationship between dielectric loss and the transparency of a material?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the relationship between dielectric loss and the transparency of a material?
 Context: -Absorption and reflection Some materials cause absorption of electromagnetic waves, preventing it from reaching the receiver, in other cases, particularly with metallic or conductive materials reflection occurs. This can cause dead zones where no reception is available. Aluminium foiled thermal isolation in modern homes can easily reduce indoor mobile signals by 10 dB frequently leading to complaints about the bad reception of long-distance rural cell signals.
-If the object is transparent, then the light waves are passed on to neighboring atoms through the bulk of the material and re-emitted on the opposite side of the object. Such frequencies of light waves are said to be transmitted.
Transparency in insulators An object may be not transparent either because it reflects the incoming light or because it absorbs the incoming light. Almost all solids reflect a part and absorb a part of the incoming light.
-Attenuation causes the reflected wave to decrease in power as distance from the reflective material increases. As the power of the reflective wave decreases compared to the power of the incident wave, interference also decreases. And as interference decreases, so does the phase difference between sound pressure and particle velocity. At a large enough distance from the reflective material, there is no interference left anymore. At this distance one can speak of the far field.
-The change in permittivity occurs because of the disruption in the atomic structure of the materials. That is, the changes are due to the breaking of bonds and re-bonding within the atomic structure of the amorphous or crystalline structures. This modification in turn modifies the carrier traps within the band structure, reducing them, and hence ensuing the decrement in the permittivityThis contrasts with the photorefractive effect where the change is induced by the alteration in the electron distribution due to the photon-absorption.
-Influence of wavelength The refractive index of a given sample varies with wavelength for all materials. This dispersion relation is nonlinear and is characteristic for every material. In the visible range, a decrease of the refractive index comes with increasing wavelength. In glass prisms very little absorption is observable. In the infrared wavelength range several absorption maxima and fluctuations in the refractive index appear. To guarantee a high quality measurement with an accuracy of up to 0.00002 in the refractive index the wavelength has to be determined correctly. Therefore, in modern refractometers the wavelength is tuned to a bandwidth of +/-0.2 nm to ensure correct results for samples with different dispersions.
 A. Dielectric loss in a material can cause refraction, which can decrease the material's transparency at higher frequencies.
 B. Dielectric loss in a material can cause absorption, which can reduce the material's transparency at higher frequencies.
 C. Dielectric loss in a material can cause reflection, which can increase the material's transparency at higher frequencies.
 D. Dielectric loss in a material has no effect on the material's transparency at any frequency.
 E. Dielectric loss in a material can cause scattering, which can increase the material's transparency at higher frequencies. "
What is the purpose of measuring the Larmor precession fields at about 100 microtesla with highly sensitive superconducting quantum interference devices (SQUIDs) in ultra-low field MRI?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the purpose of measuring the Larmor precession fields at about 100 microtesla with highly sensitive superconducting quantum interference devices (SQUIDs) in ultra-low field MRI?
 Context: -T1 and T2 Each tissue returns to its equilibrium state after excitation by the independent relaxation processes of T1 (spin-lattice; that is, magnetization in the same direction as the static magnetic field) and T2 (spin-spin; transverse to the static magnetic field).
To create a T1-weighted image, magnetization is allowed to recover before measuring the MR signal by changing the repetition time (TR). This image weighting is useful for assessing the cerebral cortex, identifying fatty tissue, characterizing focal liver lesions, and in general, obtaining morphological information, as well as for post-contrast imaging.
To create a T2-weighted image, magnetization is allowed to decay before measuring the MR signal by changing the echo time (TE). This image weighting is useful for detecting edema and inflammation, revealing white matter lesions, and assessing zonal anatomy in the prostate and uterus.
-T1 and T2 Each tissue returns to its equilibrium state after excitation by the independent relaxation processes of T1 (spin-lattice; that is, magnetization in the same direction as the static magnetic field) and T2 (spin-spin; transverse to the static magnetic field).
To create a T1-weighted image, magnetization is allowed to recover before measuring the MR signal by changing the repetition time (TR). This image weighting is useful for assessing the cerebral cortex, identifying fatty tissue, characterizing focal liver lesions, and in general, obtaining morphological information, as well as for post-contrast imaging.
To create a T2-weighted image, magnetization is allowed to decay before measuring the MR signal by changing the echo time (TE). This image weighting is useful for detecting edema and inflammation, revealing white matter lesions, and assessing zonal anatomy in the prostate and uterus.
The standard display of MR images is to represent fluid characteristics in black-and-white images, where different tissues turn out as follows: 
-In MRI scanners, sections of the body are exposed to a strong magnetic field causing primarily the hydrogen nuclei (""spins"") of water in tissues to be polarized in the direction of the magnetic field. An intense radiofrequency pulse is applied that tips the magnetization generated by the hydrogen nuclei in the direction of the receiver coil where the spin polarization can be detected. Random molecular rotational oscillations matching the resonance frequency of the nuclear spins provide the ""relaxation"" mechanisms that bring the net magnetization back to its equilibrium position in alignment with the applied magnetic field. The magnitude of the spin polarization detected by the receiver is used to form the MR image but decays with a characteristic time constant known as the T1 relaxation time. Water protons in different tissues have different T1 values, which is one of the main sources of contrast in MR images. A contrast agent usually shortens, but in some instances increases, the value of T1 of nearby water protons thereby altering the contrast in the image.
-In MRI, the static magnetic field is augmented by a field gradient coil to vary across the scanned region, so that different spatial locations become associated with different precession frequencies. Only those regions where the field is such that the precession frequencies match the RF frequency will experience excitation. Usually, these field gradients are modulated to sweep across the region to be scanned, and it is the almost infinite variety of RF and gradient pulse sequences that gives MRI its versatility. Change of field gradient spreads the responding FID signal in the frequency domain, but this can be recovered and measured by a refocusing gradient (to create a so-called ""gradient echo""), or by a radio frequency pulse (to create a so-called ""spin-echo""), or in digital post-processing of the spread signal. The whole process can be repeated when some T1-relaxation has occurred and the thermal equilibrium of the spins has been more or less restored. The repetition time (TR) is the time between two successive excitations of the same slice.Typically, in soft tissues T1 is around one second while T2 and T*2 are a few tens of milliseconds. However, these values can vary widely between different tissues, as well as between different external magnetic fields. This behavior is one factor giving MRI its tremendous soft tissue contrast.
-T2*-weighted imaging is built from the basic physics of magnetic resonance imaging where there is spin–spin relaxation, that is, the transverse component of the magnetization vector exponentially decays towards its equilibrium value. It is characterized by the spin–spin relaxation time, known as T2. In an idealized system, all nuclei in a given chemical environment, in a magnetic field, relax with the same frequency. However, in real systems, there are minor differences in chemical environment which can lead to a distribution of resonance frequencies around the ideal. Over time, this distribution can lead to a dispersion of the tight distribution of magnetic spin vectors, and loss of signal (Free Induction Decay). In fact, for most magnetic resonance experiments, this ""relaxation"" dominates. This results in dephasing.
 A. To measure the magnetization in the same direction as the static magnetic field in T1 relaxation.
 B. To create a T1-weighted image that is useful for assessing the cerebral cortex, identifying fatty tissue, and characterizing focal liver lesions.
 C. To obtain sufficient signal quality in the microtesla-to-millitesla range, where MRI has been demonstrated recently.
 D. To measure the independent relaxation processes of T1 and T2 in each tissue after excitation.
 E. To change the repetition time (TR) and obtain morphological information in post-contrast imaging. "
What is the difference between illuminance and luminance?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the difference between illuminance and luminance?
 Context: -Illuminance Illuminance is a measure of how much luminous flux is spread over a given area. One can think of luminous flux (with the unit lumen) as a measure of the total ""amount"" of visible light present, and the illuminance as a measure of the intensity of illumination on a surface. A given amount of light will illuminate a surface more dimly if it is spread over a larger area, so illuminance is inversely proportional to area when the luminous flux is held constant.  One lux is equal to one lumen per square metre: 1 lx = 1 lm/m2 = 1 cd·sr/m2.A flux of 1000 lumens, spread uniformly over an area of 1 square metre, lights up that square metre with an illuminance of 1000 lux. However, the same 1000 lumens spread out over 10 square metres produces a dimmer illuminance of only 100 lux.
-Luminance is a photometric measure of the luminous intensity per unit area of light travelling in a given direction. It describes the amount of light that passes through, is emitted from, or is reflected from a particular area, and falls within a given solid angle.  The procedure for conversion from spectral radiance to luminance is standardized by the CIE and ISO.Brightness is the term for the subjective impression of the objective luminance measurement standard (see Objectivity (science) § Objectivity in measurement for the importance of this contrast).
-In photometry, illuminance is the total luminous flux incident on a surface, per unit area. It is a measure of how much the incident light illuminates the surface, wavelength-weighted by the luminosity function to correlate with human brightness perception. Similarly, luminous emittance is the luminous flux per unit area emitted from a surface. Luminous emittance is also known as luminous exitance.In SI units illuminance is measured in lux (lx), or equivalently in lumens per square metre (lm·m−2). Luminous exitance is measured in lm·m−2 only, not lux. In the CGS system, the unit of illuminance is the phot, which is equal to 10000 lux. The foot-candle is a non-metric unit of illuminance that is used in photography.Illuminance was formerly often called brightness, but this leads to confusion with other uses of the word, such as to mean luminance. ""Brightness"" should never be used for quantitative description, but only for nonquantitative references to physiological sensations and perceptions of light.
-The United States Federal Trade Commission (FTC) has assigned an unconventional meaning to brightness when applied to lamps. When appearing on light bulb packages, brightness means luminous flux, while in other contexts it means luminance. Luminous flux is the total amount of light coming from a source, such as a lighting device. Luminance, the original meaning of brightness, is the amount of light per solid angle coming from an area, such as the sky. The table below shows the standard ways of indicating the amount of light.
-Brightness: The amount of light emitted from the display. It is sometimes synonymous with the term luminance, which is defined as the amount of light per area and is measured in SI units as candela per square meter.
 A. Illuminance is the amount of light absorbed by a surface per unit area, while luminance is the amount of light reflected by a surface per unit area.
 B. Illuminance is the amount of light falling on a surface per unit area, while luminance is the amount of light emitted by a source per unit area.
 C. Illuminance is the amount of light concentrated into a smaller area, while luminance is the amount of light filling a larger solid angle.
 D. Illuminance is the amount of light emitted by a source per unit area, while luminance is the amount of light falling on a surface per unit area.
 E. Illuminance is the amount of light reflected by a surface per unit area, while luminance is the amount of light absorbed by a surface per unit area. "
What is a magnetic monopole in particle physics?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is a magnetic monopole in particle physics?
 Context: -In particle physics, a magnetic monopole is a hypothetical elementary particle that is an isolated magnet with only one magnetic pole (a north pole without a south pole or vice versa). A magnetic monopole would have a net north or south ""magnetic charge"". Modern interest in the concept stems from particle theories, notably the grand unified and superstring theories, which predict their existence. The known elementary particles that have electric charge are electric monopoles.
-The pole model usually treats magnetic charge as a mathematical abstraction, rather than a physical property of particles. However, a magnetic monopole is a hypothetical particle (or class of particles) that physically has only one magnetic pole (either a north pole or a south pole). In other words, it would possess a ""magnetic charge"" analogous to an electric charge. Magnetic field lines would start or end on magnetic monopoles, so if they exist, they would give exceptions to the rule that magnetic field lines neither start nor end. Some theories (such as Grand Unified Theories) have predicted the existence of magnetic monopoles, but so far, none have been observed.
-Magnetic monopoles Since a bar magnet gets its ferromagnetism from electrons distributed evenly throughout the bar, when a bar magnet is cut in half, each of the resulting pieces is a smaller bar magnet. Even though a magnet is said to have a north pole and a south pole, these two poles cannot be separated from each other. A monopole—if such a thing exists—would be a new and fundamentally different kind of magnetic object. It would act as an isolated north pole, not attached to a south pole, or vice versa. Monopoles would carry ""magnetic charge"" analogous to electric charge. Despite systematic searches since 1931, as of 2010, they have never been observed, and could very well not exist.Nevertheless, some theoretical physics models predict the existence of these magnetic monopoles. Paul Dirac observed in 1931 that, because electricity and magnetism show a certain symmetry, just as quantum theory predicts that individual positive or negative electric charges can be observed without the opposing charge, isolated South or North magnetic poles should be observable. Using quantum theory Dirac showed that if magnetic monopoles exist, then one could explain the quantization of electric charge—that is, why the observed elementary particles carry charges that are multiples of the charge of the electron.
-In physics, a neutral particle is a particle without an electric charge, such as a neutron.  The term neutral particles should not be confused with truly neutral particles, the subclass of neutral particles that are also identical to their own antiparticles.
-For many magnets the first non-zero term is the magnetic dipole moment. (To date, no isolated magnetic monopoles have been experimentally detected.) A magnetic dipole is the limit of either a current loop or a pair of poles as the dimensions of the source are reduced to zero while keeping the moment constant. As long as these limits only apply to fields far from the sources, they are equivalent. However, the two models give different predictions for the internal field (see below).
 A. A hypothetical elementary particle that is an isolated electric charge with both positive and negative poles.
 B. A hypothetical elementary particle that is an isolated magnet with no magnetic poles.
 C. A hypothetical elementary particle that is an isolated electric charge with only one electric pole, either a positive pole or a negative pole.
 D. A hypothetical elementary particle that is an isolated magnet with both north and south poles.
 E. A hypothetical elementary particle that is an isolated magnet with only one magnetic pole, either a north pole or a south pole. "
What is the difference between redshift due to the expansion of the universe and Doppler redshift?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the difference between redshift due to the expansion of the universe and Doppler redshift?
 Context: -Redshift is also used to measure the expansion of space, but this is not truly a Doppler effect. Rather, redshifting due to the expansion of space is known as cosmological redshift, which can be derived purely from the Robertson-Walker metric under the formalism of general relativity. Having said this, it also happens that there are detectable Doppler effects on cosmological scales, which, if incorrectly interpreted as cosmological in origin, lead to the observation of redshift-space distortions.
-There is a distinction between a redshift in cosmological context as compared to that witnessed when nearby objects exhibit a local Doppler-effect redshift. Rather than cosmological redshifts being a consequence of the relative velocities that are subject to the laws of special relativity (and thus subject to the rule that no two locally separated objects can have relative velocities with respect to each other faster than the speed of light), the photons instead increase in wavelength and redshift because of a global feature of the spacetime through which they are traveling. One interpretation of this effect is the idea that space itself is expanding. Due to the expansion increasing as distances increase, the distance between two remote galaxies can increase at more than 3×108 m/s, but this does not imply that the galaxies move faster than the speed of light at their present location (which is forbidden by Lorentz covariance).
-At the time of discovery and development of Hubble's law, it was acceptable to explain redshift phenomenon as a Doppler shift in the context of special relativity, and use the Doppler formula to associate redshift z with velocity. Today, in the context of general relativity, velocity between distant objects depends on the choice of coordinates used, and therefore, the redshift can be equally described as a Doppler shift or a cosmological shift (or gravitational) due to the expanding space, or some combination of the two.
-Distinguishing between cosmological and local effects For cosmological redshifts of z < 0.01 additional Doppler redshifts and blueshifts due to the peculiar motions of the galaxies relative to one another cause a wide scatter from the standard Hubble Law. The resulting situation can be illustrated by the Expanding Rubber Sheet Universe, a common cosmological analogy used to describe the expansion of space. If two objects are represented by ball bearings and spacetime by a stretching rubber sheet, the Doppler effect is caused by rolling the balls across the sheet to create peculiar motion. The cosmological redshift occurs when the ball bearings are stuck to the sheet and the sheet is stretched.The redshifts of galaxies include both a component related to recessional velocity from expansion of the universe, and a component related to peculiar motion (Doppler shift). The redshift due to expansion of the universe depends upon the recessional velocity in a fashion determined by the cosmological model chosen to describe the expansion of the universe, which is very different from how Doppler redshift depends upon local velocity. Describing the cosmological expansion origin of redshift, cosmologist Edward Robert Harrison said, ""Light leaves a galaxy, which is stationary in its local region of space, and is eventually received by observers who are stationary in their own local region of space. Between the galaxy and the observer, light travels through vast regions of expanding space. As a result, all wavelengths of the light are stretched by the expansion of space. It is as simple as that..."" Steven Weinberg clarified, ""The increase of wavelength from emission to absorption of light does not depend on the rate of change of a(t) [here a(t) is the Robertson–Walker scale factor] at the times of emission or absorption, but on the increase of a(t) in the whole period from emission to absorption.""Popular literature often uses the expression ""Doppler redshift"" instead of ""cosmological redshift"" to describe the redshift of galaxies dominated by the expansion of spacetime, but the cosmological redshift is not found using the relativistic Doppler equation which is instead characterized by special relativity; thus v ≥ c is impossible while, in contrast, v ≥ c is possible for cosmological redshifts because the space which separates the objects (for example, a quasar from the Earth) can expand faster than the speed of light. More mathematically, the viewpoint that ""distant galaxies are receding"" and the viewpoint that ""the space between galaxies is expanding"" are related by changing coordinate systems. Expressing this precisely requires working with the mathematics of the Friedmann–Robertson–Walker metric.If the universe were contracting instead of expanding, we would see distant galaxies blueshifted by an amount proportional to their distance instead of redshifted.
-Doppler effect If a source of the light is moving away from an observer, then redshift (z > 0) occurs; if the source moves towards the observer, then blueshift (z < 0) occurs. This is true for all electromagnetic waves and is explained by the Doppler effect. Consequently, this type of redshift is called the Doppler redshift. If the source moves away from the observer with velocity v, which is much less than the speed of light (v ≪ c), the redshift is given by z≈vc (since  γ≈1 )where c is the speed of light. In the classical Doppler effect, the frequency of the source is not modified, but the recessional motion causes the illusion of a lower frequency.
 A. Redshift due to the expansion of the universe depends on the rate of change of a(t) at the times of emission or absorption, while Doppler redshift depends on the increase of a(t) in the whole period from emission to absorption.
 B. Redshift due to the expansion of the universe depends on the local velocity of the object emitting the light, while Doppler redshift depends on the cosmological model chosen to describe the expansion of the universe.
 C. There is no difference between redshift due to the expansion of the universe and Doppler redshift.
 D. Redshift due to the expansion of the universe depends on the cosmological model chosen to describe the expansion of the universe, while Doppler redshift depends on the local velocity of the object emitting the light.
 E. Redshift due to the expansion of the universe depends on the increase of a(t) in the whole period from emission to absorption, while Doppler redshift depends on the rate of change of a(t) at the times of emission or absorption. "
What is the relationship between Coordinated Universal Time (UTC) and Universal Time (UT1)?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the relationship between Coordinated Universal Time (UTC) and Universal Time (UT1)?
 Context: -Universal Time (UT1) is the Earth Rotation Angle (ERA) linearly scaled to match historical definitions of mean solar time at 0° longitude. At high precision, Earth's rotation is irregular and is determined from the positions of distant quasars using long baseline interferometry, laser ranging of the Moon and artificial satellites, as well as GPS satellite orbits.
Coordinated Universal Time (UTC) is an atomic time scale designed to approximate UT1. UTC differs from TAI by an integral number of seconds. UTC is kept within 0.9 second of UT1 by the introduction of one-second steps to UTC, the ""leap second"". To date these steps (and difference ""TAI-UTC"") have always been positive.
The Global Positioning System broadcasts a very precise time signal worldwide, along with instructions for converting GPS time (GPST) to UTC. It was defined with a constant offset from TAI: GPST = TAI - 19 s. The GPS time standard is maintained independently but regularly synchronized with or from, UTC time.
-International Atomic Time (TAI) is the primary international time standard from which other time standards are calculated. Universal Time (UT1) is mean solar time at 0° longitude, computed from astronomical observations. It varies from TAI because of the irregularities in Earth's rotation. Coordinated Universal Time (UTC) is an atomic time scale designed to approximate Universal Time. UTC differs from TAI by an integral number of seconds. UTC is kept within 0.9 second of UT1 by the introduction of one-second steps to UTC, the ""leap second"". The Global Positioning System broadcasts a very precise time signal based on UTC time.
-Contrary to TAI, UTC is a discontinuous time scale. It is occasionally adjusted by leap seconds. Between these adjustments, it is composed of segments that are mapped to atomic time by a constant offset. From its beginning in 1961 through December 1971, the adjustments were made regularly in fractional leap seconds so that UTC approximated UT2. Afterward, these adjustments were made only in whole seconds to approximate UT1. This was a compromise arrangement in order to enable a publicly broadcast time scale. The less frequent whole-second adjustments meant that the time scale would be more stable and easier to synchronize internationally. The fact that it continues to approximate UT1 means that tasks such as navigation which require a source of Universal Time continue to be well served by the public broadcast of UTC.
-The TAI and UT1 time scales are precisely defined, the former by atomic clocks (and thus independent of Earth's rotation) and the latter by astronomical observations (that measure actual planetary rotation and thus the solar time at the Greenwich meridian). UTC (on which civil time is usually based) is a compromise, stepping with atomic seconds but periodically reset by a leap second to match UT1.
-A set of atomic clocks throughout the world keeps time by consensus: the clocks ""vote"" on the correct time, and all voting clocks are steered to agree with the consensus, which is called International Atomic Time (TAI). TAI ""ticks"" atomic seconds.: 207–218 Civil time is defined to agree with the rotation of the Earth. The international standard for timekeeping is Coordinated Universal Time (UTC). This time scale ""ticks"" the same atomic seconds as TAI, but inserts or omits leap seconds as necessary to correct for variations in the rate of rotation of the Earth.: 16–17, 207 A time scale in which the seconds are not exactly equal to atomic seconds is UT1, a form of universal time. UT1 is defined by the rotation of the Earth with respect to the Sun, and does not contain any leap seconds.: 68, 232  UT1 always differs from UTC by less than a second.
 A. UTC and Universal Time (UT1) are identical time scales that are used interchangeably in science and engineering.
 B. UTC is a time scale that is completely independent of Universal Time (UT1). UTC is kept within 0.9 second of UT1 by the introduction of one-second steps to UTC, the ""leap second"".
 C. UTC is an atomic time scale designed to approximate Universal Time (UT1), but it differs from UT1 by a non-integral number of seconds. UTC is kept within 0.9 second of UT1 by the introduction of one-second steps to UTC, the ""leap second"".
 D. UTC is an atomic time scale designed to approximate Universal Time (UT1), but it differs from UT1 by an integral number of seconds. UTC is kept within 0.9 second of UT1 by the introduction of one-second steps to UTC, the ""leap second"".
 E. UTC is a time scale that is based on the irregularities in Earth's rotation and is completely independent of Universal Time (UT1). "
What is the reason for heating metals to a temperature just above the upper critical temperature?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the reason for heating metals to a temperature just above the upper critical temperature?
 Context: -Metallic materials consist of a microstructure of small crystals called ""grains"" or crystallites. The nature of the grains (i.e. grain size and composition) is one of the most effective factors that can determine the overall mechanical behavior of the metal. Heat treatment provides an efficient way to manipulate the properties of the metal by controlling the rate of diffusion and the rate of cooling within the microstructure. Heat treating is often used to alter the mechanical properties of a metallic alloy, manipulating properties such as the hardness, strength, toughness, ductility, and elasticity.There are two mechanisms that may change an alloy's properties during heat treatment: the formation of martensite causes the crystals to deform intrinsically, and the diffusion mechanism causes changes in the homogeneity of the alloy.The crystal structure consists of atoms that are grouped in a very specific arrangement, called a lattice. In most elements, this order will rearrange itself, depending on conditions like temperature and pressure. This rearrangement called allotropy or polymorphism, may occur several times, at many different temperatures for a particular metal. In alloys, this rearrangement may cause an element that will not normally dissolve into the base metal to suddenly become soluble, while a reversal of the allotropy will make the elements either partially or completely insoluble.When in the soluble state, the process of diffusion causes the atoms of the dissolved element to spread out, attempting to form a homogenous distribution within the crystals of the base metal. If the alloy is cooled to an insoluble state, the atoms of the dissolved constituents (solutes) may migrate out of the solution. This type of diffusion, called precipitation, leads to nucleation, where the migrating atoms group together at the grain-boundaries. This forms a microstructure generally consisting of two or more distinct phases. For instance, steel that has been heated above the austenizing temperature (red to orange-hot, or around 1,500 °F (820 °C) to 1,600 °F (870 °C) depending on carbon content), and then cooled slowly, forms a laminated structure composed of alternating layers of ferrite and cementite, becoming soft pearlite. After heating the steel to the austenite phase and then quenching it in water, the microstructure will be in the martensitic phase. This is due to the fact that the steel will change from the austenite phase to the martensite phase after quenching. Some pearlite or ferrite may be present if the quench did not rapidly cool off all the steel.Unlike iron-based alloys, most heat-treatable alloys do not experience a ferrite transformation. In these alloys, the nucleation at the grain-boundaries often reinforces the structure of the crystal matrix. These metals harden by precipitation. Typically a slow process, depending on temperature, this is often referred to as ""age hardening"".Many metals and non-metals exhibit a martensite transformation when cooled quickly (with external media like oil, polymer, water, etc.). When a metal is cooled very quickly, the insoluble atoms may not be able to migrate out of the solution in time. This is called a ""diffusionless transformation."" When the crystal matrix changes to its low-temperature arrangement, the atoms of the solute become trapped within the lattice. The trapped atoms prevent the crystal matrix from completely changing into its low-temperature allotrope, creating shearing stresses within the lattice. When some alloys are cooled quickly, such as steel, the martensite transformation hardens the metal, while in others, like aluminum, the alloy becomes softer.
-Because a smaller grain size usually enhances mechanical properties, such as toughness, shear strength and tensile strength, these metals are often heated to a temperature that is just above the upper critical temperature, in order to prevent the grains of solution from growing too large. For instance, when steel is heated above the upper critical-temperature, small grains of austenite form. These grow larger as the temperature is increased. When cooled very quickly, during a martensite transformation, the austenite grain-size directly affects the martensitic grain-size. Larger grains have large grain-boundaries, which serve as weak spots in the structure. The grain size is usually controlled to reduce the probability of breakage.The diffusion transformation is very time-dependent. Cooling a metal will usually suppress the precipitation to a much lower temperature. Austenite, for example, usually only exists above the upper critical temperature. However, if the austenite is cooled quickly enough, the transformation may be suppressed for hundreds of degrees below the lower critical temperature. Such austenite is highly unstable and, if given enough time, will precipitate into various microstructures of ferrite and cementite. The cooling rate can be used to control the rate of grain growth or can even be used to produce partially martensitic microstructures. However, the martensite transformation is time-independent. If the alloy is cooled to the martensite transformation (Ms) temperature before other microstructures can fully form, the transformation will usually occur at just under the speed of sound.When austenite is cooled slow enough that a martensite transformation does not occur, the austenite grain size will have an effect on the rate of nucleation, but it is generally temperature and the rate of cooling that controls the grain size and microstructure. When austenite is cooled extremely slowly, it will form large ferrite crystals filled with spherical inclusions of cementite. This microstructure is referred to as ""sphereoidite"". If cooled a little faster, then coarse pearlite will form. Even faster, and fine pearlite will form. If cooled even faster, bainite will form. Similarly, these microstructures will also form, if cooled to a specific temperature and then held there for a certain time.Most non-ferrous alloys are also heated in order to form a solution. Most often, these are then cooled very quickly to produce a martensite transformation, putting the solution into a supersaturated state. The alloy, being in a much softer state, may then be cold worked. This causes work hardening that increases the strength and hardness of the alloy. Moreover, the defects caused by plastic deformation tend to speed up precipitation, increasing the hardness beyond what is normal for the alloy. Even if not cold worked, the solutes in these alloys will usually precipitate, although the process may take much longer. Sometimes these metals are then heated to a temperature that is below the lower critical (A1) temperature, preventing recrystallization, in order to speed-up the precipitation.
-Initial grain size affects the critical temperature. Grain boundaries are good sites for nuclei to form. Since an increase in grain size results in fewer boundaries this results in a decrease in the nucleation rate and hence an increase in the recrystallization temperature Deformation affects the final grain size. Increasing the deformation, or reducing the deformation temperature, increases the rate of nucleation faster than it increases the rate of growth. As a result, the final grain size is reduced by increased deformation.
-In materials science, grain growth is the increase in size of grains (crystallites) in a material at high temperature. This occurs when recovery and recrystallisation are complete and further reduction in the internal energy can only be achieved by reducing the total area of grain boundary. The term is commonly used in metallurgy but is also used in reference to ceramics and minerals. The behaviors of grain growth is analogous to the coarsening behaviors of grains, which implied that both of grain growth and coarsening may be dominated by the same physical mechanism.
-Grain Boundary Precipitation In superalloys strengthened by metal carbides, increasingly large carbide particles form preferentially at grain boundaries, preventing grain boundary sliding at high temperatures. This leads to an increase in the yield strength, and thus a yield strength anomaly.
 A. To prevent the grains of solution from growing too large, which decreases mechanical properties such as toughness, shear strength, and tensile strength.
 B. To increase the size of the grains of solution, which enhances mechanical properties such as toughness, shear strength, and tensile strength.
 C. To prevent the grains of solution from growing too large, which enhances mechanical properties such as toughness, shear strength, and tensile strength.
 D. To prevent the grains of solution from growing too small, which enhances mechanical properties such as toughness, shear strength, and tensile strength.
 E. To increase the size of the grains of solution, which decreases mechanical properties such as toughness, shear strength, and tensile strength. "
What is the cause of the observed change in the periods of moons orbiting a distant planet when measured from Earth?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the cause of the observed change in the periods of moons orbiting a distant planet when measured from Earth?
 Context: -Ole Christensen Rømer used an astronomical measurement to make the first quantitative estimate of the speed of light in the year 1676. When measured from Earth, the periods of moons orbiting a distant planet are shorter when the Earth is approaching the planet than when the Earth is receding from it. The distance travelled by light from the planet (or its moon) to Earth is shorter when the Earth is at the point in its orbit that is closest to its planet than when the Earth is at the farthest point in its orbit, the difference in distance being the diameter of the Earth's orbit around the Sun. The observed change in the moon's orbital period is caused by the difference in the time it takes light to traverse the shorter or longer distance. Rømer observed this effect for Jupiter's innermost major moon Io and deduced that light takes 22 minutes to cross the diameter of the Earth's orbit.
-The instantaneous lunar distance is constantly changing. The actual distance between the Moon and Earth can change as quickly as 75 meters per second, or more than 1,000 km (620 mi) in just 6 hours, due to its non-circular orbit. There are other effects that also influence the lunar distance. Some factors include: Perturbations and eccentricity The distance to the Moon can be measured to an accuracy of 2 mm over a 1-hour sampling period, which results in an overall uncertainty of a decimeter for the semi-major axis. However, due to its elliptical orbit with varying eccentricity, the instantaneous distance varies with monthly periodicity. Furthermore, the distance is perturbed by the gravitational effects of various astronomical bodies – most significantly the Sun and less so Venus and Jupiter. Other forces responsible for minute perturbations are: gravitational attraction to other planets in the Solar System and to asteroids; tidal forces; and relativistic effects. The effect of radiation pressure from the Sun contributes an amount of ±3.6 mm to the lunar distance.Although the instantaneous uncertainty is a few millimeters, the measured lunar distance can change by more than 21,000 km (13,000 mi) from the mean value throughout a typical month. These perturbations are well understood and the lunar distance can be accurately modeled over thousands of years.
-Orbit eccentricity causes the planet/Sun distance to change during the year: The higher is the eccentricity, the higher is the change; Sun rays intensity in various moments of the year changes as the planet/Sun distance changes. Earth eccentricity is very low (0.0167 in a scale from 0 to 1.0000), hence it does not affect so much temperature changes during the year.
-The instantaneous Earth–Moon distance, or distance to the Moon, is the distance from the center of Earth to the center of the Moon. Lunar distance (LD or  Δ ⊕ L {\textstyle \Delta _{\oplus L}} ), or Earth–Moon characteristic distance, is a unit of measure in astronomy. More technically, it is the semi-major axis of the geocentric lunar orbit. The lunar distance is on average approximately 385,000 km (239,000 mi), or 1.28 light-seconds; this is roughly 30 times Earth's diameter or 9.5 times Earth's circumference. A little less than 400 lunar distances make up an astronomical unit.
-Since Earth's diameter is 3.7 times the Moon's, the length of the planet's umbra is correspondingly 3.7 times that of the lunar umbra: roughly 1,400,000 km (870,000 mi).
 A. The difference in the size of the planet's moons when the Earth is at the point in its orbit that is closest to its planet than when the Earth is at the farthest point in its orbit, the difference in distance being the diameter of the Earth's orbit around the Sun.
 B. The difference in the speed of light when the Earth is at the point in its orbit that is closest to its planet than when the Earth is at the farthest point in its orbit, the difference in distance being the diameter of the Earth's orbit around the Sun.
 C. The difference in distance travelled by light from the planet (or its moon) to Earth when the Earth is at the point in its orbit that is closest to its planet than when the Earth is at the farthest point in its orbit, the difference in distance being the diameter of the Earth's orbit around the Sun.
 D. The difference in the atmospheric conditions of the planet when the Earth is at the point in its orbit that is closest to its planet than when the Earth is at the farthest point in its orbit, the difference in distance being the diameter of the Earth's orbit around the Sun.
 E. The difference in the gravitational pull of the planet on its moons when the Earth is at the point in its orbit that is closest to its planet than when the Earth is at the farthest point in its orbit, the difference in distance being the diameter of the Earth's orbit around the Sun. "
What is the origin of the radio emission observed from supernova remnants?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the origin of the radio emission observed from supernova remnants?
 Context: -Supernova remnants A supernova occurs when a high-mass star reaches the end of its life. When nuclear fusion in the core of the star stops, the star collapses. The gas falling inward either rebounds or gets so strongly heated that it expands outwards from the core, thus causing the star to explode. The expanding shell of gas forms a supernova remnant, a special diffuse nebula. Although much of the optical and X-ray emission from supernova remnants originates from ionized gas, a great amount of the radio emission is a form of non-thermal emission called synchrotron emission. This emission originates from high-velocity electrons oscillating within magnetic fields.
-In Supernovae When a star explodes in a supernova, the fastest ejecta move at semi-relativistic speeds approximately 10% the speed of light. This blast wave gyrates electrons in ambient magnetic fields and generates synchrotron emission, revealing the radius of the blast wave at the location of the emission. Synchrotron emission can also reveal the strength of the magnetic field at the front of the shock wave, as well as the circumstellar density it encounters, but strongly depends on the choice of energy partition between the magnetic field, proton kinetic energy, and electron kinetic energy. Radio synchrotron emission has allowed astronomers to shed light on mass loss and stellar winds that occur just prior to stellar death.
-Energy output Although supernovae are primarily known as luminous events, the electromagnetic radiation they release is almost a minor side-effect. Particularly in the case of core collapse supernovae, the emitted electromagnetic radiation is a tiny fraction of the total energy released during the event.There is a fundamental difference between the balance of energy production in the different types of supernova. In type Ia white dwarf detonations, most of the energy is directed into heavy element synthesis and the kinetic energy of the ejecta. In core collapse supernovae, the vast majority of the energy is directed into neutrino emission, and while some of this apparently powers the observed destruction, 99%+ of the neutrinos escape the star in the first few minutes following the start of the collapse.Standard type Ia supernovae derive their energy from a runaway nuclear fusion of a carbon-oxygen white dwarf. The details of the energetics are still not fully understood, but the result is the ejection of the entire mass of the original star at high kinetic energy. Around half a solar mass of that mass is 56Ni generated from silicon burning. 56Ni is radioactive and decays into 56Co by beta plus decay (with a half life of six days) and gamma rays. 56Co itself decays by the beta plus (positron) path with a half life of 77 days into stable 56Fe. These two processes are responsible for the electromagnetic radiation from type Ia supernovae. In combination with the changing transparency of the ejected material, they produce the rapidly declining light curve.Core collapse supernovae are on average visually fainter than type Ia supernovae, but the total energy released is far higher, as outlined in the following table.
-The majority of these emissions are thought to be produced by a mechanism called ""cyclotron maser instability"", which develops close to the auroral regions. Electrons moving parallel to the magnetic field precipitate into the atmosphere while those with a sufficient perpendicular velocity are reflected by the converging magnetic field. This results in an unstable velocity distribution. This velocity distribution spontaneously generates radio waves at the local electron cyclotron frequency. The electrons involved in the generation of radio waves are probably those carrying currents from the poles of the planet to the magnetodisk. The intensity of Jovian radio emissions usually varies smoothly with time. However, there are short and powerful bursts (S bursts) of emission superimposed on the more gradual variations and which can outshine all other components. The total emitted power of the DAM component is about 100 GW, while the power of all other HOM/KOM components is about 10 GW. In comparison, the total power of Earth's radio emissions is about 0.1 GW.Jupiter's radio and particle emissions are strongly modulated by its rotation, which makes the planet somewhat similar to a pulsar. This periodical modulation is probably related to asymmetries in the Jovian magnetosphere, which are caused by the tilt of the magnetic moment with respect to the rotational axis as well as by high-latitude magnetic anomalies. The physics governing Jupiter's radio emissions is similar to that of radio pulsars. They differ only in the scale, and Jupiter can be considered a very small radio pulsar too. In addition, Jupiter's radio emissions strongly depend on solar wind pressure and, hence, on solar activity.In addition to relatively long-wavelength radiation, Jupiter also emits synchrotron radiation (also known as the Jovian decimetric radiation or DIM radiation) with frequencies in the range of 0.1–15 GHz (wavelength from 3 m to 2 cm),. These emissions are from relativistic electrons trapped in the inner radiation belts of the planet. The energy of the electrons that contribute to the DIM emissions is from 0.1 to 100 MeV, while the leading contribution comes from the electrons with energy in the range 1–20 MeV. This radiation is well understood and was used since the beginning of the 1960s to study the structure of the planet's magnetic field and radiation belts. The particles in the radiation belts originate in the outer magnetosphere and are adiabatically accelerated, when they are transported to the inner magnetosphere. However, this requires a source population of moderately high energy electrons (>> 1 keV), and the origin of this population is not well understood.
-Other sources include: Sun Jupiter Sagittarius A, the Galactic Center of the Milky Way, with one portion Sagittarius A* thought to be a radio wave emitting supermassive black hole Active galactic nuclei and pulsars have jets of charged particles which emit synchrotron radiation Merging galaxy clusters often show diffuse radio emission Supernova remnants can also show diffuse radio emission; pulsars are a type of supernova remnant that shows highly synchronous emission.
 A. The radio emission from supernova remnants originates from the rebound of gas falling inward during the supernova explosion. This emission is a form of non-thermal emission called synchrotron emission.
 B. The radio emission from supernova remnants originates from high-velocity electrons oscillating within magnetic fields. This emission is a form of non-thermal emission called synchrotron emission.
 C. The radio emission from supernova remnants originates from the fusion of hydrogen and helium in the core of the star. This emission is a form of non-thermal emission called synchrotron emission.
 D. The radio emission from supernova remnants originates from the expansion of the shell of gas during the supernova explosion. This emission is a form of thermal emission called synchrotron emission.
 E. The radio emission from supernova remnants originates from the ionized gas present in the remnants. This emission is a form of thermal emission called synchrotron emission. "
What is the relationship between the Hamiltonians and eigenstates in supersymmetric quantum mechanics?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the relationship between the Hamiltonians and eigenstates in supersymmetric quantum mechanics?
 Context: -SUSY quantum mechanics involves pairs of Hamiltonians which share a particular mathematical relationship, which are called partner Hamiltonians. (The potential energy terms which occur in the Hamiltonians are then known as partner potentials.) An introductory theorem shows that for every eigenstate of one Hamiltonian, its partner Hamiltonian has a corresponding eigenstate with the same energy. This fact can be exploited to deduce many properties of the eigenstate spectrum. It is analogous to the original description of SUSY, which referred to bosons and fermions. We can imagine a ""bosonic Hamiltonian"", whose eigenstates are the various bosons of our theory. The SUSY partner of this Hamiltonian would be ""fermionic"", and its eigenstates would be the theory's fermions. Each boson would have a fermionic partner of equal energy.
-SUSY quantum mechanics involves pairs of Hamiltonians which share a particular mathematical relationship, which are called partner Hamiltonians. (The potential energy terms which occur in the Hamiltonians are then called partner potentials.) An introductory theorem shows that for every eigenstate of one Hamiltonian, its partner Hamiltonian has a corresponding eigenstate with the same energy (except possibly for zero energy eigenstates). This fact can be exploited to deduce many properties of the eigenstate spectrum. It is analogous to the original description of SUSY, which referred to bosons and fermions. We can imagine a ""bosonic Hamiltonian"", whose eigenstates are the various bosons of our theory. The SUSY partner of this Hamiltonian would be ""fermionic"", and its eigenstates would be the theory's fermions. Each boson would have a fermionic partner of equal energy—but, in the relativistic world, energy and mass are interchangeable, so we can just as easily say that the partner particles have equal mass.
-Mathematically, the relation of degeneracy with symmetry can be clarified as follows. Consider a symmetry operation associated with a unitary operator S. Under such an operation, the new Hamiltonian is related to the original Hamiltonian by a similarity transformation generated by the operator S, such that  H′=SHS−1=SHS† , since S is unitary. If the Hamiltonian remains unchanged under the transformation operation S, we have  SHS†=H SHS−1=H SH=HS [S,H]=0 Now, if  |α⟩ is an energy eigenstate, H|α⟩=E|α⟩ where E is the corresponding energy eigenvalue.
-In quantum mechanics, the Hamiltonian of a system is an operator corresponding to the total energy of that system, including both kinetic energy and potential energy. Its spectrum, the system's energy spectrum or its set of energy eigenvalues, is the set of possible outcomes obtainable from a measurement of the system's total energy. Due to its close relation to the energy spectrum and time-evolution of a system, it is of fundamental importance in most formulations of quantum theory.
-Notice that if a system is in an eigenstate of a given Hamiltonian, the system remains in that state.
 A. For every eigenstate of one Hamiltonian, its partner Hamiltonian has a corresponding eigenstate with the same energy.
 B. For every eigenstate of one Hamiltonian, its partner Hamiltonian has a corresponding eigenstate with a higher energy.
 C. For every eigenstate of one Hamiltonian, its partner Hamiltonian has a corresponding eigenstate with a different spin.
 D. For every eigenstate of one Hamiltonian, its partner Hamiltonian has a corresponding eigenstate with a different energy.
 E. For every eigenstate of one Hamiltonian, its partner Hamiltonian has a corresponding eigenstate with a lower energy. "
What is the proposed name for the field that is responsible for cosmic inflation and the metric expansion of space?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the proposed name for the field that is responsible for cosmic inflation and the metric expansion of space?
 Context: -Cosmology Inflaton There has been considerable scientific research on possible links between the Higgs field and the inflaton – a hypothetical field suggested as the explanation for the expansion of space during the first fraction of a second of the universe (known as the ""inflationary epoch""). Some theories suggest that a fundamental scalar field might be responsible for this phenomenon; the Higgs field is such a field, and its existence has led to papers analysing whether it could also be the inflaton responsible for this exponential expansion of the universe during the Big Bang. Such theories are highly tentative and face significant problems related to unitarity, but may be viable if combined with additional features such as large non-minimal coupling, a Brans–Dicke scalar, or other ""new"" physics, and they have received treatments suggesting that Higgs inflation models are still of interest theoretically.
-The inflaton field is a hypothetical scalar field which is conjectured to have driven cosmic inflation in the very early universe.
The field, originally postulated by Alan Guth, provides a mechanism by which a period of rapid expansion from 10−35 to 10−34 seconds after the initial expansion can be generated, forming a universe consistent with observed spatial isotropy and homogeneity.
-Scalar fields are hypothesized to have caused the high accelerated expansion of the early universe (inflation), helping to solve the horizon problem and giving a hypothetical reason for the non-vanishing cosmological constant of cosmology. Massless (i.e. long-ranged) scalar fields in this context are known as inflatons. Massive (i.e. short-ranged) scalar fields are proposed, too, using for example Higgs-like fields.
-Cosmic expansion is a key feature of Big Bang cosmology. It can be modeled mathematically with the Friedmann–Lemaître–Robertson–Walker metric, where it corresponds to an increase in the scale of the spatial part of the universe's spacetime metric (which governs the size and geometry of spacetime). Within this framework, stationary objects separate over time because space is expanding. However, this is not a generally covariant description but rather only a choice of coordinates. Contrary to common misconception, it is equally valid to adopt a description in which space does not expand and objects simply move apart under the influence of their mutual gravity. Although cosmic expansion is often framed as a consequence of general relativity, it is also predicted by Newtonian gravity.According to inflation theory, during the inflationary epoch about 10−32 of a second after the Big Bang, the universe suddenly expanded, and its volume increased by a factor of at least 1078 (an expansion of distance by a factor of at least 1026 in each of the three dimensions). This would be equivalent to expanding an object 1 nanometer (10−9 m, about half the width of a molecule of DNA) in length to one approximately 10.6 light years (about 1017 m or 62 trillion miles) long. Cosmic expansion subsequently decelerated down to much slower rates, until at around 9.8 billion years after the Big Bang (4 billion years ago) it began to gradually expand more quickly, and is still doing so. Physicists have postulated the existence of dark energy, appearing as a cosmological constant in the simplest gravitational models, as a way to explain this late-time acceleration. According to the simplest extrapolation of the currently favored cosmological model, the Lambda-CDM model, this acceleration becomes more dominant into the future.
-In physical cosmology, cosmic inflation, cosmological inflation, or just inflation, is a theory of exponential expansion of space in the early universe. The inflationary epoch is believed to have lasted from 10−36 seconds to between 10−33 and 10−32 seconds after the Big Bang. Following the inflationary period, the universe continued to expand, but at a slower rate. The acceleration of this expansion due to dark energy began after the universe was already over 7.7 billion years old (5.4 billion years ago).Inflation theory was developed in the late 1970s and early 80s, with notable contributions by several theoretical physicists, including Alexei Starobinsky at Landau Institute for Theoretical Physics, Alan Guth at Cornell University, and Andrei Linde at Lebedev Physical Institute. Alexei Starobinsky, Alan Guth, and Andrei Linde won the 2014 Kavli Prize ""for pioneering the theory of cosmic inflation"". It was developed further in the early 1980s. It explains the origin of the large-scale structure of the cosmos. Quantum fluctuations in the microscopic inflationary region, magnified to cosmic size, become the seeds for the growth of structure in the Universe (see galaxy formation and evolution and structure formation). Many physicists also believe that inflation explains why the universe appears to be the same in all directions (isotropic), why the cosmic microwave background radiation is distributed evenly, why the universe is flat, and why no magnetic monopoles have been observed.
 A. Inflaton
 B. Quanta
 C. Scalar
 D. Metric
 E. Conformal cyclic cosmology "
Which of the following statements accurately describes the characteristics of gravitational waves?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: Which of the following statements accurately describes the characteristics of gravitational waves?
 Context: -The effects of a passing gravitational wave, in an extremely exaggerated form, can be visualized by imagining a perfectly flat region of spacetime with a group of motionless test particles lying in a plane, e.g., the surface of a computer screen. As a gravitational wave passes through the particles along a line perpendicular to the plane of the particles, i.e., following the observer's line of vision into the screen, the particles will follow the distortion in spacetime, oscillating in a ""cruciform"" manner, as shown in the animations. The area enclosed by the test particles does not change and there is no motion along the direction of propagation.The oscillations depicted in the animation are exaggerated for the purpose of discussion – in reality a gravitational wave has a very small amplitude (as formulated in linearized gravity). However, they help illustrate the kind of oscillations associated with gravitational waves as produced by a pair of masses in a circular orbit. In this case the amplitude of the gravitational wave is constant, but its plane of polarization changes or rotates at twice the orbital rate, so the time-varying gravitational wave size, or 'periodic spacetime strain', exhibits a variation as shown in the animation. If the orbit of the masses is elliptical then the gravitational wave's amplitude also varies with time according to Einstein's quadrupole formula.As with other waves, there are a number of characteristics used to describe a gravitational wave: Amplitude: Usually denoted h, this is the size of the wave – the fraction of stretching or squeezing in the animation. The amplitude shown here is roughly h = 0.5 (or 50%). Gravitational waves passing through the Earth are many sextillion times weaker than this – h ≈ 10−20.
-Gravitational waves Predicted in 1916 by Albert Einstein, there are gravitational waves: ripples in the metric of spacetime that propagate at the speed of light. These are one of several analogies between weak-field gravity and electromagnetism in that, they are analogous to electromagnetic waves. On 11 February 2016, the Advanced LIGO team announced that they had directly detected gravitational waves from a pair of black holes merging.The simplest type of such a wave can be visualized by its action on a ring of freely floating particles. A sine wave propagating through such a ring towards the reader distorts the ring in a characteristic, rhythmic fashion (animated image to the right). Since Einstein's equations are non-linear, arbitrarily strong gravitational waves do not obey linear superposition, making their description difficult. However, linear approximations of gravitational waves are sufficiently accurate to describe the exceedingly weak waves that are expected to arrive here on Earth from far-off cosmic events, which typically result in relative distances increasing and decreasing by  10 21 or less. Data analysis methods routinely make use of the fact that these linearized waves can be Fourier decomposed.Some exact solutions describe gravitational waves without any approximation, e.g., a wave train traveling through empty space or Gowdy universes, varieties of an expanding cosmos filled with gravitational waves. But for gravitational waves produced in astrophysically relevant situations, such as the merger of two black holes, numerical methods are presently the only way to construct appropriate models.
-Speed: This is the speed at which a point on the wave (for example, a point of maximum stretch or squeeze) travels. For gravitational waves with small amplitudes, this wave speed is equal to the speed of light (c).The speed, wavelength, and frequency of a gravitational wave are related by the equation c = λf, just like the equation for a light wave. For example, the animations shown here oscillate roughly once every two seconds. This would correspond to a frequency of 0.5 Hz, and a wavelength of about 600 000 km, or 47 times the diameter of the Earth.
-The direct detection of gravitational waves is complicated by the extraordinarily small effect the waves produce on a detector. The amplitude of a spherical wave falls off as the inverse of the distance from the source. Thus, even waves from extreme systems such as merging binary black holes die out to a very small amplitude by the time they reach the Earth. Astrophysicists predicted that some gravitational waves passing the Earth might produce differential motion on the order 10−18 m in a LIGO-size instrument.
-Gravitational waves Gravitational waves, a direct consequence of Einstein's theory, are distortions of geometry that propagate at the speed of light, and can be thought of as ripples in spacetime. They should not be confused with the gravity waves of fluid dynamics, which are a different concept.
 A. Gravitational waves have an amplitude denoted by h, which represents the size of the wave. The amplitude varies with time according to Newton's quadrupole formula. Gravitational waves also have a frequency denoted by f, which is the frequency of the wave's oscillation, and a wavelength denoted by λ, which is the distance between points of minimum stretch or squeeze.
 B. Gravitational waves have an amplitude denoted by λ, which represents the distance between points of maximum stretch or squeeze. The amplitude varies with time according to Einstein's quadrupole formula. Gravitational waves also have a frequency denoted by h, which is the size of the wave, and a wavelength denoted by f, which is the frequency of the wave's oscillation.
 C. Gravitational waves have an amplitude denoted by h, which represents the size of the wave. The amplitude varies with time according to Einstein's quadrupole formula. Gravitational waves also have a frequency denoted by f, which is the frequency of the wave's oscillation, and a wavelength denoted by λ, which is the distance between points of maximum stretch or squeeze.
 D. Gravitational waves have an amplitude denoted by f, which represents the frequency of the wave's oscillation. The amplitude varies with time according to Einstein's quadrupole formula. Gravitational waves also have a frequency denoted by h, which is the size of the wave, and a wavelength denoted by λ, which is the distance between points of maximum stretch or squeeze.
 E. Gravitational waves have an amplitude denoted by f, which represents the frequency of the wave's oscillation. The amplitude varies with time according to Newton's quadrupole formula. Gravitational waves also have a frequency denoted by h, which is the size of the wave, and a wavelength denoted by λ, which is the distance between points of minimum stretch or squeeze. "
What is the difference between the coevolution of myrmecophytes and the mutualistic symbiosis of mycorrhiza?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the difference between the coevolution of myrmecophytes and the mutualistic symbiosis of mycorrhiza?
 Context: -Mutualism With Mycorrhizae The relationship between plants and mycorrhizal fungi is an example of mutualism because plants provides fungi with carbohydrates and mycorrhizal fungi help plants absorb more water and nutrients. Since mycorrhizal fungi increase plants' uptake of below-ground resources, plants who form a mutualistic relationship with fungi have stimulated shoot growth and a higher shoot to root ratio.
-Mycorrhizal fungi form a mutualistic relationship with the roots of most plant species. In such a relationship, both the plants themselves and those parts of the roots that host the fungi, are said to be mycorrhizal. Relatively few of the mycorrhizal relationships between plant species and fungi have been examined to date, but 95% of the plant families investigated are predominantly mycorrhizal either in the sense that most of their species associate beneficially with mycorrhizae, or are absolutely dependent on mycorrhizae. The Orchidaceae are notorious as a family in which the absence of the correct mycorrhizae is fatal even to germinating seeds.Recent research into ectomycorrhizal plants in boreal forests has indicated that mycorrhizal fungi and plants have a relationship that may be more complex than simply mutualistic. This relationship was noted when mycorrhizal fungi were unexpectedly found to be hoarding nitrogen from plant roots in times of nitrogen scarcity. Researchers argue that some mycorrhizae distribute nutrients based upon the environment with surrounding plants and other mycorrhizae. They go on to explain how this updated model could explain why mycorrhizae do not alleviate plant nitrogen limitation, and why plants can switch abruptly from a mixed strategy with both mycorrhizal and nonmycorrhizal roots to a purely mycorrhizal strategy as soil nitrogen availability declines. It has also been suggested that evolutionary and phylogenetic relationships can explain much more variation in the strength of mycorrhizal mutualisms than ecological factors.
-Coevolution Ecological interactions can be classified broadly into a host and an associate relationship. A host is any entity that harbours another that is called the associate. Relationships between species that are mutually or reciprocally beneficial are called mutualisms. Examples of mutualism include fungus-growing ants employing agricultural symbiosis, bacteria living in the guts of insects and other organisms, the fig wasp and yucca moth pollination complex, lichens with fungi and photosynthetic algae, and corals with photosynthetic algae. If there is a physical connection between host and associate, the relationship is called symbiosis. Approximately 60% of all plants, for example, have a symbiotic relationship with arbuscular mycorrhizal fungi living in their roots forming an exchange network of carbohydrates for mineral nutrients.Indirect mutualisms occur where the organisms live apart. For example, trees living in the equatorial regions of the planet supply oxygen into the atmosphere that sustains species living in distant polar regions of the planet. This relationship is called commensalism because many others receive the benefits of clean air at no cost or harm to trees supplying the oxygen. If the associate benefits while the host suffers, the relationship is called parasitism. Although parasites impose a cost to their host (e.g., via damage to their reproductive organs or propagules, denying the services of a beneficial partner), their net effect on host fitness is not necessarily negative and, thus, becomes difficult to forecast. Co-evolution is also driven by competition among species or among members of the same species under the banner of reciprocal antagonism, such as grasses competing for growth space. The Red Queen Hypothesis, for example, posits that parasites track down and specialize on the locally common genetic defense systems of its host that drives the evolution of sexual reproduction to diversify the genetic constituency of populations responding to the antagonistic pressure.
-Some plants, called legumes, can form simultaneous symbiotic relationships with both AM fungi and the nitrogen-fixing bacteria Rhizobia. In fact, both organisms trigger the same pathways in plants during early colonization, indicating that the two very different responses could share a common origin. While the bacteria can supply nitrogen, they cannot provide other benefits of AM fungi; AM actually enhances bacterial colonization, probably by supplying extra phosphorus for the formation of the bacterial habitat within the plant, and thus contributing indirectly to the plant's nitrogen status. It is not known if there is signaling between the two, or only between the plant and each microbe. There is almost certainly competition between the bacterial and fungal partners, whether directly or indirectly, due to the fact that both are dependent on the plant as their sole source of energy. The plant must strive to strike a delicate balance between the maintenance of both partners based on its nutrient status.
-Mycorrhizae – Mycorrhizae are similar to rhizobia in that they interact with plants at their roots. Whereas rhizobia are bacteria that fix nitrogen, mycorrhizae are fungi that bring nutrients to the plants in return for carbon. Mycorrhizas are also capable of improving water uptake and communicating to their hosts to resist to pathogens. Three main types of mycorrhizae exist:Arbuscula: found in non-woody and tropical plants Ectomycorrhiza: found in boreal and temperate forests Ericoid: found in species of the heathland.Digestive symbiotes – Digestive symbiotes are an example of an important trophic mutualism that does not occur between an autotroph and heterotroph. Bacteria known as ""extracellular symbionts"" live within the gastrointestinal tracts of vertebrates, where they aid in the digestion of food. The bacteria benefits by extracting substrates from the eaten food, while the animal’s assimilation is increased by being able to digest certain foods that its natural system cannot. (book) In addition, these bacteria create short-chain fatty acids (SCFA), providing the vertebrate with energy totaling up to anywhere from 29%-79% of the vertebrate’s maintenance energy depending on the species.
 A. Myrmecophytes coevolve with ants, providing them with a home and sometimes food, while the ants defend the plant from herbivores and competing plants. On the other hand, mycorrhiza is a mutualistic symbiosis between plants and fungi, where the fungi help the plants gain water and mineral nutrients from the soil, while the plant gives the fungi carbohydrates manufactured in photosynthesis.
 B. Myrmecophytes coevolve with ants, providing them with food, while the ants defend the plant from herbivores and competing plants. On the other hand, mycorrhiza is a mutualistic symbiosis between plants and fungi, where the fungi help the plants gain water and mineral nutrients from the soil, while the plant gives the fungi water and mineral nutrients.
 C. Myrmecophytes coevolve with butterflies, providing them with a home and sometimes food, while the butterflies defend the plant from herbivores and competing plants. On the other hand, mycorrhiza is a mutualistic symbiosis between plants and birds, where the birds help the plants gain water and mineral nutrients from the soil, while the plant gives the birds carbohydrates manufactured in photosynthesis.
 D. Myrmecophytes coevolve with birds, providing them with a home and sometimes food, while the birds defend the plant from herbivores and competing plants. On the other hand, mycorrhiza is a mutualistic symbiosis between plants and bacteria, where the bacteria help the plants gain water and mineral nutrients from the soil, while the plant gives the bacteria carbohydrates manufactured in photosynthesis.
 E. Myrmecophytes coevolve with bees, providing them with a home and sometimes food, while the bees defend the plant from herbivores and competing plants. On the other hand, mycorrhiza is a mutualistic symbiosis between plants and insects, where the insects help the plants gain water and mineral nutrients from the soil, while the plant gives the insects carbohydrates manufactured in photosynthesis. "
What is the Kelvin-Helmholtz instability and how does it affect Earth's magnetosphere?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the Kelvin-Helmholtz instability and how does it affect Earth's magnetosphere?
 Context: -The Kelvin–Helmholtz instability (after Lord Kelvin and Hermann von Helmholtz) is a fluid instability that occurs when there is velocity shear in a single continuous fluid or a velocity difference across the interface between two fluids. Kelvin-Helmholtz instabilities are visible in the atmospheres of planets and moons, such as in cloud formations on Earth or the Red Spot on Jupiter, and the atmospheres of the Sun and other stars.
-The research characterised variances in formation of the interplanetary magnetic field (IMF) largely influenced by Kelvin–Helmholtz instability (which occur at the interface of two fluids) as a result of differences in thickness and numerous other characteristics of the boundary layer. Experts believe that this was the first occasion that the appearance of Kelvin–Helmholtz waves at the magnetopause had been displayed at high latitude downward orientation of the IMF. These waves are being seen in unforeseen places under solar wind conditions that were formerly believed to be undesired for their generation. These discoveries show how Earth's magnetosphere can be penetrated by solar particles under specific IMF circumstances. The findings are also relevant to studies of magnetospheric progressions around other planetary bodies. This study suggests that Kelvin–Helmholtz waves can be a somewhat common, and possibly constant, instrument for the entrance of solar wind into terrestrial magnetospheres under various IMF orientations.
-Earth's magnetosphere Over Earth's equator, the magnetic field lines become almost horizontal, then return to reconnect at high latitudes. However, at high altitudes, the magnetic field is significantly distorted by the solar wind and its solar magnetic field. On the dayside of Earth, the magnetic field is significantly compressed by the solar wind to a distance of approximately 65,000 kilometers (40,000 mi). Earth's bow shock is about 17 kilometers (11 mi) thick and located about 90,000 kilometers (56,000 mi) from Earth. The magnetopause exists at a distance of several hundred kilometers above Earth's surface. Earth's magnetopause has been compared to a sieve because it allows solar wind particles to enter. Kelvin–Helmholtz instabilities occur when large swirls of plasma travel along the edge of the magnetosphere at a different velocity from the magnetosphere, causing the plasma to slip past. This results in magnetic reconnection, and as the magnetic field lines break and reconnect, solar wind particles are able to enter the magnetosphere. On Earth's nightside, the magnetic field extends in the magnetotail, which lengthwise exceeds 6,300,000 kilometers (3,900,000 mi). Earth's magnetotail is the primary source of the polar aurora. Also, NASA scientists have suggested that Earth's magnetotail might cause ""dust storms"" on the Moon by creating a potential difference between the day side and the night side.
-Kelvin–Helmholtz instability The Kelvin–Helmholtz instability (KHI) is an application of hydrodynamic stability that can be seen in nature. It occurs when there are two fluids flowing at different velocities. The difference in velocity of the fluids causes a shear velocity at the interface of the two layers. The shear velocity of one fluid moving induces a shear stress on the other which, if greater than the restraining surface tension, then results in an instability along the interface between them. This motion causes the appearance of a series of overturning ocean waves, a characteristic of the Kelvin–Helmholtz instability. Indeed, the apparent ocean wave-like nature is an example of vortex formation, which are formed when a fluid is rotating about some axis, and is often associated with this phenomenon.
-If the density and velocity vary continuously in space (with the lighter layers uppermost, so that the fluid is RT-stable), the dynamics of the Kelvin-Helmholtz instability is described by the Taylor–Goldstein equation:  where  {\textstyle N={\sqrt {g/L_{\rho }}}} denotes the Brunt–Väisälä frequency, U is the horizontal parallel velocity, k is the wave number, c is the eigenvalue parameter of the problem,  ϕ~ is complex amplitude of the stream function. Its onset is given by the Richardson number  Ri . Typically the layer is unstable for  0.25 . These effects are common in cloud layers. The study of this instability is applicable in plasma physics, for example in inertial confinement fusion and the plasma–beryllium interface. In situations where there is a state of static stability, evident by heavier fluids found below than the lower fluid, the Rayleigh-Taylor instability can be ignored as the Kelvin–Helmholtz instability is sufficient given the conditions.Numerically, the Kelvin–Helmholtz instability is simulated in a temporal or a spatial approach. In the temporal approach, the flow is considered in a periodic (cyclic) box ""moving"" at mean speed (absolute instability). In the spatial approach, simulations mimic a lab experiment with natural inlet and outlet conditions (convective instability).
 A. The Kelvin-Helmholtz instability is a phenomenon that occurs when large swirls of plasma travel along the edge of the magnetosphere at a different velocity from the magnetosphere, causing the plasma to slip past. This results in magnetic reconnection, and as the magnetic field lines break and reconnect, solar wind particles are able to enter the magnetosphere.
 B. The Kelvin-Helmholtz instability is a phenomenon that occurs when the magnetosphere is compared to a sieve because it allows solar wind particles to enter.
 C. The Kelvin-Helmholtz instability is a phenomenon that occurs when Earth's bow shock is about 17 kilometers (11 mi) thick and located about 90,000 kilometers (56,000 mi) from Earth.
 D. The Kelvin-Helmholtz instability is a phenomenon that occurs when the magnetic field extends in the magnetotail on Earth's nightside, which lengthwise exceeds 6,300,000 kilometers (3,900,000 mi).
 E. The Kelvin-Helmholtz instability is a phenomenon that occurs when the magnetosphere is compressed by the solar wind to a distance of approximately 65,000 kilometers (40,000 mi) on the dayside of Earth. This results in the magnetopause existing at a distance of several hundred kilometers above Earth's surface. "
What is the significance of the high degree of fatty-acyl disorder in the thylakoid membranes of plants?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the significance of the high degree of fatty-acyl disorder in the thylakoid membranes of plants?
 Context: -Plant thylakoid membranes maintain high fluidity, even at relatively cold environmental temperatures, due to the abundance of 18-carbon fatty acyl chains with three double bonds, linolenic acid, as has been revealed by 13-C NMR studies.
-Membrane The thylakoid membrane is the site of the light-dependent reactions of photosynthesis with the photosynthetic pigments embedded directly in the membrane. It is an alternating pattern of dark and light bands measuring each 1 nanometre. The thylakoid lipid bilayer shares characteristic features with prokaryotic membranes and the inner chloroplast membrane. For example, acidic lipids can be found in thylakoid membranes, cyanobacteria and other photosynthetic bacteria and are involved in the functional integrity of the photosystems. The thylakoid membranes of higher plants are composed primarily of phospholipids and galactolipids that are asymmetrically arranged along and across the membranes. Thylakoid membranes are richer in galactolipids rather than phospholipids; also they predominantly consist of hexagonal phase II forming monogalacotosyl diglyceride lipid. Despite this unique composition, plant thylakoid membranes have been shown to assume largely lipid-bilayer dynamic organization. Lipids forming the thylakoid membranes, richest in high-fluidity linolenic acid are synthesized in a complex pathway involving exchange of lipid precursors between the endoplasmic reticulum and inner membrane of the plastid envelope and transported from the inner membrane to the thylakoids via vesicles.
-Phase transitions in biological systems Phase transitions play many important roles in biological systems. Examples include the lipid bilayer formation, the coil-globule transition in the process of protein folding and DNA melting, liquid crystal-like transitions in the process of DNA condensation, and cooperative ligand binding to DNA and proteins with the character of phase transition.In biological membranes, gel to liquid crystalline phase transitions play a critical role in physiological functioning of biomembranes. In gel phase, due to low fluidity of membrane lipid fatty-acyl chains, membrane proteins have restricted movement and thus are restrained in exercise of their physiological role. Plants depend critically on photosynthesis by chloroplast thylakoid membranes which are exposed cold environmental temperatures. Thylakoid membranes retain innate fluidity even at relatively low temperatures because of high degree of fatty-acyl disorder allowed by their high content of linolenic acid, 18-carbon chain with 3-double bonds. Gel-to-liquid crystalline phase transition temperature of biological membranes can be determined by many techniques including calorimetry, fluorescence, spin label electron paramagnetic resonance and NMR by recording measurements of the concerned parameter by at series of sample temperatures. A simple method for its determination from 13-C NMR line intensities has also been proposed.It has been proposed that some biological systems might lie near critical points. Examples include neural networks in the salamander retina, bird flocks gene expression networks in Drosophila, and protein folding. However, it is not clear whether or not alternative reasons could explain some of the phenomena supporting arguments for criticality. It has also been suggested that biological organisms share two key properties of phase transitions: the change of macroscopic behavior and the coherence of a system at a critical point. Phase transitions are prominent feature of motor behavior in biological systems. Spontaneous gait transitions, as well as fatigue-induced motor task disengagements, show typical critical behavior as an intimation of the sudden qualitative change of the previously stable motor behavioral pattern.
-A study of central linewidths of electron spin resonance spectra of thylakoid membranes and aqueous dispersions of their total extracted lipids, labeled with stearic acid spin label (having spin or doxyl moiety at 5,7,9,12,13,14 and 16th carbons, with reference to carbonyl group), reveals a fluidity gradient. Decreasing linewidth from 5th to 16th carbons represents increasing degree of motional freedom (fluidity gradient) from headgroup-side to methyl terminal in both native membranes and their aqueous lipid extract (a multilamellar liposomal structure, typical of lipid bilayer organization). This pattern points at similarity of lipid bilayer organization in both native membranes and liposomes. This observation is critical, as thylakoid membranes comprising largely galactolipids, contain only 10% phospholipid, unlike other biological membranes consisting largely of phospholipids. Proteins in chloroplast thylakoid membranes, apparently, restrict lipid fatty acyl chain segmental mobility from 9th to 16th carbons vis a vis their liposomal counterparts. Surprisingly, liposomal fatty acyl chains are more restricted at 5th and 7th carbon positions as compared at these positions in thylakoid membranes. This is explainable as due to motional restricting effect at these positions, because of steric hindrance by large chlorophyll headgroups, specially so, in liposomes. However, in native thylakoid membranes, chlorophylls are mainly complexed with proteins as light-harvesting complexes and may not largely be free to restrain lipid fluidity, as such.
-Cholesterol is normally found dispersed in varying degrees throughout cell membranes, in the irregular spaces between the hydrophobic tails of the membrane lipids, where it confers a stiffening and strengthening effect on the membrane. Additionally, the amount of cholesterol in biological membranes varies between organisms, cell types, and even in individual cells. Cholesterol, a major component of plasma membranes, regulates the fluidity of the overall membrane, meaning that cholesterol controls the amount of movement of the various cell membrane components based on its concentrations. In high temperatures, cholesterol inhibits the movement of phospholipid fatty acid chains, causing a reduced permeability to small molecules and reduced membrane fluidity. The opposite is true for the role of cholesterol in cooler temperatures. Cholesterol production, and thus concentration, is up-regulated (increased) in response to cold temperature. At cold temperatures, cholesterol interferes with fatty acid chain interactions. Acting as antifreeze, cholesterol maintains the fluidity of the membrane. Cholesterol is more abundant in cold-weather animals than warm-weather animals. In plants, which lack cholesterol, related compounds called sterols perform the same function as cholesterol.
 A. The high degree of fatty-acyl disorder in the thylakoid membranes of plants is responsible for the low fluidity of membrane lipid fatty-acyl chains in the gel phase.
 B. The high degree of fatty-acyl disorder in the thylakoid membranes of plants is responsible for the exposure of chloroplast thylakoid membranes to cold environmental temperatures.
 C. The high degree of fatty-acyl disorder in the thylakoid membranes of plants allows for innate fluidity even at relatively low temperatures.
 D. The high degree of fatty-acyl disorder in the thylakoid membranes of plants allows for a gel-to-liquid crystalline phase transition temperature to be determined by many techniques.
 E. The high degree of fatty-acyl disorder in the thylakoid membranes of plants restricts the movement of membrane proteins, thus hindering their physiological role. "
What is the explanation for the effective supersymmetry in quark-diquark models?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the explanation for the effective supersymmetry in quark-diquark models?
 Context: -Nonetheless, color-charged particles may combine to form color neutral composite particles called hadrons. A quark may pair up with an antiquark: the quark has a color and the antiquark has the corresponding anticolor. The color and anticolor cancel out, forming a color neutral meson. Alternatively, three quarks can exist together, one quark being ""red"", another ""blue"", another ""green"". These three colored quarks together form a color-neutral baryon. Symmetrically, three antiquarks with the colors ""antired"", ""antiblue"" and ""antigreen"" can form a color-neutral antibaryon.
-Quarks carry not only electric charge, but also charges such as color charge and weak isospin. Because of a phenomenon known as color confinement, a hadron cannot have a net color charge; that is, the total color charge of a particle has to be zero (""white""). A quark can have one of three ""colors"", dubbed ""red"", ""green"", and ""blue""; while an antiquark may be either ""anti-red"", ""anti-green"" or ""anti-blue"".For normal hadrons, a white color can thus be achieved in one of three ways:  A quark of one color with an antiquark of the corresponding anticolor, giving a meson with baryon number 0, Three quarks of different colors, giving a baryon with baryon number +1, Three antiquarks of different anticolors, giving an antibaryon with baryon number −1.The baryon number was defined long before the quark model was established, so rather than changing the definitions, particle physicists simply gave quarks one third the baryon number. Nowadays it might be more accurate to speak of the conservation of quark number.
-The quarks are bound together by the strong force, which acts in such a way as to cancel the colour charges within the particle. In a meson, this means a quark is partnered with an antiquark with an opposite colour charge – blue and antiblue, for example – while in a baryon, the three quarks have between them all three colour charges – red, blue, and green. In a pentaquark, the colours also need to cancel out, and the only feasible combination is to have one quark with one colour (e.g. red), one quark with a second colour (e.g. green), two quarks with the third colour (e.g. blue), and one antiquark to counteract the surplus colour (e.g. antiblue).The binding mechanism for pentaquarks is not yet clear. They may consist of five quarks tightly bound together, but it is also possible that they are more loosely bound and consist of a three-quark baryon and a two-quark meson interacting relatively weakly with each other via pion exchange (the same force that binds atomic nuclei) in a ""meson-baryon molecule"".
-According to the quark model, the properties of hadrons are primarily determined by their so-called valence quarks. For example, a proton is composed of two up quarks (each with electric charge ++2⁄3, for a total of +4⁄3 together) and one down quark (with electric charge −+1⁄3). Adding these together yields the proton charge of +1. Although quarks also carry color charge, hadrons must have zero total color charge because of a phenomenon called color confinement. That is, hadrons must be ""colorless"" or ""white"". The simplest ways for this to occur are with a quark of one color and an antiquark of the corresponding anticolor, or three quarks of different colors. Hadrons with the first arrangement are a type of meson, and those with the second arrangement are a type of baryon.
-All three colours mixed together, or any one of these colours and its complement (or negative), is ""colourless"" or ""white"" and has a net colour charge of zero. Due to a property of the strong interaction called colour confinement, free particles must have a colour charge of zero.  A baryon is composed of three quarks, which must be one each of red, green, and blue colours; likewise an antibaryon is composed of three antiquarks, one each of antired, antigreen and antiblue. A meson is made from one quark and one antiquark; the quark can be any colour, and the antiquark has the matching anticolor.
 A. Two different color charges close together appear as the corresponding anti-color under coarse resolution, which makes a diquark cluster viewed with coarse resolution effectively appear as an antiquark. Therefore, a baryon containing 3 valence quarks, of which two tend to cluster together as a diquark, behaves like a meson.
 B. Two different color charges close together appear as the corresponding color under coarse resolution, which makes a diquark cluster viewed with coarse resolution effectively appear as a quark. Therefore, a baryon containing 3 valence quarks, of which two tend to cluster together as a diquark, behaves like a baryon.
 C. Two different color charges close together appear as the corresponding color under fine resolution, which makes a diquark cluster viewed with fine resolution effectively appear as a quark. Therefore, a baryon containing 3 valence quarks, of which two tend to cluster together as a diquark, behaves like a baryon.
 D. Two different color charges close together appear as the corresponding anti-color under fine resolution, which makes a diquark cluster viewed with fine resolution effectively appear as an antiquark. Therefore, a baryon containing 3 valence quarks, of which two tend to cluster together as a diquark, behaves like a meson.
 E. Two different color charges close together appear as the corresponding anti-color under any resolution, which makes a diquark cluster viewed with any resolution effectively appear as an antiquark. Therefore, a baryon containing 3 valence quarks, of which two tend to cluster together as a diquark, behaves like a meson. "
What is the relationship between the complete electromagnetic Hamiltonian of a molecule and the parity operation?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the relationship between the complete electromagnetic Hamiltonian of a molecule and the parity operation?
 Context: -Molecules The complete (rotational-vibrational-electronic-nuclear spin) electromagnetic Hamiltonian of any molecule commutes with (or is invariant to) the parity operation P (or E*, in the notation introduced by Longuet-Higgins) and its eigenvalues can be given the parity symmetry label + or - as they are even or odd, respectively. The parity operation involves the inversion of electronic and nuclear spatial coordinates at the molecular center of mass.
-Centrosymmetric molecules at equilibrium have a centre of symmetry at their midpoint (the nuclear center of mass). This includes all homonuclear diatomic molecules as well as certain symmetric molecules such as ethylene, benzene, xenon tetrafluoride and sulphur hexafluoride. For centrosymmetric molecules, the point group contains the operation i which is not to be confused with the parity operation. The operation i involves the inversion of the electronic and vibrational displacement coordinates at the nuclear centre of mass. For centrosymmetric molecules the operation i commutes with the rovibronic (rotation-vibration-electronic) Hamiltonian and can be used to label such states. Electronic and vibrational states of centrosymmetric molecules are either unchanged by the operation i, or they are changed in sign by i. The former are denoted by the subscript g and are called gerade, while the latter are denoted by the subscript u and are called ungerade. The complete Hamiltonian of a centrosymmetric molecule does not commute with the point group inversion operation i because of the effect of the nuclear hyperfine Hamiltonian. The nuclear hyperfine Hamiltonian can mix the rotational levels of g and u vibronic states (called ortho-para mixing) and give rise to ortho-para transitions Nuclei In atomic nuclei, the state of each nucleon (proton or neutron) has even or odd parity, and nucleon configurations can be predicted using the nuclear shell model. As for electrons in atoms, the nucleon state has odd overall parity if and only if the number of nucleons in odd-parity states is odd. The parity is usually written as a + (even) or − (odd) following the nuclear spin value. For example, the isotopes of oxygen include 17O(5/2+), meaning that the spin is 5/2 and the parity is even. The shell model explains this because the first 16 nucleons are paired so that each pair has spin zero and even parity, and the last nucleon is in the 1d5/2 shell, which has even parity since ℓ = 2 for a d orbital.
-The complete Hamiltonian of a diatomic molecule (as for all molecules) commutes with the parity operation P or E* and rovibronic (rotation-vibration-electronic) energy levels (often called rotational levels) can be given the parity symmetry label + or -. The complete Hamiltonian of a homonuclear diatomic molecule also commutes with the operation of permuting (or exchanging) the coordinates of the two (identical) nuclei and rotational levels  gain the additional label s or a depending on whether the total wavefunction is unchanged (symmetric) or changed in sign (antisymmetric) by the permutation operation. Thus, the rotational levels of heteronuclear diatomic molecules are labelled + or -, whereas those of homonuclear diatomic molecules are labelled +s, +a, -s or -a. The rovibronic nuclear spin states are classified using the appropriate permutation-inversion group.The complete Hamiltonian of a homonuclear diatomic molecule (as for all centro-symmetric molecules) does not commute with the point group inversion operation i because of the effect of the nuclear hyperfine Hamiltonian. The nuclear hyperfine Hamiltonian can mix the rotational levels of g and u vibronic states (called ortho-para mixing) and give rise to ortho-para transitions Spin and total angular momentum If S denotes the resultant of the individual electron spins,  s(s+1)ℏ2 are the eigenvalues of S and as in the case of atoms, each electronic term of the molecule is also characterised by the value of S. If spin-orbit coupling is neglected, there is a degeneracy of order  2s+1 associated with each  s for a given  Λ . Just as for atoms, the quantity  2s+1 is called the multiplicity of the term and.is written as a (left) superscript, so that the term symbol is written as  2s+1Λ . For example, the symbol  3Π denotes a term such that  Λ=1 and  s=1 . It is worth noting that the ground state (often labelled by the symbol  X ) of most diatomic molecules is such that  s=0 and exhibits maximum symmetry. Thus, in most cases it is a  1Σ+ state (written as  X1Σ+ , excited states are written with  A,B,C,...
-All fundamental interactions of elementary particles, with the exception of the weak interaction, are symmetric under parity. The weak interaction is chiral and thus provides a means for probing chirality in physics. In interactions that are symmetric under parity, such as electromagnetism in atomic and molecular physics, parity serves as a powerful controlling principle underlying quantum transitions.
A matrix representation of P (in any number of dimensions) has determinant equal to −1, and hence is distinct from a rotation, which has a determinant equal to 1. In a two-dimensional plane, a simultaneous flip of all coordinates in sign is not a parity transformation; it is the same as a 180° rotation.
In quantum mechanics, wave functions that are unchanged by a parity transformation are described as even functions, while those that change sign under a parity transformation are odd functions.
-In quantum mechanics, Hamiltonians are invariant (symmetric) under a parity transformation if  P^ commutes with the Hamiltonian. In non-relativistic quantum mechanics, this happens for any scalar potential, i.e.,  V=V(r) , hence the potential is spherically symmetric. The following facts can be easily proven: If  |φ⟩ and  |ψ⟩ have the same parity, then  ⟨φ|X^|ψ⟩=0 where  X^ is the position operator.
For a state  |L→,Lz⟩ of orbital angular momentum  L→ with z-axis projection  Lz , then  P^|L→,Lz⟩=(−1)L|L→,Lz⟩ If  [H^,P^]=0 , then atomic dipole transitions only occur between states of opposite parity.
 A. The complete electromagnetic Hamiltonian of any molecule is invariant to the parity operation, and its eigenvalues cannot be given the parity symmetry label + or -.
 B. The complete electromagnetic Hamiltonian of any molecule is dependent on the parity operation, and its eigenvalues can be given the parity symmetry label even or odd, respectively.
 C. The complete electromagnetic Hamiltonian of any molecule is dependent on the parity operation, and its eigenvalues can be given the parity symmetry label + or - depending on whether they are even or odd, respectively.
 D. The complete electromagnetic Hamiltonian of any molecule is invariant to the parity operation, and its eigenvalues can be given the parity symmetry label + or - depending on whether they are even or odd, respectively.
 E. The complete electromagnetic Hamiltonian of any molecule does not involve the parity operation, and its eigenvalues cannot be given the parity symmetry label + or -. "
What is the difference between active and passive transport in cells?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the difference between active and passive transport in cells?
 Context: -Unlike passive transport, which uses the kinetic energy and natural entropy of molecules moving down a gradient, active transport uses cellular energy to move them against a gradient, polar repulsion, or other resistance. Active transport is usually associated with accumulating high concentrations of molecules that the cell needs, such as ions, glucose and amino acids. Examples of active transport include the uptake of glucose in the intestines in humans and the uptake of mineral ions into root hair cells of plants.
-Passive transport is a type of membrane transport that does not require energy to move substances across cell membranes. Instead of using cellular energy, like active transport, passive transport relies on the second law of thermodynamics to drive the movement of substances across cell membranes. Fundamentally, substances follow Fick's first law, and move from an area of high concentration to one of low concentration because this movement increases the entropy of the overall system. The rate of passive transport depends on the permeability of the cell membrane, which, in turn, depends on the organization and characteristics of the membrane lipids and proteins. The four main kinds of passive transport are simple diffusion, facilitated diffusion, filtration, and/or osmosis.
-In cellular biology, active transport is the movement of molecules or ions across a cell membrane from a region of lower concentration to a region of higher concentration—against the concentration gradient. Active transport requires cellular energy to achieve this movement. There are two types of active transport: primary active transport that uses adenosine triphosphate (ATP), and secondary active transport that uses an electrochemical gradient. This process is in contrast to passive transport, which allows molecules or ions to move down their concentration gradient, from an area of high concentration to an area of low concentration, without energy.
-An example of active transport of ions is the Na+-K+-ATPase (NKA). NKA is powered by the hydrolysis of ATP into ADP and an inorganic phosphate; for every molecule of ATP hydrolized, three Na+ are transported outside and two K+ are transported inside the cell. This makes the inside of the cell more negative than the outside and more specifically generates a membrane potential Vmembrane of about −60 mV.An example of passive transport is ion fluxes through Na+, K+, Ca2+, and Cl− channels. Unlike active transport, passive transport is powered by the arithmetic sum of osmosis (a concentration gradient) and an electric field (the transmembrane potential). Formally, the molar Gibbs free energy change associated with successful transport is where R represents the gas constant, T represents absolute temperature, z is the charge per ion, and F represents the Faraday constant.: 464–465 In the example of Na+, both terms tend to support transport: the negative electric potential inside the cell attracts the positive ion and since Na+ is concentrated outside the cell, osmosis supports diffusion through the Na+ channel into the cell. In the case of K+, the effect of osmosis is reversed: although external ions are attracted by the negative intracellular potential, entropy seeks to diffuse the ions already concentrated inside the cell. The converse phenomenon (osmosis supports transport, electric potential opposes it) can be achieved for Na+ in cells with abnormal transmembrane potentials: at +70 mV, the Na+ influx halts; at higher potentials, it becomes an efflux.
-active transport In cellular biology, the movement of molecules across a membrane from a region of their lower concentration to a region of their higher concentration—against the concentration gradient. Active transport requires cellular energy to achieve this movement. There are two types of active transport: primary active transport that uses ATP, and secondary active transport that uses an electrochemical gradient.
 A. Active transport and passive transport both require energy input from the cell to function.
 B. Passive transport is powered by the arithmetic sum of osmosis and an electric field, while active transport requires energy input from the cell.
 C. Passive transport requires energy input from the cell, while active transport is powered by the arithmetic sum of osmosis and an electric field.
 D. Active transport and passive transport are both powered by the arithmetic sum of osmosis and an electric field.
 E. Active transport is powered by the arithmetic sum of osmosis and an electric field, while passive transport requires energy input from the cell. "
What is the Heisenberg uncertainty principle and how does it relate to angular momentum in quantum mechanics?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the Heisenberg uncertainty principle and how does it relate to angular momentum in quantum mechanics?
 Context: -In quantum mechanics, angular momentum (like other quantities) is expressed as an operator, and its one-dimensional projections have quantized eigenvalues. Angular momentum is subject to the Heisenberg uncertainty principle, implying that at any time, only one projection (also called ""component"") can be measured with definite precision; the other two then remain uncertain. Because of this, the axis of rotation of a quantum particle is undefined. Quantum particles do possess a type of non-orbital angular momentum called ""spin"", but this angular momentum does not correspond to a spinning motion. In relativistic quantum mechanics the above relativistic definition becomes a tensorial operator.
-Spin projection quantum number and multiplicity In classical mechanics, the angular momentum of a particle possesses not only a magnitude (how fast the body is rotating), but also a direction (either up or down on the axis of rotation of the particle). Quantum-mechanical spin also contains information about direction, but in a more subtle form. Quantum mechanics states that the component of angular momentum for a spin-s particle measured along any direction can only take on the values Si=ℏsi,si∈{−s,−(s−1),…,s−1,s}, where Si is the spin component along the i-th axis (either x, y, or z), si is the spin projection quantum number along the i-th axis, and s is the principal spin quantum number (discussed in the previous section). Conventionally the direction chosen is the z axis: Sz=ℏsz,sz∈{−s,−(s−1),…,s−1,s}, where Sz is the spin component along the z axis, sz is the spin projection quantum number along the z axis.
-Angular momentum and spin Similarly for the angular momentum which implies that the angular momentum of the photon is the quantum interpretation of this expression is that the photon has a probability of  |ψR|2 of having an angular momentum of  ℏ and a probability of  |ψL|2 of having an angular momentum of  −ℏ . We can therefore think of the angular momentum of the photon being quantized as well as the energy. This has indeed been experimentally verified. Photons have only been observed to have angular momenta of  ±ℏ Spin operator The spin of the photon is defined as the coefficient of  ℏ in the angular momentum calculation. A photon has spin 1 if it is in the  |R⟩ state and -1 if it is in the  |L⟩ state. The spin operator is defined as the outer product The eigenvectors of the spin operator are  |R⟩ and  |L⟩ with eigenvalues 1 and -1, respectively.
-While many introductory texts treat photons using the mathematical techniques of non-relativistic quantum mechanics, this is in some ways an awkward oversimplification, as photons are by nature intrinsically relativistic. Because photons have zero rest mass, no wave function defined for a photon can have all the properties familiar from wave functions in non-relativistic quantum mechanics. In order to avoid these difficulties, physicists employ the second-quantized theory of photons described below, quantum electrodynamics, in which photons are quantized excitations of electromagnetic modes.Another difficulty is finding the proper analogue for the uncertainty principle, an idea frequently attributed to Heisenberg, who introduced the concept in analyzing a thought experiment involving an electron and a high-energy photon. However, Heisenberg did not give precise mathematical definitions of what the ""uncertainty"" in these measurements meant. The precise mathematical statement of the position–momentum uncertainty principle is due to Kennard, Pauli, and Weyl. The uncertainty principle applies to situations where an experimenter has a choice of measuring either one of two ""canonically conjugate"" quantities, like the position and the momentum of a particle. According to the uncertainty principle, no matter how the particle is prepared, it is not possible to make a precise prediction for both of the two alternative measurements: if the outcome of the position measurement is made more certain, the outcome of the momentum measurement becomes less so, and vice versa. A coherent state minimizes the overall uncertainty as far as quantum mechanics allows. Quantum optics makes use of coherent states for modes of the electromagnetic field. There is a tradeoff, reminiscent of the position–momentum uncertainty relation, between measurements of an electromagnetic wave's amplitude and its phase. This is sometimes informally expressed in terms of the uncertainty in the number of photons present in the electromagnetic wave,  ΔN , and the uncertainty in the phase of the wave,  Δϕ . However, this cannot be an uncertainty relation of the Kennard–Pauli–Weyl type, since unlike position and momentum, the phase  ϕ cannot be represented by a Hermitian operator.
-Spin, orbital, and total angular momentum The classical definition of angular momentum as  L=r×p can be carried over to quantum mechanics, by reinterpreting r as the quantum position operator and p as the quantum momentum operator. L is then an operator, specifically called the orbital angular momentum operator. The components of the angular momentum operator satisfy the commutation relations of the Lie algebra so(3). Indeed, these operators are precisely the infinitesimal action of the rotation group on the quantum Hilbert space. (See also the discussion below of the angular momentum operators as the generators of rotations.) However, in quantum physics, there is another type of angular momentum, called spin angular momentum, represented by the spin operator S. Spin is often depicted as a particle literally spinning around an axis, but this is a misleading and inaccurate picture: spin is an intrinsic property of a particle, unrelated to any sort of motion in space and fundamentally different from orbital angular momentum. All elementary particles have a characteristic spin (possibly zero), and almost all elementary particles have nonzero spin. For example electrons have ""spin 1/2"" (this actually means ""spin ħ/2""), photons have ""spin 1"" (this actually means ""spin ħ""), and pi-mesons have spin 0.Finally, there is total angular momentum J, which combines both the spin and orbital angular momentum of all particles and fields. (For one particle, J = L + S.) Conservation of angular momentum applies to J, but not to L or S; for example, the spin–orbit interaction allows angular momentum to transfer back and forth between L and S, with the total remaining constant. Electrons and photons need not have integer-based values for total angular momentum, but can also have half-integer values.In molecules the total angular momentum F is the sum of the rovibronic (orbital) angular momentum N, the electron spin angular momentum S, and the nuclear spin angular momentum I. For electronic singlet states the rovibronic angular momentum is denoted J rather than N. As explained by Van Vleck, the components of the molecular rovibronic angular momentum referred to molecule-fixed axes have different commutation relations from those for the components about space-fixed axes.
 A. The Heisenberg uncertainty principle states that the axis of rotation of a quantum particle is undefined, and that quantum particles possess a type of non-orbital angular momentum called ""spin"". This is because angular momentum, like other quantities in quantum mechanics, is expressed as a tensorial operator in relativistic quantum mechanics.
 B. The Heisenberg uncertainty principle states that the total angular momentum of a system of particles is equal to the sum of the individual particle angular momenta, and that the centre of mass is for the system. This is because angular momentum, like other quantities in quantum mechanics, is expressed as an operator with quantized eigenvalues.
 C. The Heisenberg uncertainty principle states that the total angular momentum of a system of particles is subject to quantization, and that the individual particle angular momenta are expressed as operators. This is because angular momentum, like other quantities in quantum mechanics, is subject to the Heisenberg uncertainty principle.
 D. The Heisenberg uncertainty principle states that the axis of rotation of a quantum particle is undefined, and that at any given time, only one projection of angular momentum can be measured with definite precision, while the other two remain uncertain. This is because angular momentum, like other quantities in quantum mechanics, is subject to quantization and expressed as an operator with quantized eigenvalues.
 E. The Heisenberg uncertainty principle states that at any given time, only one projection of angular momentum can be measured with definite precision, while the other two remain uncertain. This is because angular momentum, like other quantities in quantum mechanics, is expressed as an operator with quantized eigenvalues. "
What is the difference between natural convection and forced convection?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the difference between natural convection and forced convection?
 Context: -Forced convection: when a fluid is forced to flow over the surface by an internal source such as fans, by stirring, and pumps, creating an artificially induced convection current.In many real-life applications (e.g. heat losses at solar central receivers or cooling of photovoltaic panels), natural and forced convection occur at the same time (mixed convection).Internal and external flow can also classify convection. Internal flow occurs when a fluid is enclosed by a solid boundary such as when flowing through a pipe. An external flow occurs when a fluid extends indefinitely without encountering a solid surface. Both of these types of convection, either natural or forced, can be internal or external because they are independent of each other. The bulk temperature, or the average fluid temperature, is a convenient reference point for evaluating properties related to convective heat transfer, particularly in applications related to flow in pipes and ducts.
-Free, or natural, convection occurs when bulk fluid motions (streams and currents) are caused by buoyancy forces that result from density variations due to variations of temperature in the fluid. Forced convection is a term used when the streams and currents in the fluid are induced by external means—such as fans, stirrers, and pumps—creating an artificially induced convection current.
-Convection can be ""forced"" by movement of a fluid by means other than buoyancy forces (for example, a water pump in an automobile engine). Thermal expansion of fluids may also force convection. In other cases, natural buoyancy forces alone are entirely responsible for fluid motion when the fluid is heated, and this process is called ""natural convection"". An example is the draft in a chimney or around any fire. In natural convection, an increase in temperature produces a reduction in density, which in turn causes fluid motion due to pressures and forces when fluids of different densities are affected by gravity (or any g-force). For example, when water is heated on a stove, hot water from the bottom of the pan is displaced (or forced up) by the colder denser liquid, which falls. After heating has stopped, mixing and conduction from this natural convection eventually result in a nearly homogeneous density, and even temperature. Without the presence of gravity (or conditions that cause a g-force of any type), natural convection does not occur, and only forced-convection modes operate.
-In fluid thermodynamics, combined forced convection and natural convection, or mixed convection, occurs when natural convection and forced convection mechanisms act together to transfer heat. This is also defined as situations where both pressure forces and buoyant forces interact. How much each form of convection contributes to the heat transfer is largely determined by the flow, temperature, geometry, and orientation. The nature of the fluid is also influential, since the Grashof number increases in a fluid as temperature increases, but is maximized at some point for a gas.
-Heat convection occurs when the bulk flow of a fluid (gas or liquid) carries its heat through the fluid. All convective processes also move heat partly by diffusion, as well. The flow of fluid may be forced by external processes, or sometimes (in gravitational fields) by buoyancy forces caused when thermal energy expands the fluid (for example in a fire plume), thus influencing its own transfer. The latter process is often called ""natural convection"". The former process is often called ""forced convection."" In this case, the fluid is forced to flow by use of a pump, fan, or other mechanical means.
 A. Natural convection and forced convection are the same phenomenon, where a fluid is forced to flow over the surface by an internal source such as fans, stirring, and pumps, causing the fluid to be less dense and displaced.
 B. Natural convection and forced convection are two different phenomena that do not relate to each other.
 C. Natural convection occurs when a fluid is in contact with a hot surface, causing the fluid to be less dense and displaced, while forced convection is when a fluid is forced to flow over the surface by an internal source such as fans, stirring, and pumps.
 D. Natural convection is when a fluid is forced to flow over the surface by an internal source such as fans, stirring, and pumps, while forced convection occurs when a fluid is in contact with a hot surface, causing the fluid to be less dense and displaced.
 E. Natural convection and forced convection are the same phenomenon, where a fluid is in contact with a hot surface, causing the fluid to be less dense and displaced, and then forced to flow over the surface by an internal source such as fans, stirring, and pumps. "
What is magnetic susceptibility?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is magnetic susceptibility?
 Context: -In electromagnetism, the magnetic susceptibility (from Latin susceptibilis 'receptive'; denoted χ, chi) is a measure of how much a material will become magnetized in an applied magnetic field. It is the ratio of magnetization M (magnetic moment per unit volume) to the applied magnetizing field intensity H. This allows a simple classification, into two categories, of most materials' responses to an applied magnetic field: an alignment with the magnetic field, χ > 0, called paramagnetism, or an alignment against the field, χ < 0, called diamagnetism.
-Volume susceptibility Magnetic susceptibility is a dimensionless proportionality constant that indicates the degree of magnetization of a material in response to an applied magnetic field. A related term is magnetizability, the proportion between magnetic moment and magnetic flux density. A closely related parameter is the permeability, which expresses the total magnetization of material and volume.
-Magnetic susceptibility indicates whether a material is attracted into or repelled out of a magnetic field. Paramagnetic materials align with the applied field and are attracted to regions of greater magnetic field. Diamagnetic materials are anti-aligned and are pushed away, toward regions of lower magnetic fields. On top of the applied field, the magnetization of the material adds its own magnetic field, causing the field lines to concentrate in paramagnetism, or be excluded in diamagnetism. Quantitative measures of the magnetic susceptibility also provide insights into the structure of materials, providing insight into bonding and energy levels. Furthermore, it is widely used in geology for paleomagnetic studies and structural geology.The magnetizability of materials comes from the atomic-level magnetic properties of the particles of which they are made. Usually, this is dominated by the magnetic moments of electrons. Electrons are present in all materials, but without any external magnetic field, the magnetic moments of the electrons are usually either paired up or random so that the overall magnetism is zero (the exception to this usual case is ferromagnetism). The fundamental reasons why the magnetic moments of the electrons line up or do not are very complex and cannot be explained by classical physics. However, a useful simplification is to measure the magnetic susceptibility of a material and apply the macroscopic form of Maxwell's equations. This allows classical physics to make useful predictions while avoiding the underlying quantum mechanical details.
-In electricity (electromagnetism), the electric susceptibility ( χe ; Latin: susceptibilis ""receptive"") is a dimensionless proportionality constant that indicates the degree of polarization of a dielectric material in response to an applied electric field. The greater the electric susceptibility, the greater the ability of a material to polarize in response to the field, and thereby reduce the total electric field inside the material(and store energy). It is in this way that the electric susceptibility influences the electric permittivity of the material and thus influences many other phenomena in that medium, from the capacitance of capacitors to the speed of light.
-In physics the susceptibility is a quantification for the change of an extensive property under variation of an intensive property. The word may refer to: In physics, the susceptibility of a material or substance describes its response to an applied field. For example: Magnetic susceptibility Electric susceptibility The two types of susceptibility above are examples of a linear response function; sometimes the terms susceptibility and linear response function are used interchangeably.
 A. Magnetic susceptibility is a measure of how much a material will absorb magnetization in an applied magnetic field. It is the ratio of magnetization to the applied magnetizing field intensity, allowing for a simple classification of most materials' responses to an applied magnetic field.
 B. Magnetic susceptibility is a measure of how much a material will become magnetized in an applied magnetic field. It is the ratio of magnetization to the applied magnetizing field intensity, allowing for a simple classification of most materials' responses to an applied magnetic field.
 C. Magnetic susceptibility is a measure of how much a material will resist magnetization in an applied magnetic field. It is the ratio of magnetization to the applied magnetizing field intensity, allowing for a simple classification of most materials' responses to an applied magnetic field.
 D. Magnetic susceptibility is a measure of how much a material will conduct magnetization in an applied magnetic field. It is the ratio of magnetization to the applied magnetizing field intensity, allowing for a simple classification of most materials' responses to an applied magnetic field.
 E. Magnetic susceptibility is a measure of how much a material will reflect magnetization in an applied magnetic field. It is the ratio of magnetization to the applied magnetizing field intensity, allowing for a simple classification of most materials' responses to an applied magnetic field. "
"What is a transient condensation cloud, also known as a Wilson cloud?","Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is a transient condensation cloud, also known as a Wilson cloud?
 Context: -A transient condensation cloud, also called a Wilson cloud, is observable surrounding large explosions in humid air.
-The lifetime of the Wilson cloud during nuclear air bursts can be shortened by the thermal radiation from the fireball, which heats the cloud above to the dew point and evaporates the droplets.
Non-nuclear explosions Any sufficiently large explosion, such as one caused by a large quantity of conventional explosives or a volcanic eruption, can create a condensation cloud, as seen in Operation Sailor Hat or in the 2020 Beirut explosion, where a very large Wilson cloud expanded outwards from the blast.
-Similar condensation effects can be observed as Wilson clouds, also called condensation clouds, at large explosions in humid air and other Prandtl–Glauert singularity effects.
-Condensation effects Nuclear mushroom clouds are often accompanied by short-lived vapour clouds, known variously as ""Wilson clouds"", condensation clouds, or vapor rings. The ""negative phase"" following the positive overpressure behind a shock front causes a sudden rarefaction of the surrounding medium. This low pressure region causes an adiabatic drop in temperature, causing moisture in the air to condense in an outward moving shell surrounding the explosion. When the pressure and temperature return to normal, the Wilson cloud dissipates. Scientists observing the Operation Crossroads nuclear tests in 1946 at Bikini Atoll named that transitory cloud a ""Wilson cloud"" because of its visual similarity to a Wilson cloud chamber; the cloud chamber uses condensation from a rapid pressure drop to mark the tracks of electrically charged subatomic particles. Analysts of later nuclear bomb tests used the more general term ""condensation cloud"" in preference to ""Wilson cloud"".
-When a nuclear weapon or a large amount of a conventional explosive is detonated in sufficiently humid air, the ""negative phase"" of the shock wave causes a rarefaction of the air surrounding the explosion, but not contained within it. This rarefaction results in a temporary cooling of that air, which causes a condensation of some of the water vapor contained in it. When the pressure and the temperature return to normal, the Wilson cloud dissipates.
 A. A visible cloud of smoke that forms when a nuclear weapon or a large amount of a conventional explosive is detonated in humid air, due to the burning of materials in the explosion.
 B. A visible cloud of microscopic water droplets that forms when a nuclear weapon or a large amount of a conventional explosive is detonated in humid air, due to a temporary cooling of the air caused by a rarefaction of the air surrounding the explosion.
 C. A visible cloud of microscopic water droplets that forms when a nuclear weapon or a large amount of a conventional explosive is detonated in dry air, due to a temporary cooling of the air caused by a rarefaction of the air surrounding the explosion.
 D. A visible cloud of gas that forms when a nuclear weapon or a large amount of a conventional explosive is detonated in humid air, due to the release of gases from the explosion.
 E. A visible cloud of smoke that forms when a nuclear weapon or a large amount of a conventional explosive is detonated in dry air, due to the burning of materials in the explosion. "
What is a uniform tiling in the hyperbolic plane?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is a uniform tiling in the hyperbolic plane?
 Context: -In hyperbolic geometry, a uniform hyperbolic tiling (or regular, quasiregular or semiregular hyperbolic tiling) is an edge-to-edge filling of the hyperbolic plane which has regular polygons as faces and is vertex-transitive (transitive on its vertices, isogonal, i.e. there is an isometry mapping any vertex onto any other). It follows that all vertices are congruent, and the tiling has a high degree of rotational and translational symmetry.
-Tessellations in non-Euclidean geometries It is possible to tessellate in non-Euclidean geometries such as hyperbolic geometry. A uniform tiling in the hyperbolic plane (that may be regular, quasiregular, or semiregular) is an edge-to-edge filling of the hyperbolic plane, with regular polygons as faces; these are vertex-transitive (transitive on its vertices), and isogonal (there is an isometry mapping any vertex onto any other).A uniform honeycomb in hyperbolic space is a uniform tessellation of uniform polyhedral cells. In three-dimensional (3-D) hyperbolic space there are nine Coxeter group families of compact convex uniform honeycombs, generated as Wythoff constructions, and represented by permutations of rings of the Coxeter diagrams for each family.
-In geometry, a uniform tiling is a tessellation of the plane by regular polygon faces with the restriction of being vertex-transitive.
Uniform tilings can exist in both the Euclidean plane and hyperbolic plane. Uniform tilings are related to the finite uniform polyhedra which can be considered uniform tilings of the sphere.
Most uniform tilings can be made from a Wythoff construction starting with a symmetry group and a singular generator point inside of the fundamental domain. A planar symmetry group has a polygonal fundamental domain and can be represented by the group name represented by the order of the mirrors in sequential vertices.
A fundamental domain triangle is (p q r), and a right triangle (p q 2), where p, q, r are whole numbers greater than 1. The triangle may exist as a spherical triangle, a Euclidean plane triangle, or a hyperbolic plane triangle, depending on the values of p, q and r.
-There are an infinite number of dual uniform tilings in hyperbolic plane with isogonal irregular pentagonal faces. They have face configurations as V3.3.p.3.q.
The binary tiling can be made into a pentagonal tiling if one replaces the horocyclic edges by line segments.
-This tiling is one of 10 uniform tilings constructed from [8,3] hyperbolic symmetry and three subsymmetries [1+,8,3], [8,3+] and [8,3]+.
This tiling can be considered a member of a sequence of uniform patterns with vertex figure (4.6.2p) and Coxeter-Dynkin diagram . For p < 6, the members of the sequence are omnitruncated polyhedra (zonohedrons), shown below as spherical tilings. For p > 6, they are tilings of the hyperbolic plane, starting with the truncated triheptagonal tiling.
 A. A uniform tiling in the hyperbolic plane is a tessellation of the hyperbolic plane with irregular polygons as faces. These are not vertex-transitive and isogonal.
 B. A uniform tiling in the hyperbolic plane is a tessellation of the hyperbolic plane with regular polygons as faces. These are not vertex-transitive and isogonal.
 C. A uniform tiling in the hyperbolic plane is a tessellation of the hyperbolic plane with irregular polygons as faces. These are vertex-transitive and isogonal.
 D. A uniform tiling in the hyperbolic plane is an edge-to-edge filling of the hyperbolic plane, with regular polygons as faces. These are vertex-transitive and isogonal.
 E. A uniform tiling in the hyperbolic plane is an edge-to-edge filling of the hyperbolic plane, with irregular polygons as faces. These are vertex-transitive and isogonal. "
What is the relation between the three moment theorem and the bending moments at three successive supports of a continuous beam?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the relation between the three moment theorem and the bending moments at three successive supports of a continuous beam?
 Context: -In civil engineering and structural analysis Clapeyron's theorem of three moments is a relationship among the bending moments at three consecutive supports of a horizontal beam.
Let A,B,C-D be the three consecutive points of support, and denote by- l the length of AB and  l′ the length of BC, by w and  w′ the weight per unit of length in these segments. Then the bending moments  MA,MB,MC at the three points are related by: MAl+2MB(l+l′)+MCl′=14wl3+14w′(l′)3.
-Mohr's first theorem The change in slope of a deflection curve between two points of a beam is equal to the area of the M/EI diagram between those two points.(Figure 02) Mohr's second theorem Consider two points k1 and k2 on a beam. The deflection of k1 and k2 relative to the point of intersection between tangent at k1 and k2 and vertical through k1 is equal to the moment of M/EI diagram between k1 and k2 about k1.(Figure 03) The three moment equation expresses the relation between bending moments at three successive supports of a continuous beam, subject to a loading on a two adjacent span with or without settlement of the supports.
-Let A' B' and C' be the final positions of the beam ABC due to support settlements.
Derivation of three moment theorem PB'Q is a tangent drawn at B' for final Elastic Curve A'B'C' of the beam ABC. RB'S is a horizontal line drawn through B'.  Consider, Triangles RB'P and QB'S.
PRRB′=SQB′S, From (1), (2), and (3), ΔB−ΔA+PA′L1=ΔC−ΔB−QC′L2 Draw the M/EI diagram to find the PA' and QC'.
From Mohr's Second Theorem  PA' = First moment of area of M/EI diagram between A and B about A.
PA′=(12×M1E1I1×L1)×L1×13+(12×M2E2I2×L1)×L1×23+A1X1E1I1 QC' = First moment of area of M/EI diagram between B and C about C.
QC′=(12×M3E2I2×L2)×L2×13+(12×M2E2I2×L2)×L2×23+A2X2E2I2 Substitute in PA' and QC' on equation (a), the Three Moment Theorem (TMT) can be obtained.
-The moment-area theorem is an engineering tool to derive the slope, rotation and deflection of beams and frames. This theorem was developed by Mohr and later stated namely by Charles Ezra Greene in 1873. This method is advantageous when we solve problems involving beams, especially for those subjected to a series of concentrated loadings or having segments with different moments of inertia.
-K=pw where  p is the applied force and  w is the deflection. According to elementary beam theory, the relationship between the applied bending moment  M and the resulting curvature  κ of the beam is: M=EIκ=EId2wdx2 where  w is the deflection of the beam and  x is the distance along the beam. Double integration of the above equation leads to computing the deflection of the beam, and in turn, the bending stiffness of the beam.
 A. The three moment theorem expresses the relation between the deflection of two points on a beam relative to the point of intersection between tangent at those two points and the vertical through the first point.
 B. The three moment theorem is used to calculate the maximum allowable bending moment of a beam, which is determined by the weight distribution of each segment of the beam.
 C. The three moment theorem describes the relationship between bending moments at three successive supports of a continuous beam, subject to a loading on two adjacent spans with or without settlement of the supports.
 D. The three moment theorem is used to calculate the weight distribution of each segment of a beam, which is required to apply Mohr's theorem.
 E. The three moment theorem is used to derive the change in slope of a deflection curve between two points of a beam, which is equal to the area of the M/EI diagram between those two points. "
"What is the throttling process, and why is it important?","Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the throttling process, and why is it important?
 Context: -Throttling One of the simple applications of the concept of enthalpy is the so-called throttling process, also known as Joule–Thomson expansion. It concerns a steady adiabatic flow of a fluid through a flow resistance (valve, porous plug, or any other type of flow resistance) as shown in the figure. This process is very important, since it is at the heart of domestic refrigerators, where it is responsible for the temperature drop between ambient temperature and the interior of the refrigerator. It is also the final stage in many types of liquefiers.
-The gas-cooling throttling process is commonly exploited in refrigeration processes such as liquefiers in air separation industrial process. In hydraulics, the warming effect from Joule–Thomson throttling can be used to find internally leaking valves as these will produce heat which can be detected by thermocouple or thermal-imaging camera. Throttling is a fundamentally irreversible process. The throttling due to the flow resistance in supply lines, heat exchangers, regenerators, and other components of (thermal) machines is a source of losses that limits their performance.
-Throttling valves control the amount or pressure of a fluid allowed to pass through and are designed to withstand the stress and wear caused by this operation. Because they may wear out in this usage, they are often installed alongside isolation valves which can temporarily disconnect a failing throttling valve from the rest of the system, so it can be refurbished or replaced.
-A very convenient way to get a quantitative understanding of the throttling process is by using diagrams such as h-T diagrams, h-P diagrams, and others. Commonly used are the so-called T-s diagrams. Figure 2 shows the T-s diagram of nitrogen as an example. Various points are indicated as follows: As shown before, throttling keeps h constant. E.g. throttling from 200 bar and 300 K (point a in fig. 2) follows the isenthalpic (line of constant specific enthalpy) of 430 kJ/kg. At 1 bar it results in point b which has a temperature of 270 K. So throttling from 200 bar to 1 bar gives a cooling from room temperature to below the freezing point of water. Throttling from 200 bar and an initial temperature of 133 K (point c in fig. 2) to 1 bar results in point d, which is in the two-phase region of nitrogen at a temperature of 77.2 K. Since the enthalpy is an extensive parameter the enthalpy in d (hd) is equal to the enthalpy in e (he) multiplied with the mass fraction of the liquid in d (xd) plus the enthalpy in f (hf) multiplied with the mass fraction of the gas in d (1 − xd). So hd=xdhe+(1−xd)hf.
-Thermally Enhanced Grout High performance grout with a greater TC than more commonly used products Throttling Valve Most commonly used to restrict flow between desuperheaters and buffer tanks, throttling valves allow an operator to achieve higher delta T between entering and leaving water by slowing the GPM flow.
Ton of Refrigeration A measure of the amount of heat absorption required to melt I ton of ice in 24 hours. A ton of refrigeration is a measure of the amount of cooling delivered by a heat pump (or other air conditioning system). One ton of refrigeration is equivalent to a cooling rate of 12,000 Btu per hour.
Total Cooling Load The total amount of heat energy that must be removed from a space to keep it at the thermostat set point temperature as well as at the desired humidity level, defined to be the sum of the sensible cooling load and the latent cooling load.
Tremie Line The pipe used to pump an appropriate grouting material into a borehole from the bottom of the hole to the top. A tremie line will commonly be made of I-inch or I-IL-inch diameter HDPE pipe.
Turbulent Flow Regime The flow condition where fluid flow becomes chaotic and disordered. The mixing effect caused by turbulent flow maximizes heat transfer between the fluid and pipe walls in the closed-loop GHEX while also increasing the system pumping pressure.
 A. The throttling process is a steady flow of a fluid through a flow resistance, such as a valve or porous plug, and is responsible for the pressure increase in domestic refrigerators. This process is important because it is at the heart of the refrigeration cycle.
 B. The throttling process is a steady adiabatic flow of a fluid through a flow resistance, such as a valve or porous plug, and is responsible for the temperature drop in domestic refrigerators. This process is important because it is at the heart of the refrigeration cycle.
 C. The throttling process is a steady adiabatic flow of a fluid through a flow resistance, such as a valve or porous plug, and is responsible for the pressure drop in domestic refrigerators. This process is important because it is at the heart of the refrigeration cycle.
 D. The throttling process is a steady flow of a fluid through a flow resistance, such as a valve or porous plug, and is responsible for the temperature increase in domestic refrigerators. This process is important because it is at the heart of the refrigeration cycle.
 E. The throttling process is a steady adiabatic flow of a fluid through a flow resistance, such as a valve or porous plug, and is responsible for the temperature drop in domestic refrigerators. This process is not important because it is not used in the refrigeration cycle. "
What happens to excess base metal as a solution cools from the upper transformation temperature towards an insoluble state?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What happens to excess base metal as a solution cools from the upper transformation temperature towards an insoluble state?
 Context: -Similarly, a hypoeutectoid alloy has two critical temperatures, called ""arrests"". Between these two temperatures, the alloy will exist partly as the solution and partly as a separate crystallizing phase, called the ""pro eutectoid phase"". These two temperatures are called the upper (A3) and lower (A1) transformation temperatures. As the solution cools from the upper transformation temperature toward an insoluble state, the excess base metal will often be forced to ""crystallize-out"", becoming the pro eutectoid. This will occur until the remaining concentration of solutes reaches the eutectoid level, which will then crystallize as a separate microstructure.
-Hypereutectoid alloys A hypereutectic alloy also has different melting points. However, between these points, it is the constituent with the higher melting point that will be solid. Similarly, a hypereutectoid alloy has two critical temperatures. When cooling a hypereutectoid alloy from the upper transformation temperature, it will usually be the excess solutes that crystallize-out first, forming the pro-eutectoid. This continues until the concentration in the remaining alloy becomes eutectoid, which then crystallizes into a separate microstructure.
-Geochemical fractionations Certain isotopes of trace metals are preferentially oxidized or reduced; thus, transitions between redox species of the metal ions (e.g., Fe2+ → Fe3+) are fractionating, resulting in different isotopic compositions between the different redox pools in the environment. Additionally, at high temperatures, metals ions can evaporate (and subsequently condense upon cooling), and the relative differences in isotope masses of a given heavy metal leads to fractionation during these evaporation and condensation processes. Diffusion of isotopes through a solution or material can also result in fractionations, as the lighter mass isotopes are able to diffuse at a faster rate. Additionally, isotopes can have slight variations in their solubility and other chemical and physical properties, which can also drive fractionation.
-Like oil and water, a molten metal may not always mix with another element. For example, pure iron is almost completely insoluble with copper. Even when the constituents are soluble, each will usually have a saturation point, beyond which no more of the constituent can be added. Iron, for example, can hold a maximum of 6.67% carbon. Although the elements of an alloy usually must be soluble in the liquid state, they may not always be soluble in the solid state. If the metals remain soluble when solid, the alloy forms a solid solution, becoming a homogeneous structure consisting of identical crystals, called a phase. If as the mixture cools the constituents become insoluble, they may separate to form two or more different types of crystals, creating a heterogeneous microstructure of different phases, some with more of one constituent than the other. However, in other alloys, the insoluble elements may not separate until after crystallization occurs. If cooled very quickly, they first crystallize as a homogeneous phase, but they are supersaturated with the secondary constituents. As time passes, the atoms of these supersaturated alloys can separate from the crystal lattice, becoming more stable, and forming a second phase that serves to reinforce the crystals internally.
-For example, a hypoeutectoid steel contains less than 0.77% carbon. Upon cooling a hypoeutectoid steel from the austenite transformation temperature, small islands of proeutectoid-ferrite will form. These will continue to grow and the carbon will recede until the eutectoid concentration in the rest of the steel is reached. This eutectoid mixture will then crystallize as a microstructure of pearlite. Since ferrite is softer than pearlite, the two microstructures combine to increase the ductility of the alloy. Consequently, the hardenability of the alloy is lowered.
 A. The excess base metal will often solidify, becoming the proeutectoid until the remaining concentration of solutes reaches the eutectoid level, which will then crystallize as a separate microstructure.
 B. The excess base metal will often crystallize-out, becoming the proeutectoid until the remaining concentration of solutes reaches the eutectoid level, which will then crystallize as a separate microstructure.
 C. The excess base metal will often dissolve, becoming the proeutectoid until the remaining concentration of solutes reaches the eutectoid level, which will then crystallize as a separate microstructure.
 D. The excess base metal will often liquefy, becoming the proeutectoid until the remaining concentration of solutes reaches the eutectoid level, which will then crystallize as a separate microstructure.
 E. The excess base metal will often evaporate, becoming the proeutectoid until the remaining concentration of solutes reaches the eutectoid level, which will then crystallize as a separate microstructure. "
"What is the relationship between mass, force, and acceleration, according to Sir Isaac Newton's laws of motion?","Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What is the relationship between mass, force, and acceleration, according to Sir Isaac Newton's laws of motion?
 Context: -Mass is (among other properties) an inertial property; that is, the tendency of an object to remain at constant velocity unless acted upon by an outside force. Under Sir Isaac Newton's 336-year-old laws of motion and an important formula that sprang from his work, F = ma, an object with a mass, m, of one kilogram accelerates, a, at one meter per second per second (about one-tenth the acceleration due to earth's gravity) when acted upon by a force, F, of one newton.
-Mass – is both a property of a physical body and a measure of its resistance to acceleration (rate of change of velocity with respect to time) when a net force is applied. An object's mass also determines the strength of its gravitational attraction to other bodies. The SI base unit of mass is the kilogram (kg). In physics, mass is not the same as weight, even though mass is often determined by measuring the object's weight using a spring scale, rather than balance scale comparing it directly with known masses. An object on the Moon would weigh less than it does on Earth because of the lower gravity, but it would still have the same mass. This is because weight is a force, while mass is the property that (along with gravity) determines the strength of this force.
-Mass Mass refers to the intrinsic property of all material objects to resist changes in their momentum. Weight, on the other hand, refers to the downward force produced when a mass is in a gravitational field. In free fall, (no net gravitational forces) objects lack weight but retain their mass. The Imperial units of mass include the ounce, pound, and ton. The metric units gram and kilogram are units of mass.
-In more formal terms, Newton's second law of motion states that the force exerted on an object is directly proportional to the acceleration hence acquired by that object, thus: F=ma, where  m represents the mass of the object undergoing an acceleration  a . As a result, the newton may be defined in terms of the kilogram ( kg ), metre ( m ), and second ( s ) as kg ⋅ms2.
-In scientific contexts, mass is the amount of ""matter"" in an object (though ""matter"" may be difficult to define), but weight is the force exerted on an object's matter by gravity. At the Earth's surface, an object whose mass is exactly one kilogram weighs approximately 9.81 newtons, the product of its mass and the gravitational field strength there. The object's weight is less on Mars, where gravity is weaker; more on Saturn, where gravity is stronger; and very small in space, far from significant sources of gravity, but it always has the same mass.
 A. Mass is a property that determines the weight of an object. According to Newton's laws of motion and the formula F = ma, an object with a mass of one kilogram accelerates at one meter per second per second when acted upon by a force of one newton.
 B. Mass is an inertial property that determines an object's tendency to remain at constant velocity unless acted upon by an outside force. According to Newton's laws of motion and the formula F = ma, an object with a mass of one kilogram accelerates at ten meters per second per second when acted upon by a force of one newton.
 C. Mass is an inertial property that determines an object's tendency to remain at constant velocity unless acted upon by an outside force. According to Newton's laws of motion and the formula F = ma, an object with a mass of one kilogram accelerates at ten meters per second per second when acted upon by a force of ten newtons.
 D. Mass is an inertial property that determines an object's tendency to remain at constant velocity unless acted upon by an outside force. According to Newton's laws of motion and the formula F = ma, an object with a mass of one kilogram accelerates at one meter per second per second when acted upon by a force of one newton.
 E. Mass is a property that determines the size of an object. According to Newton's laws of motion and the formula F = ma, an object with a mass of one kilogram accelerates at one meter per second per second when acted upon by a force of ten newtons. "
What did Arthur Eddington discover about two of Einstein's types of gravitational waves?,"Answer the following question with A, B, C, D, or E. You can use the context provided. Question: What did Arthur Eddington discover about two of Einstein's types of gravitational waves?
 Context: -The possibility of gravitational waves was discussed in 1893 by Oliver Heaviside, using the analogy between the inverse-square law of gravitation and the electrostatic force. In 1905, Henri Poincaré proposed gravitational waves, emanating from a body and propagating at the speed of light, as being required by the Lorentz transformations and suggested that, in analogy to an accelerating electrical charge producing electromagnetic waves, accelerated masses in a relativistic field theory of gravity should produce gravitational waves. When Einstein published his general theory of relativity in 1915, he was skeptical of Poincaré's idea since the theory implied there were no ""gravitational dipoles"". Nonetheless, he still pursued the idea and based on various approximations came to the conclusion there must, in fact, be three types of gravitational waves (dubbed longitudinal–longitudinal, transverse–longitudinal, and transverse–transverse by Hermann Weyl).However, the nature of Einstein's approximations led many (including Einstein himself) to doubt the result. In 1922, Arthur Eddington showed that two of Einstein's types of waves were artifacts of the coordinate system he used, and could be made to propagate at any speed by choosing appropriate coordinates, leading Eddington to jest that they ""propagate at the speed of thought"".: 72  This also cast doubt on the physicality of the third (transverse–transverse) type that Eddington showed always propagate at the speed of light regardless of coordinate system. In 1936, Einstein and Nathan Rosen submitted a paper to Physical Review in which they claimed gravitational waves could not exist in the full general theory of relativity because any such solution of the field equations would have a singularity. The journal sent their manuscript to be reviewed by Howard P. Robertson, who anonymously reported that the singularities in question were simply the harmless coordinate singularities of the employed cylindrical coordinates. Einstein, who was unfamiliar with the concept of peer review, angrily withdrew the manuscript, never to publish in Physical Review again. Nonetheless, his assistant Leopold Infeld, who had been in contact with Robertson, convinced Einstein that the criticism was correct, and the paper was rewritten with the opposite conclusion and published elsewhere.: 79ff  In 1956, Felix Pirani remedied the confusion caused by the use of various coordinate systems by rephrasing the gravitational waves in terms of the manifestly observable Riemann curvature tensor.At the time, Pirani's work was overshadowed by the community's focus on a different question: whether gravitational waves could transmit energy. This matter was settled by a thought experiment proposed by Richard Feynman during the first ""GR"" conference at Chapel Hill in 1957. In short, his argument known as the ""sticky bead argument"" notes that if one takes a rod with beads then the effect of a passing gravitational wave would be to move the beads along the rod; friction would then produce heat, implying that the passing wave had done work. Shortly after, Hermann Bondi, published a detailed version of the ""sticky bead argument"". This later lead to a series of articles (1959 to 1989)  by Bondi and Pirani that established the existence of plane wave solutions for gravitational waves.After the Chapel Hill conference, Joseph Weber started designing and building the first gravitational wave detectors now known as Weber bars. In 1969, Weber claimed to have detected the first gravitational waves, and by 1970 he was ""detecting"" signals regularly from the Galactic Center; however, the frequency of detection soon raised doubts on the validity of his observations as the implied rate of energy loss of the Milky Way would drain our galaxy of energy on a timescale much shorter than its inferred age. These doubts were strengthened when, by the mid-1970s, repeated experiments from other groups building their own Weber bars across the globe failed to find any signals, and by the late 1970s consensus was that Weber's results were spurious.In the same period, the first indirect evidence of gravitational waves was discovered. In 1974, Russell Alan Hulse and Joseph Hooton Taylor, Jr. discovered the first binary pulsar, which earned them the 1993 Nobel Prize in Physics. Pulsar timing observations over the next decade showed a gradual decay of the orbital period of the Hulse–Taylor pulsar that matched the loss of energy and angular momentum in gravitational radiation predicted by general relativity.This indirect detection of gravitational waves motivated further searches, despite Weber's discredited result. Some groups continued to improve Weber's original concept, while others pursued the detection of gravitational waves using laser interferometers. The idea of using a laser interferometer for this seems to have been floated independently by various people, including M. E. Gertsenshtein and V. I. Pustovoit in 1962, and Vladimir B. Braginskiĭ in 1966. The first prototypes were developed in the 1970s by Robert L. Forward and Rainer Weiss. In the decades that followed, ever more sensitive instruments were constructed, culminating in the construction of GEO600, LIGO, and Virgo.After years of producing null results, improved detectors became operational in 2015. On 11 February 2016, the LIGO-Virgo collaborations announced the first observation of gravitational waves, from a signal (dubbed GW150914) detected at 09:50:45 GMT on 14 September 2015 of two black holes with masses of 29 and 36 solar masses merging about 1.3 billion light-years away. During the final fraction of a second of the merger, it released more than 50 times the power of all the stars in the observable universe combined. The signal increased in frequency from 35 to 250 Hz over 10 cycles (5 orbits) as it rose in strength for a period of 0.2 second. The mass of the new merged black hole was 62 solar masses. Energy equivalent to three solar masses was emitted as gravitational waves. The signal was seen by both LIGO detectors in Livingston and Hanford, with a time difference of 7 milliseconds due to the angle between the two detectors and the source. The signal came from the Southern Celestial Hemisphere, in the rough direction of (but much farther away than) the Magellanic Clouds. The confidence level of this being an observation of gravitational waves was 99.99994%.A year earlier, the BICEP2 collaboration claimed that they had detected the imprint of gravitational waves in the cosmic microwave background. However, they were later forced to retract this result.In 2017, the Nobel Prize in Physics was awarded to Rainer Weiss, Kip Thorne and Barry Barish for their role in the detection of gravitational waves.In 2023, NANOGrav, EPTA, PPTA, and IPTA announced that they found evidence of a universal gravitational wave background. North American Nanohertz Observatory for Gravitational Waves states, that they were created over cosmological time scales by supermassive black holes, identifying the distinctive Hellings-Downs curve in 15 years of radio observations of 25 pulsars.
-In 1922, Arthur Stanley Eddington wrote a paper expressing (apparently for the first time) the view that gravitational waves are in essence ripples in coordinates, and have no physical meaning. He did not appreciate Einstein's arguments that the waves are real.In 1936, together with Nathan Rosen, Einstein rediscovered the Beck vacuums, a family of exact gravitational wave solutions with cylindrical symmetry (sometimes also called Einstein–Rosen waves). While investigating the motion of test particles in these solutions, Einstein and Rosen became convinced that gravitational waves were unstable to collapse. Einstein reversed himself and declared that gravitational radiation was not after all a prediction of his theory. Einstein wrote to his friend Max Born Together with a young collaborator, I arrived at the interesting result that gravitational waves do not exist, though they had been assumed a certainty to the first approximation. This shows that the nonlinear field equations can show us more, or rather limit us more, than we have believed up till now.
-Gravitational waves Predicted in 1916 by Albert Einstein, there are gravitational waves: ripples in the metric of spacetime that propagate at the speed of light. These are one of several analogies between weak-field gravity and electromagnetism in that, they are analogous to electromagnetic waves. On 11 February 2016, the Advanced LIGO team announced that they had directly detected gravitational waves from a pair of black holes merging.The simplest type of such a wave can be visualized by its action on a ring of freely floating particles. A sine wave propagating through such a ring towards the reader distorts the ring in a characteristic, rhythmic fashion (animated image to the right). Since Einstein's equations are non-linear, arbitrarily strong gravitational waves do not obey linear superposition, making their description difficult. However, linear approximations of gravitational waves are sufficiently accurate to describe the exceedingly weak waves that are expected to arrive here on Earth from far-off cosmic events, which typically result in relative distances increasing and decreasing by  10 21 or less. Data analysis methods routinely make use of the fact that these linearized waves can be Fourier decomposed.Some exact solutions describe gravitational waves without any approximation, e.g., a wave train traveling through empty space or Gowdy universes, varieties of an expanding cosmos filled with gravitational waves. But for gravitational waves produced in astrophysically relevant situations, such as the merger of two black holes, numerical methods are presently the only way to construct appropriate models.
-Albert Einstein originally predicted the existence of gravitational waves in 1916, on the basis of his theory of general relativity. General relativity interprets gravity as a consequence of distortions in space-time, caused by mass. Therefore, Einstein also predicted that events in the cosmos would cause ""ripples"" in space-time – distortions of space-time itself – which would spread outward, although they would be so minuscule that they would be nearly impossible to detect by any technology foreseen at that time. It was also predicted that objects moving in an orbit would lose energy for this reason (a consequence of the law of conservation of energy), as some energy would be given off as gravitational waves, although this would be insignificantly small in all but the most extreme cases.One case where gravitational waves would be strongest is during the final moments of the merger of two compact objects such as neutron stars or black holes. Over a span of millions of years, binary neutron stars, and binary black holes lose energy, largely through gravitational waves, and as a result, they spiral in towards each other. At the very end of this process, the two objects will reach extreme velocities, and in the final fraction of a second of their merger a substantial amount of their mass would theoretically be converted into gravitational energy, and travel outward as gravitational waves, allowing a greater than usual chance for detection. However, since little was known about the number of compact binaries in the universe and reaching that final stage can be very slow, there was little certainty as to how often such events might happen.
-At the end of the 19th century, the electromagnetic field was understood as a collection of two vector fields in space. Nowadays, one recognizes this as a single antisymmetric 2nd-rank tensor field in spacetime.
Gravitation in general relativity Einstein's theory of gravity, called general relativity, is another example of a field theory. Here the principal field is the metric tensor, a symmetric 2nd-rank tensor field in spacetime. This replaces Newton's law of universal gravitation.
Waves as fields Waves can be constructed as physical fields, due to their finite propagation speed and causal nature when a simplified physical model of an isolated closed system is set. They are also subject to the inverse-square law.
For electromagnetic waves, there are optical fields, and terms such as near- and far-field limits for diffraction. In practice though, the field theories of optics are superseded by the electromagnetic field theory of Maxwell.
 A. Arthur Eddington showed that two of Einstein's types of waves were artifacts of the coordinate system he used, and could only be made to propagate at the speed of gravity by choosing appropriate coordinates.
 B. Arthur Eddington showed that two of Einstein's types of waves were artifacts of the coordinate system he used, and could only be made to propagate at the speed of sound by choosing appropriate coordinates.
 C. Arthur Eddington showed that two of Einstein's types of waves were artifacts of the coordinate system he used, and could be made to propagate at any speed by choosing appropriate coordinates.
 D. Arthur Eddington showed that two of Einstein's types of waves were artifacts of the coordinate system he used, and could not be made to propagate at any speed by choosing appropriate coordinates.
 E. Arthur Eddington showed that two of Einstein's types of waves were artifacts of the coordinate system he used, and could only be made to propagate at the speed of light by choosing appropriate coordinates. "

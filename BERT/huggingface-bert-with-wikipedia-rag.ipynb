{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":54662,"databundleVersionId":6169864,"sourceType":"competition"},{"sourceId":5632975,"sourceType":"datasetVersion","datasetId":3238926},{"sourceId":6146260,"sourceType":"datasetVersion","datasetId":3521629},{"sourceId":6359012,"sourceType":"datasetVersion","datasetId":3662908},{"sourceId":6476221,"sourceType":"datasetVersion","datasetId":3741139},{"sourceId":6536614,"sourceType":"datasetVersion","datasetId":3778974},{"sourceId":6537899,"sourceType":"datasetVersion","datasetId":3779689},{"sourceId":6602268,"sourceType":"datasetVersion","datasetId":3809212},{"sourceId":6960458,"sourceType":"datasetVersion","datasetId":3998432}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **BERT with RAG**","metadata":{}},{"cell_type":"code","source":"# Installing offline dependencies\n!pip install -U --no-deps /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install -U --no-deps /kaggle/input/datasets-214/datasets-2.14.5-py3-none-any.whl\n!pip install -U --no-deps /kaggle/input/optimum-113/optimum-1.13.2-py3-none-any.whl\n!pip install -U --no-deps /kaggle/input/transformers-432/transformers-4.32.1-py3-none-any.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-16T10:34:48.586178Z","iopub.execute_input":"2023-11-16T10:34:48.587062Z","iopub.status.idle":"2023-11-16T10:35:07.540537Z","shell.execute_reply.started":"2023-11-16T10:34:48.587025Z","shell.execute_reply":"2023-11-16T10:35:07.539499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nfrom time import time\nfrom concurrent.futures import ThreadPoolExecutor\nimport ctypes\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\n# For RAG\nimport faiss\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_from_disk, Dataset\n\nNUM_TITLES = 5\nMAX_SEQ_LEN = 512\nMODEL_PATH = \"/kaggle/input/bge-small-faiss/\"\n\n# For LLM\nfrom transformers import AutoTokenizer, AutoModel\n\nMAX_LENGTH = 4096\nMAX_CONTEXT = 1200","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:35:17.959327Z","iopub.execute_input":"2023-11-16T10:35:17.960409Z","iopub.status.idle":"2023-11-16T10:35:23.598231Z","shell.execute_reply.started":"2023-11-16T10:35:17.960363Z","shell.execute_reply":"2023-11-16T10:35:23.597285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/train.csv\", index_col=\"id\")","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:36:59.204219Z","iopub.execute_input":"2023-11-16T10:36:59.205306Z","iopub.status.idle":"2023-11-16T10:36:59.235293Z","shell.execute_reply.started":"2023-11-16T10:36:59.205269Z","shell.execute_reply":"2023-11-16T10:36:59.234384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Wikipedia Retrieval Augmented Generation (RAG)\n\nThe following code is adapted from [the notebook of @MGöksu](https://www.kaggle.com/code/mgoksu/0-807-sharing-my-trained-with-context-model) and [the notebook of @MB](https://www.kaggle.com/code/mbanaei/86-2-with-only-270k-articles/notebook). We use the [bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) to embed the Wikipedia dataset.","metadata":{}},{"cell_type":"code","source":"# New SentenceTransformer class similar to the one used in @Mgöksu notebook but relying on the transformers library only\n\nclass SentenceTransformer:\n    def __init__(self, checkpoint, device=\"cuda:0\"):\n        self.device = device\n        self.checkpoint = checkpoint\n        self.model = AutoModel.from_pretrained(checkpoint).to(self.device).half()\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n    def transform(self, batch):\n        tokens = self.tokenizer(batch[\"text\"], truncation=True, padding=True, return_tensors=\"pt\", max_length=MAX_SEQ_LEN)\n        return tokens.to(self.device)  \n\n    def get_dataloader(self, sentences, batch_size=32):\n        sentences = [\"Represent this sentence for searching relevant passages: \" + x for x in sentences]\n        dataset = Dataset.from_dict({\"text\": sentences})\n        dataset.set_transform(self.transform)\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n        return dataloader\n\n    def encode(self, sentences, show_progress_bar=False, batch_size=32):\n        dataloader = self.get_dataloader(sentences, batch_size=batch_size)\n        pbar = tqdm(dataloader) if show_progress_bar else dataloader\n\n        embeddings = []\n        for batch in pbar:\n            with torch.no_grad():\n                e = self.model(**batch).pooler_output\n                e = F.normalize(e, p=2, dim=1)\n                embeddings.append(e.detach().cpu().numpy())\n        embeddings = np.concatenate(embeddings, axis=0)\n        return embeddings","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:37:00.774998Z","iopub.execute_input":"2023-11-16T10:37:00.775403Z","iopub.status.idle":"2023-11-16T10:37:00.786130Z","shell.execute_reply.started":"2023-11-16T10:37:00.775371Z","shell.execute_reply":"2023-11-16T10:37:00.785197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load embedding model\nstart = time()\nprint(f\"Starting prompt embedding, t={time() - start :.1f}s\")\nmodel = SentenceTransformer(MODEL_PATH, device=\"cuda:0\")\n\n# Get embeddings of prompts\nf = lambda row : \" \".join([row[\"prompt\"], row[\"A\"], row[\"B\"], row[\"C\"], row[\"D\"], row[\"E\"]])\ninputs = df.apply(f, axis=1).values # better results than prompt only\nprompt_embeddings = model.encode(inputs, show_progress_bar=False)\n\n# Search closest sentences in the wikipedia index \nprint(f\"Loading faiss index, t={time() - start :.1f}s\")\nfaiss_index = faiss.read_index(MODEL_PATH + '/faiss.index')\n# faiss_index = faiss.index_cpu_to_all_gpus(faiss_index) # causes OOM, and not that long on CPU\n\nprint(f\"Starting text search, t={time() - start :.1f}s\")\nsearch_index = faiss_index.search(np.float32(prompt_embeddings), NUM_TITLES)[1]\n\nprint(f\"Starting context extraction, t={time() - start :.1f}s\")\ndataset = load_from_disk(\"/kaggle/input/all-paraphs-parsed-expanded\")\nfor i in range(len(df)):\n    df.loc[i, \"context\"] = \"-\" + \"\\n-\".join([dataset[int(j)][\"text\"] for j in search_index[i]])\n\n# Free memory\nfaiss_index.reset()\ndel faiss_index, prompt_embeddings, model, dataset\n# clean_memory()\nprint(f\"Context added, t={time() - start :.1f}s\")","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:37:03.094009Z","iopub.execute_input":"2023-11-16T10:37:03.094399Z","iopub.status.idle":"2023-11-16T10:38:04.771922Z","shell.execute_reply.started":"2023-11-16T10:37:03.094367Z","shell.execute_reply":"2023-11-16T10:38:04.770538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nfeatures = ['prompt', 'A', 'B', 'C', 'D', 'E', 'context']\n\ny = df['answer']\nX = df[features]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:38:04.773881Z","iopub.execute_input":"2023-11-16T10:38:04.774243Z","iopub.status.idle":"2023-11-16T10:38:05.130712Z","shell.execute_reply.started":"2023-11-16T10:38:04.774210Z","shell.execute_reply":"2023-11-16T10:38:05.129649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\nmodel_dir = 'bert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(model_dir)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:38:05.132072Z","iopub.execute_input":"2023-11-16T10:38:05.132395Z","iopub.status.idle":"2023-11-16T10:38:06.291480Z","shell.execute_reply.started":"2023-11-16T10:38:05.132364Z","shell.execute_reply":"2023-11-16T10:38:06.290539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"options = 'ABCDE'\nindices = list(range(5))\n\noption_index = {option: index for option, index in zip(options, indices)}\nindex_option = {index: option for option, index in zip(options, indices)}\ntrain_dataset = Dataset.from_pandas(df)\n\ndef preprocess(examples):\n    text = examples['context'] + examples['prompt']\n    first_sentences = [text] * 5\n    second_sentences = []\n    for option in options:\n        second_sentences.append(examples[option])\n    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n    tokenized_examples['label'] = option_index[examples['answer']]\n    return tokenized_examples\n\ntokenized_train_ds = train_dataset.map(preprocess, batched=False, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer', 'context'])","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:38:06.294142Z","iopub.execute_input":"2023-11-16T10:38:06.294510Z","iopub.status.idle":"2023-11-16T10:38:08.472244Z","shell.execute_reply.started":"2023-11-16T10:38:06.294477Z","shell.execute_reply":"2023-11-16T10:38:08.471372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Following datacollator (adapted from https://huggingface.co/docs/transformers/tasks/multiple_choice)\n# will dynamically pad our questions at batch-time so we don't have to make every question the length\n# of our longest question.\nfrom dataclasses import dataclass\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom typing import Optional, Union\nimport torch\n\n@dataclass\nclass DataCollatorForMultipleChoice:\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    \n    def __call__(self, features):\n        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n        labels = [feature.pop(label_name) for feature in features]\n        batch_size = len(features)\n        num_choices = len(features[0]['input_ids'])\n        flattened_features = [\n            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n        ]\n        flattened_features = sum(flattened_features, [])\n        \n        batch = self.tokenizer.pad(\n            flattened_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors='pt',\n        )\n        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n        return batch","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:38:08.473448Z","iopub.execute_input":"2023-11-16T10:38:08.473718Z","iopub.status.idle":"2023-11-16T10:38:08.483835Z","shell.execute_reply.started":"2023-11-16T10:38:08.473695Z","shell.execute_reply":"2023-11-16T10:38:08.482858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\nmodel = AutoModelForMultipleChoice.from_pretrained(model_dir)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:38:08.484934Z","iopub.execute_input":"2023-11-16T10:38:08.485254Z","iopub.status.idle":"2023-11-16T10:38:11.630131Z","shell.execute_reply.started":"2023-11-16T10:38:08.485222Z","shell.execute_reply":"2023-11-16T10:38:11.629381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dir = 'bert-base-context'\ntraining_args = TrainingArguments(\n    output_dir=model_dir,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    learning_rate=5e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=15,\n    weight_decay=0.01,\n    report_to='none',\n    logging_steps=50,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:38:11.631402Z","iopub.execute_input":"2023-11-16T10:38:11.631745Z","iopub.status.idle":"2023-11-16T10:38:11.639077Z","shell.execute_reply.started":"2023-11-16T10:38:11.631712Z","shell.execute_reply":"2023-11-16T10:38:11.638201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:38:11.640083Z","iopub.execute_input":"2023-11-16T10:38:11.640364Z","iopub.status.idle":"2023-11-16T10:38:26.277305Z","shell.execute_reply.started":"2023-11-16T10:38:11.640317Z","shell.execute_reply":"2023-11-16T10:38:26.276386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import evaluate\n\naccuracy = evaluate.load(\"accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:38:26.278919Z","iopub.execute_input":"2023-11-16T10:38:26.279299Z","iopub.status.idle":"2023-11-16T10:38:29.009942Z","shell.execute_reply.started":"2023-11-16T10:38:26.279257Z","shell.execute_reply":"2023-11-16T10:38:29.009189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    f1 = f1_score(labels, preds, average=\"weighted\")\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc, \"f1\": f1}","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:38:29.013124Z","iopub.execute_input":"2023-11-16T10:38:29.013483Z","iopub.status.idle":"2023-11-16T10:38:29.019101Z","shell.execute_reply.started":"2023-11-16T10:38:29.013458Z","shell.execute_reply":"2023-11-16T10:38:29.018040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_ds,\n    eval_dataset=tokenized_train_ds,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:38:29.160799Z","iopub.execute_input":"2023-11-16T10:38:29.162017Z","iopub.status.idle":"2023-11-16T10:38:29.172017Z","shell.execute_reply.started":"2023-11-16T10:38:29.161978Z","shell.execute_reply":"2023-11-16T10:38:29.171030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install psutil GPUtil","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:40:43.707650Z","iopub.execute_input":"2023-11-16T10:40:43.708366Z","iopub.status.idle":"2023-11-16T10:40:57.659232Z","shell.execute_reply.started":"2023-11-16T10:40:43.708310Z","shell.execute_reply":"2023-11-16T10:40:57.658200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import threading\nimport psutil\nimport GPUtil\nimport time\nimport matplotlib.pyplot as plt\n\n# Function to monitor CPU and GPU usage\ndef monitor_resources(stop_event, interval=1):\n    cpu_usage = []\n    gpu_usage = []\n    timestamps = []\n\n    while not stop_event.is_set():\n        timestamps.append(time.time())\n        cpu_usage.append(psutil.cpu_percent(interval=None))\n\n        gpus = GPUtil.getGPUs()\n        gpu_usage.append(gpus[0].load * 100 if gpus else 0)  # Assumes one GPU\n\n        time.sleep(interval)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(timestamps, cpu_usage, label='CPU Usage (%)')\n    plt.plot(timestamps, gpu_usage, label='GPU Usage (%)')\n    plt.xlabel('Time (s)')\n    plt.ylabel('Usage (%)')\n    plt.title('CPU and GPU Usage Over Time')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n    \n    print(timestamps[-1] - timestamps[0])\n    print(max(cpu_usage))\n    print(max(gpu_usage))\n\n# Monitoring setup\nstop_event = threading.Event()\nmonitor_thread = threading.Thread(target=monitor_resources, args=(stop_event,))\n\n# Start monitoring\nmonitor_thread.start()\n\n# Start training\ntrainer.train()\n\n# Stop monitoring\nstop_event.set()\nmonitor_thread.join()\n\n# At this point, the monitoring thread has completed\nprint(\"Training and monitoring completed.\")","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:41:03.423095Z","iopub.execute_input":"2023-11-16T10:41:03.423505Z","iopub.status.idle":"2023-11-16T10:59:44.881748Z","shell.execute_reply.started":"2023-11-16T10:41:03.423471Z","shell.execute_reply":"2023-11-16T10:59:44.880800Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "202df06c",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-11-05T19:52:45.106282Z",
     "iopub.status.busy": "2023-11-05T19:52:45.105947Z",
     "iopub.status.idle": "2023-11-05T19:53:08.351747Z",
     "shell.execute_reply": "2023-11-05T19:53:08.350137Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 23.257641,
     "end_time": "2023-11-05T19:53:08.354295",
     "exception": false,
     "start_time": "2023-11-05T19:52:45.096654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n",
      "Archive:  /usr/share/nltk_data/corpora/wordnet.zip\r\n",
      "   creating: /usr/share/nltk_data/corpora/wordnet/\r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/README  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efdecb63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:53:08.374547Z",
     "iopub.status.busy": "2023-11-05T19:53:08.372642Z",
     "iopub.status.idle": "2023-11-05T19:53:08.402004Z",
     "shell.execute_reply": "2023-11-05T19:53:08.400421Z"
    },
    "papermill": {
     "duration": 0.042176,
     "end_time": "2023-11-05T19:53:08.404687",
     "exception": false,
     "start_time": "2023-11-05T19:53:08.362511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb658b21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:53:08.422173Z",
     "iopub.status.busy": "2023-11-05T19:53:08.421788Z",
     "iopub.status.idle": "2023-11-05T19:53:08.427969Z",
     "shell.execute_reply": "2023-11-05T19:53:08.426438Z"
    },
    "papermill": {
     "duration": 0.018062,
     "end_time": "2023-11-05T19:53:08.430753",
     "exception": false,
     "start_time": "2023-11-05T19:53:08.412691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sub = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d451e171",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:53:08.447749Z",
     "iopub.status.busy": "2023-11-05T19:53:08.447370Z",
     "iopub.status.idle": "2023-11-05T19:53:09.904118Z",
     "shell.execute_reply": "2023-11-05T19:53:09.902486Z"
    },
    "papermill": {
     "duration": 1.468028,
     "end_time": "2023-11-05T19:53:09.906602",
     "exception": false,
     "start_time": "2023-11-05T19:53:08.438574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lower(text):\n",
    "    return text.lower()\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "def preprocess(text):\n",
    "    text = lower(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize_words(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "588421e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:53:09.923669Z",
     "iopub.status.busy": "2023-11-05T19:53:09.923329Z",
     "iopub.status.idle": "2023-11-05T19:53:11.317831Z",
     "shell.execute_reply": "2023-11-05T19:53:11.315911Z"
    },
    "papermill": {
     "duration": 1.406011,
     "end_time": "2023-11-05T19:53:11.320426",
     "exception": false,
     "start_time": "2023-11-05T19:53:09.914415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sub['prompt'] = df[\"prompt\"].apply(lambda text: preprocess(text))\n",
    "df_sub['A'] = df[\"A\"].apply(lambda text: preprocess(text))\n",
    "df_sub['B'] = df[\"B\"].apply(lambda text: preprocess(text))\n",
    "df_sub['C'] = df[\"C\"].apply(lambda text: preprocess(text))\n",
    "df_sub['D'] = df[\"D\"].apply(lambda text: preprocess(text))\n",
    "df_sub['E'] = df[\"E\"].apply(lambda text: preprocess(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3076822",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:53:11.342484Z",
     "iopub.status.busy": "2023-11-05T19:53:11.342108Z",
     "iopub.status.idle": "2023-11-05T19:53:11.351602Z",
     "shell.execute_reply": "2023-11-05T19:53:11.350368Z"
    },
    "papermill": {
     "duration": 0.024215,
     "end_time": "2023-11-05T19:53:11.354813",
     "exception": false,
     "start_time": "2023-11-05T19:53:11.330598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vocabs(df, col):\n",
    "    split_col = df[col].apply(lambda x: x.split())\n",
    "    uniq = list()\n",
    "    for row in split_col:\n",
    "        uniq = uniq + row\n",
    "    return list(set(uniq))\n",
    "\n",
    "uniq_vocabs = vocabs(df_sub,'prompt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e1c0a22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:53:11.374536Z",
     "iopub.status.busy": "2023-11-05T19:53:11.374185Z",
     "iopub.status.idle": "2023-11-05T19:53:11.382479Z",
     "shell.execute_reply": "2023-11-05T19:53:11.380020Z"
    },
    "papermill": {
     "duration": 0.020573,
     "end_time": "2023-11-05T19:53:11.384847",
     "exception": false,
     "start_time": "2023-11-05T19:53:11.364274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vectorize(tokens, filtered_vocabs):\n",
    "    ''' This function takes list of words in a sentence as input \n",
    "    and returns a vector of size of filtered_vocab.It puts 0 if the \n",
    "    word is not present in tokens and count of token if present.'''\n",
    "    vector=[]\n",
    "    for w in filtered_vocabs:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9d6fc66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:53:11.403720Z",
     "iopub.status.busy": "2023-11-05T19:53:11.402447Z",
     "iopub.status.idle": "2023-11-05T19:53:11.692499Z",
     "shell.execute_reply": "2023-11-05T19:53:11.691492Z"
    },
    "papermill": {
     "duration": 0.301231,
     "end_time": "2023-11-05T19:53:11.694761",
     "exception": false,
     "start_time": "2023-11-05T19:53:11.393530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sub['prompt'] =  df_sub['prompt'].apply(lambda x: vectorize(x.split(), uniq_vocabs))\n",
    "df_sub['A'] =  df_sub['A'].apply(lambda x: vectorize(x.split(), uniq_vocabs))\n",
    "df_sub['B'] =  df_sub['B'].apply(lambda x: vectorize(x.split(), uniq_vocabs))\n",
    "df_sub['C'] =  df_sub['C'].apply(lambda x: vectorize(x.split(), uniq_vocabs))\n",
    "df_sub['D'] =  df_sub['D'].apply(lambda x: vectorize(x.split(), uniq_vocabs))\n",
    "df_sub['E'] =  df_sub['E'].apply(lambda x: vectorize(x.split(), uniq_vocabs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e12eda85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:53:11.712504Z",
     "iopub.status.busy": "2023-11-05T19:53:11.711304Z",
     "iopub.status.idle": "2023-11-05T19:53:12.054237Z",
     "shell.execute_reply": "2023-11-05T19:53:12.052965Z"
    },
    "papermill": {
     "duration": 0.35467,
     "end_time": "2023-11-05T19:53:12.057090",
     "exception": false,
     "start_time": "2023-11-05T19:53:11.702420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_jaccard_sim(a, b):\n",
    "    mult1 = np.array(a)*np.array(b)    \n",
    "    mult1 = mult1 > 0 \n",
    "    mult2 = np.array(a) + np.array(b)\n",
    "    mult2 = mult2 > 0 \n",
    "    return float(sum(mult1)/sum(mult2))\n",
    "\n",
    "choice = np.array(['A','B','C','D','E'])\n",
    "\n",
    "def predict(row, prompt, options):\n",
    "    sel = []\n",
    "    for opt in options:\n",
    "        score = get_jaccard_sim(row[prompt],row[opt])\n",
    "        sel.append(score)\n",
    "    order = np.argsort(np.array(sel))[::-1]\n",
    "    return ' '.join(choice[order][:3])\n",
    "        \n",
    "df_sub['prediction'] = df_sub.apply(lambda row: predict(row,'prompt',choice), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2826f296",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:53:12.076593Z",
     "iopub.status.busy": "2023-11-05T19:53:12.074846Z",
     "iopub.status.idle": "2023-11-05T19:53:12.312805Z",
     "shell.execute_reply": "2023-11-05T19:53:12.311463Z"
    },
    "papermill": {
     "duration": 0.249195,
     "end_time": "2023-11-05T19:53:12.314945",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.065750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20/4022607774.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))\n"
     ]
    }
   ],
   "source": [
    "def cosine_sim(a,b):\n",
    "    return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "def predict(row, prompt, options):\n",
    "    sel = []\n",
    "    for opt in options:\n",
    "        score = cosine_sim(row[prompt],row[opt])\n",
    "        sel.append(score)\n",
    "    order = np.argsort(np.array(sel))[::-1]\n",
    "    return ' '.join(choice[order][:3])\n",
    "\n",
    "df_sub['prediction_2'] = df_sub.apply(lambda row: predict(row,'prompt',choice), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86ba4417",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:53:12.332396Z",
     "iopub.status.busy": "2023-11-05T19:53:12.332073Z",
     "iopub.status.idle": "2023-11-05T19:53:12.342475Z",
     "shell.execute_reply": "2023-11-05T19:53:12.341077Z"
    },
    "papermill": {
     "duration": 0.022223,
     "end_time": "2023-11-05T19:53:12.345212",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.322989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame()\n",
    "pred_col = 'prediction_2'\n",
    "submission_df['id'] = df_sub['id']\n",
    "submission_df['prediction'] = df_sub[pred_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12b613e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:53:12.363370Z",
     "iopub.status.busy": "2023-11-05T19:53:12.362958Z",
     "iopub.status.idle": "2023-11-05T19:53:12.373149Z",
     "shell.execute_reply": "2023-11-05T19:53:12.372020Z"
    },
    "papermill": {
     "duration": 0.022261,
     "end_time": "2023-11-05T19:53:12.376011",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.353750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbe6a92",
   "metadata": {
    "papermill": {
     "duration": 0.009583,
     "end_time": "2023-11-05T19:53:12.394904",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.385321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02c57c8",
   "metadata": {
    "papermill": {
     "duration": 0.007143,
     "end_time": "2023-11-05T19:53:12.409735",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.402592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d0efb2",
   "metadata": {
    "papermill": {
     "duration": 0.007189,
     "end_time": "2023-11-05T19:53:12.424646",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.417457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d9d17f",
   "metadata": {
    "papermill": {
     "duration": 0.007166,
     "end_time": "2023-11-05T19:53:12.439007",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.431841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efefdc5",
   "metadata": {
    "papermill": {
     "duration": 0.007339,
     "end_time": "2023-11-05T19:53:12.453971",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.446632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c707590",
   "metadata": {
    "papermill": {
     "duration": 0.007022,
     "end_time": "2023-11-05T19:53:12.469012",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.461990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962e156a",
   "metadata": {
    "papermill": {
     "duration": 0.007294,
     "end_time": "2023-11-05T19:53:12.483720",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.476426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "573998b2",
   "metadata": {
    "papermill": {
     "duration": 0.007213,
     "end_time": "2023-11-05T19:53:12.498622",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.491409",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Following are the three technique to get numeric vector representation of each text message:\n",
    "<br>(a) Bag-of-Words\n",
    "<br>(b) TF-IDF\n",
    "<br>(c) Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ead0f4d",
   "metadata": {
    "papermill": {
     "duration": 0.007072,
     "end_time": "2023-11-05T19:53:12.513275",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.506203",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "TF-IDF\n",
    "<br>1) The tf–idf is the product of two statistics, term frequency and inverse document frequency. There are various ways for determining the exact values of both statistics.\n",
    "<br>2) A formula that aims to define the importance of a keyword or phrase within a document or a web page.\n",
    "<br><br>Hypothesis: BoW or TF-IDF will perform poor on this dataset because all of the five options are similar to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe4b8f9",
   "metadata": {
    "papermill": {
     "duration": 0.007285,
     "end_time": "2023-11-05T19:53:12.528087",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.520802",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bow implementation on sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "760eb0a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:53:12.544468Z",
     "iopub.status.busy": "2023-11-05T19:53:12.544104Z",
     "iopub.status.idle": "2023-11-05T19:53:12.548438Z",
     "shell.execute_reply": "2023-11-05T19:53:12.547517Z"
    },
    "papermill": {
     "duration": 0.014915,
     "end_time": "2023-11-05T19:53:12.550344",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.535429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    " \n",
    "# sentence_1=\"This is a good job.I will not miss it for anything\"\n",
    "# sentence_2=\"This is not good at all\"\n",
    "# sentence_1=\"Welcome to Great Learning , Now start learning\"\n",
    "# sentence_2=\"Learning is a good practice\"\n",
    " \n",
    "# CountVec = CountVectorizer(ngram_range=(1,1), # to use bigrams ngram_range=(2,2)\n",
    "#                            stop_words='english')\n",
    "# #transform\n",
    "# Count_data = CountVec.fit_transform([sentence_1,sentence_2])\n",
    " \n",
    "# #create dataframe\n",
    "# cv_dataframe=pd.DataFrame(Count_data.toarray(),columns=CountVec.get_feature_names_out())\n",
    "# print(cv_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df4a7ab0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:53:12.568528Z",
     "iopub.status.busy": "2023-11-05T19:53:12.567815Z",
     "iopub.status.idle": "2023-11-05T19:53:12.571792Z",
     "shell.execute_reply": "2023-11-05T19:53:12.571215Z"
    },
    "papermill": {
     "duration": 0.0151,
     "end_time": "2023-11-05T19:53:12.573642",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.558542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CountVec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12d59ca7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:53:12.592028Z",
     "iopub.status.busy": "2023-11-05T19:53:12.591662Z",
     "iopub.status.idle": "2023-11-05T19:53:12.596205Z",
     "shell.execute_reply": "2023-11-05T19:53:12.594994Z"
    },
    "papermill": {
     "duration": 0.01662,
     "end_time": "2023-11-05T19:53:12.598048",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.581428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(sentence_1)\n",
    "# print(sentence_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbe70783",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:53:12.615975Z",
     "iopub.status.busy": "2023-11-05T19:53:12.614743Z",
     "iopub.status.idle": "2023-11-05T19:53:12.620766Z",
     "shell.execute_reply": "2023-11-05T19:53:12.619773Z"
    },
    "papermill": {
     "duration": 0.017034,
     "end_time": "2023-11-05T19:53:12.622976",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.605942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    " \n",
    "# sentence_1=\"This is a good job.I will not miss it for anything\"\n",
    "# sentence_2=\"This is not good at all\"\n",
    "# sentence_1=\"Welcome to Great Learning , Now start learning\"\n",
    "# sentence_2=\"Learning is a good practice\"\n",
    " \n",
    "# CountVec = CountVectorizer(ngram_range=(1,1), # to use bigrams ngram_range=(2,2)\n",
    "#                            stop_words='english')\n",
    "# #transform\n",
    "# Count_data = CountVec.fit_transform([sentence_1,sentence_2])\n",
    " \n",
    "# #create dataframe\n",
    "# cv_dataframe=pd.DataFrame(Count_data.toarray(),columns=CountVec.get_feature_names_out())\n",
    "# print(cv_dataframe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895160c2",
   "metadata": {
    "papermill": {
     "duration": 0.007498,
     "end_time": "2023-11-05T19:53:12.638551",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.631053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Limitation:\n",
    "    \n",
    "1. The model ignores the location information of the word. \n",
    "2. Bag of word models doesn’t respect the semantics of the word. e.g. ‘soccer’ and ‘football’\n",
    "3. The range of vocabulary is a big issue\n",
    "\n",
    "Source: https://www.mygreatlearning.com/blog/bag-of-words/\n",
    "Source : https://stats.stackexchange.com/questions/289400/quantify-the-similarity-of-bags-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77760a73",
   "metadata": {
    "papermill": {
     "duration": 0.00767,
     "end_time": "2023-11-05T19:53:12.654004",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.646334",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f653d86b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:53:12.672108Z",
     "iopub.status.busy": "2023-11-05T19:53:12.670680Z",
     "iopub.status.idle": "2023-11-05T19:53:12.675682Z",
     "shell.execute_reply": "2023-11-05T19:53:12.674924Z"
    },
    "papermill": {
     "duration": 0.015849,
     "end_time": "2023-11-05T19:53:12.677631",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.661782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # Create TfidfVectorizer object\n",
    "# vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a7568a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:53:12.695083Z",
     "iopub.status.busy": "2023-11-05T19:53:12.694691Z",
     "iopub.status.idle": "2023-11-05T19:53:12.699133Z",
     "shell.execute_reply": "2023-11-05T19:53:12.697582Z"
    },
    "papermill": {
     "duration": 0.015984,
     "end_time": "2023-11-05T19:53:12.701488",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.685504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vectorizer.vocabulary_\n",
    "\n",
    "## kekurangan, harus di stemming dulu biar accurate dan accurately satu vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4c78a65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:53:12.719047Z",
     "iopub.status.busy": "2023-11-05T19:53:12.718662Z",
     "iopub.status.idle": "2023-11-05T19:53:12.723596Z",
     "shell.execute_reply": "2023-11-05T19:53:12.722546Z"
    },
    "papermill": {
     "duration": 0.015759,
     "end_time": "2023-11-05T19:53:12.725477",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.709718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Generate matrix of word vectors\n",
    "# tfidf_matrix = vectorizer.fit_transform(df['prompt'])\n",
    "\n",
    "# # Print the shape of tfidf_matrix\n",
    "# print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85728402",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:53:12.743919Z",
     "iopub.status.busy": "2023-11-05T19:53:12.743348Z",
     "iopub.status.idle": "2023-11-05T19:53:12.747383Z",
     "shell.execute_reply": "2023-11-05T19:53:12.746550Z"
    },
    "papermill": {
     "duration": 0.01588,
     "end_time": "2023-11-05T19:53:12.749212",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.733332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75c16463",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:53:12.824768Z",
     "iopub.status.busy": "2023-11-05T19:53:12.824390Z",
     "iopub.status.idle": "2023-11-05T19:53:12.829461Z",
     "shell.execute_reply": "2023-11-05T19:53:12.827776Z"
    },
    "papermill": {
     "duration": 0.016255,
     "end_time": "2023-11-05T19:53:12.831572",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.815317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# indices = pd.Series(df.index, index=df['prompt']).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a11a7d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:53:12.849141Z",
     "iopub.status.busy": "2023-11-05T19:53:12.848728Z",
     "iopub.status.idle": "2023-11-05T19:53:12.852913Z",
     "shell.execute_reply": "2023-11-05T19:53:12.851929Z"
    },
    "papermill": {
     "duration": 0.015688,
     "end_time": "2023-11-05T19:53:12.855223",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.839535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #ubah jadi ambil top 3 answer yang paling relevan\n",
    "# def get_recommendations(title, cosine_sim, indices):\n",
    "#     # Get the index of the movie that matches the title\n",
    "#     idx = indices[title]\n",
    "#     # Get the pairwsie similarity scores\n",
    "#     sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "#     # Sort the movies based on the similarity scores\n",
    "#     sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "#     # Get the scores for 10 most similar movies\n",
    "#     sim_scores = sim_scores[1:11]\n",
    "#     # Get the movie indices\n",
    "#     movie_indices = [i[0] for i in sim_scores]\n",
    "#     # Return the top 10 most similar movies\n",
    "#     return metadata['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7bce2e",
   "metadata": {
    "papermill": {
     "duration": 0.007133,
     "end_time": "2023-11-05T19:53:12.870245",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.863112",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Similarity using Word2Vec and Glove Encoding\n",
    "\n",
    "https://stackoverflow.com/questions/56071689/whats-the-major-difference-between-glove-and-word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de1d710d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:53:12.886799Z",
     "iopub.status.busy": "2023-11-05T19:53:12.886218Z",
     "iopub.status.idle": "2023-11-05T19:53:12.891933Z",
     "shell.execute_reply": "2023-11-05T19:53:12.890322Z"
    },
    "papermill": {
     "duration": 0.016674,
     "end_time": "2023-11-05T19:53:12.894367",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.877693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "# doc0 = nlp(df['prompt'].loc[0])\n",
    "# doc1 = nlp(df['prompt'].loc[1])\n",
    "# doc2 = nlp(df['prompt'].loc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62715b7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:53:12.911797Z",
     "iopub.status.busy": "2023-11-05T19:53:12.911459Z",
     "iopub.status.idle": "2023-11-05T19:53:12.915491Z",
     "shell.execute_reply": "2023-11-05T19:53:12.914640Z"
    },
    "papermill": {
     "duration": 0.015364,
     "end_time": "2023-11-05T19:53:12.917792",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.902428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(doc0.similarity(doc1))\n",
    "\n",
    "# print(doc1.similarity(doc2))\n",
    "\n",
    "# print(doc0.similarity(doc2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "158377a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:53:12.936526Z",
     "iopub.status.busy": "2023-11-05T19:53:12.935268Z",
     "iopub.status.idle": "2023-11-05T19:53:12.941441Z",
     "shell.execute_reply": "2023-11-05T19:53:12.939926Z"
    },
    "papermill": {
     "duration": 0.017465,
     "end_time": "2023-11-05T19:53:12.943798",
     "exception": false,
     "start_time": "2023-11-05T19:53:12.926333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# By default spaCy calculates cosine similarity. Similarity is determined by comparing word vectors or word embeddings, multi-dimensional meaning representations of a word.\n",
    "# print(np.dot(doc1.vector, doc2.vector) / (np.linalg.norm(doc1.vector) * np.linalg.norm(doc2.vector)))\n",
    "# It seems that spaCy's .vector method created the vectors. \n",
    "# Documentation says that spaCy's models are trained from GloVe's vectors.\n",
    "\n",
    "# spaCy provides a mapping from a vocabular of common words to vectors.\n",
    "# These vectors, sometimes called \"word embeddings,\" are designed (using the GloVe algorithm) to map semantic meaning into numeric proximity.\n",
    "\n",
    "# The most notable difference between Word2vec \n",
    "# and GloVe is the training process. Word2vec uses a shallow neural network to create vectors, while GloVe uses a global matrix factorization technique."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31.714345,
   "end_time": "2023-11-05T19:53:13.873565",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-05T19:52:42.159220",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

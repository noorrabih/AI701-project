{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53a200e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:37:39.274739Z",
     "iopub.status.busy": "2023-11-06T19:37:39.274102Z",
     "iopub.status.idle": "2023-11-06T19:38:03.250121Z",
     "shell.execute_reply": "2023-11-06T19:38:03.248760Z"
    },
    "papermill": {
     "duration": 23.9913,
     "end_time": "2023-11-06T19:38:03.253091",
     "exception": false,
     "start_time": "2023-11-06T19:37:39.261791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n",
      "Archive:  /usr/share/nltk_data/corpora/wordnet.zip\r\n",
      "   creating: /usr/share/nltk_data/corpora/wordnet/\r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/README  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b0693c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:03.277784Z",
     "iopub.status.busy": "2023-11-06T19:38:03.276959Z",
     "iopub.status.idle": "2023-11-06T19:38:03.306697Z",
     "shell.execute_reply": "2023-11-06T19:38:03.305705Z"
    },
    "papermill": {
     "duration": 0.044863,
     "end_time": "2023-11-06T19:38:03.309279",
     "exception": false,
     "start_time": "2023-11-06T19:38:03.264416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66eedcef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:03.332879Z",
     "iopub.status.busy": "2023-11-06T19:38:03.332499Z",
     "iopub.status.idle": "2023-11-06T19:38:03.337737Z",
     "shell.execute_reply": "2023-11-06T19:38:03.336618Z"
    },
    "papermill": {
     "duration": 0.01981,
     "end_time": "2023-11-06T19:38:03.340091",
     "exception": false,
     "start_time": "2023-11-06T19:38:03.320281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sub = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "873eb926",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:03.363888Z",
     "iopub.status.busy": "2023-11-06T19:38:03.363506Z",
     "iopub.status.idle": "2023-11-06T19:38:03.371088Z",
     "shell.execute_reply": "2023-11-06T19:38:03.369880Z"
    },
    "papermill": {
     "duration": 0.022384,
     "end_time": "2023-11-06T19:38:03.373519",
     "exception": false,
     "start_time": "2023-11-06T19:38:03.351135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_jaccard_sim(a, b):\n",
    "    mult1 = np.array(a)*np.array(b)    \n",
    "    mult1 = mult1 > 0 \n",
    "    mult2 = np.array(a) + np.array(b)\n",
    "    mult2 = mult2 > 0 \n",
    "    return float(sum(mult1)/sum(mult2))\n",
    "\n",
    "def cosine_sim(a,b):\n",
    "    denum = np.linalg.norm(a)*np.linalg.norm(b)\n",
    "    num = np.dot(a,b)\n",
    "    if denum == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return num/denum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df180506",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:03.397239Z",
     "iopub.status.busy": "2023-11-06T19:38:03.396525Z",
     "iopub.status.idle": "2023-11-06T19:38:07.700649Z",
     "shell.execute_reply": "2023-11-06T19:38:07.699482Z"
    },
    "papermill": {
     "duration": 4.318978,
     "end_time": "2023-11-06T19:38:07.703461",
     "exception": false,
     "start_time": "2023-11-06T19:38:03.384483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lower(text):\n",
    "    return text.lower()\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "def preprocess(text):\n",
    "    text = lower(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize_words(text)\n",
    "    return text\n",
    "\n",
    "df_sub['prompt'] = df[\"prompt\"].apply(lambda text: preprocess(text))\n",
    "df_sub['A'] = df[\"A\"].apply(lambda text: preprocess(text))\n",
    "df_sub['B'] = df[\"B\"].apply(lambda text: preprocess(text))\n",
    "df_sub['C'] = df[\"C\"].apply(lambda text: preprocess(text))\n",
    "df_sub['D'] = df[\"D\"].apply(lambda text: preprocess(text))\n",
    "df_sub['E'] = df[\"E\"].apply(lambda text: preprocess(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90f96421",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:07.727988Z",
     "iopub.status.busy": "2023-11-06T19:38:07.726845Z",
     "iopub.status.idle": "2023-11-06T19:38:07.734757Z",
     "shell.execute_reply": "2023-11-06T19:38:07.733957Z"
    },
    "papermill": {
     "duration": 0.022334,
     "end_time": "2023-11-06T19:38:07.737009",
     "exception": false,
     "start_time": "2023-11-06T19:38:07.714675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vocabs(df, col):\n",
    "    split_col = df[col].apply(lambda x: x.split())\n",
    "    uniq = list()\n",
    "    for row in split_col:\n",
    "        uniq = uniq + row\n",
    "    return list(set(uniq))\n",
    "\n",
    "uniq_vocabs = vocabs(df_sub,'prompt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a53b912f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:07.760026Z",
     "iopub.status.busy": "2023-11-06T19:38:07.759431Z",
     "iopub.status.idle": "2023-11-06T19:38:07.764655Z",
     "shell.execute_reply": "2023-11-06T19:38:07.763748Z"
    },
    "papermill": {
     "duration": 0.019469,
     "end_time": "2023-11-06T19:38:07.767033",
     "exception": false,
     "start_time": "2023-11-06T19:38:07.747564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vectorize(tokens, filtered_vocabs):\n",
    "    ''' This function takes list of words in a sentence as input \n",
    "    and returns a vector of size of filtered_vocab.It puts 0 if the \n",
    "    word is not present in tokens and count of token if present.'''\n",
    "    vector=[]\n",
    "    for w in filtered_vocabs:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fef8e174",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:07.790497Z",
     "iopub.status.busy": "2023-11-06T19:38:07.789515Z",
     "iopub.status.idle": "2023-11-06T19:38:08.212616Z",
     "shell.execute_reply": "2023-11-06T19:38:08.211509Z"
    },
    "papermill": {
     "duration": 0.437908,
     "end_time": "2023-11-06T19:38:08.215578",
     "exception": false,
     "start_time": "2023-11-06T19:38:07.777670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sub['prompt'] =  df_sub['prompt'].apply(lambda x: vectorize(x.split(), uniq_vocabs))\n",
    "df_sub['A'] =  df_sub['A'].apply(lambda x: vectorize(x.split(), uniq_vocabs))\n",
    "df_sub['B'] =  df_sub['B'].apply(lambda x: vectorize(x.split(), uniq_vocabs))\n",
    "df_sub['C'] =  df_sub['C'].apply(lambda x: vectorize(x.split(), uniq_vocabs))\n",
    "df_sub['D'] =  df_sub['D'].apply(lambda x: vectorize(x.split(), uniq_vocabs))\n",
    "df_sub['E'] =  df_sub['E'].apply(lambda x: vectorize(x.split(), uniq_vocabs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ff31ded",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:08.239027Z",
     "iopub.status.busy": "2023-11-06T19:38:08.238662Z",
     "iopub.status.idle": "2023-11-06T19:38:09.187073Z",
     "shell.execute_reply": "2023-11-06T19:38:09.185843Z"
    },
    "papermill": {
     "duration": 0.964023,
     "end_time": "2023-11-06T19:38:09.190392",
     "exception": false,
     "start_time": "2023-11-06T19:38:08.226369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "choice = np.array(['A','B','C','D','E'])\n",
    "\n",
    "def predict(row, prompt, options):\n",
    "    sel = []\n",
    "    for opt in options:\n",
    "        score = get_jaccard_sim(row[prompt],row[opt])\n",
    "        sel.append(score)\n",
    "    order = np.argsort(np.array(sel))[::-1]\n",
    "    return ' '.join(choice[order][:3])\n",
    "        \n",
    "df_sub['prediction'] = df_sub.apply(lambda row: predict(row,'prompt',choice), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf6f3483",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:09.214840Z",
     "iopub.status.busy": "2023-11-06T19:38:09.214135Z",
     "iopub.status.idle": "2023-11-06T19:38:09.772215Z",
     "shell.execute_reply": "2023-11-06T19:38:09.771207Z"
    },
    "papermill": {
     "duration": 0.573426,
     "end_time": "2023-11-06T19:38:09.774946",
     "exception": false,
     "start_time": "2023-11-06T19:38:09.201520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(row, prompt, options):\n",
    "    sel = []\n",
    "    for opt in options:\n",
    "        score = cosine_sim(row[prompt],row[opt])\n",
    "        sel.append(score)\n",
    "    order = np.argsort(np.array(sel))[::-1]\n",
    "    return ' '.join(choice[order][:3])\n",
    "\n",
    "df_sub['prediction_2'] = df_sub.apply(lambda row: predict(row,'prompt',choice), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fa9b73",
   "metadata": {
    "papermill": {
     "duration": 0.01036,
     "end_time": "2023-11-06T19:38:09.796868",
     "exception": false,
     "start_time": "2023-11-06T19:38:09.786508",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Following are the three technique to get numeric vector representation of each text message:\n",
    "<br>(a) Bag-of-Words\n",
    "<br>(b) TF-IDF\n",
    "<br>(c) Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3852f7",
   "metadata": {
    "papermill": {
     "duration": 0.010893,
     "end_time": "2023-11-06T19:38:09.818548",
     "exception": false,
     "start_time": "2023-11-06T19:38:09.807655",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "TF-IDF\n",
    "<br>1) The tf–idf is the product of two statistics, term frequency and inverse document frequency. There are various ways for determining the exact values of both statistics.\n",
    "<br>2) A formula that aims to define the importance of a keyword or phrase within a document or a web page.\n",
    "<br><br>Hypothesis: BoW or TF-IDF will perform poor on this dataset because all of the five options are similar to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea212e7c",
   "metadata": {
    "papermill": {
     "duration": 0.010565,
     "end_time": "2023-11-06T19:38:09.840161",
     "exception": false,
     "start_time": "2023-11-06T19:38:09.829596",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bow implementation on sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9646436a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:09.864900Z",
     "iopub.status.busy": "2023-11-06T19:38:09.863879Z",
     "iopub.status.idle": "2023-11-06T19:38:09.901479Z",
     "shell.execute_reply": "2023-11-06T19:38:09.900516Z"
    },
    "papermill": {
     "duration": 0.052701,
     "end_time": "2023-11-06T19:38:09.904139",
     "exception": false,
     "start_time": "2023-11-06T19:38:09.851438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lower(text):\n",
    "    return text.lower()\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "def preprocess(text):\n",
    "    text = lower(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "#     text = lemmatize_words(text)\n",
    "    return text\n",
    "\n",
    "df_sub['prompt'] = df[\"prompt\"].apply(lambda text: preprocess(text))\n",
    "df_sub['A'] = df[\"A\"].apply(lambda text: preprocess(text))\n",
    "df_sub['B'] = df[\"B\"].apply(lambda text: preprocess(text))\n",
    "df_sub['C'] = df[\"C\"].apply(lambda text: preprocess(text))\n",
    "df_sub['D'] = df[\"D\"].apply(lambda text: preprocess(text))\n",
    "df_sub['E'] = df[\"E\"].apply(lambda text: preprocess(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6cbdaf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:09.927458Z",
     "iopub.status.busy": "2023-11-06T19:38:09.927029Z",
     "iopub.status.idle": "2023-11-06T19:38:09.938212Z",
     "shell.execute_reply": "2023-11-06T19:38:09.937146Z"
    },
    "papermill": {
     "duration": 0.025585,
     "end_time": "2023-11-06T19:38:09.940597",
     "exception": false,
     "start_time": "2023-11-06T19:38:09.915012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "CountVec = CountVectorizer(ngram_range=(2,2), # to use bigrams ngram_range=(2,2)\n",
    "                           stop_words='english')\n",
    "\n",
    "choice = np.array(['A','B','C','D','E'])\n",
    "#transform\n",
    "def predict2(row, prompt):\n",
    "    Count_data = CountVec.fit_transform([row[prompt],row['A'],row['B'],row['C'], row['D'], row['E']])\n",
    "    cv_dataframe=pd.DataFrame(Count_data.toarray(),columns=CountVec.get_feature_names_out())\n",
    "#     return cv_dataframe\n",
    "    score = []\n",
    "    score.append(cosine_sim(cv_dataframe.loc[0], cv_dataframe.loc[1]))\n",
    "    score.append(cosine_sim(cv_dataframe.loc[0], cv_dataframe.loc[2]))\n",
    "    score.append(cosine_sim(cv_dataframe.loc[0], cv_dataframe.loc[3]))\n",
    "    score.append(cosine_sim(cv_dataframe.loc[0], cv_dataframe.loc[4]))\n",
    "    score.append(cosine_sim(cv_dataframe.loc[0], cv_dataframe.loc[5]))\n",
    "    score_idx = np.argsort(np.array(score))[::-1][:3]\n",
    "    score = choice[score_idx]\n",
    "    return ' '.join(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "960081e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:09.963800Z",
     "iopub.status.busy": "2023-11-06T19:38:09.963388Z",
     "iopub.status.idle": "2023-11-06T19:38:10.387101Z",
     "shell.execute_reply": "2023-11-06T19:38:10.385792Z"
    },
    "papermill": {
     "duration": 0.438651,
     "end_time": "2023-11-06T19:38:10.390075",
     "exception": false,
     "start_time": "2023-11-06T19:38:09.951424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_col = 'prediction_3'\n",
    "df_sub[pred_col] = df_sub.apply(lambda row: predict2(row,'prompt'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "938c7a31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:10.414141Z",
     "iopub.status.busy": "2023-11-06T19:38:10.413432Z",
     "iopub.status.idle": "2023-11-06T19:38:11.513807Z",
     "shell.execute_reply": "2023-11-06T19:38:11.512418Z"
    },
    "papermill": {
     "duration": 1.115674,
     "end_time": "2023-11-06T19:38:11.516609",
     "exception": false,
     "start_time": "2023-11-06T19:38:10.400935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook__.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eba5750b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:11.540584Z",
     "iopub.status.busy": "2023-11-06T19:38:11.540165Z",
     "iopub.status.idle": "2023-11-06T19:38:11.554928Z",
     "shell.execute_reply": "2023-11-06T19:38:11.553814Z"
    },
    "papermill": {
     "duration": 0.030086,
     "end_time": "2023-11-06T19:38:11.557489",
     "exception": false,
     "start_time": "2023-11-06T19:38:11.527403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame()\n",
    "submission_df['id'] = df_sub['id']\n",
    "submission_df['prediction'] = df_sub[pred_col]\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f87978e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:11.581867Z",
     "iopub.status.busy": "2023-11-06T19:38:11.581455Z",
     "iopub.status.idle": "2023-11-06T19:38:12.674088Z",
     "shell.execute_reply": "2023-11-06T19:38:12.672639Z"
    },
    "papermill": {
     "duration": 1.108437,
     "end_time": "2023-11-06T19:38:12.676916",
     "exception": false,
     "start_time": "2023-11-06T19:38:11.568479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook__.ipynb  submission.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7eae45",
   "metadata": {
    "papermill": {
     "duration": 0.010383,
     "end_time": "2023-11-06T19:38:12.698122",
     "exception": false,
     "start_time": "2023-11-06T19:38:12.687739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14e4d88d",
   "metadata": {
    "papermill": {
     "duration": 0.010541,
     "end_time": "2023-11-06T19:38:12.719191",
     "exception": false,
     "start_time": "2023-11-06T19:38:12.708650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Limitation:\n",
    "    \n",
    "1. The model ignores the location information of the word. \n",
    "2. Bag of word models doesn’t respect the semantics of the word. e.g. ‘soccer’ and ‘football’\n",
    "3. The range of vocabulary is a big issue\n",
    "\n",
    "Source: https://www.mygreatlearning.com/blog/bag-of-words/\n",
    "Source : https://stats.stackexchange.com/questions/289400/quantify-the-similarity-of-bags-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32c3ce02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:12.742700Z",
     "iopub.status.busy": "2023-11-06T19:38:12.742235Z",
     "iopub.status.idle": "2023-11-06T19:38:13.826772Z",
     "shell.execute_reply": "2023-11-06T19:38:13.825590Z"
    },
    "papermill": {
     "duration": 1.099546,
     "end_time": "2023-11-06T19:38:13.829455",
     "exception": false,
     "start_time": "2023-11-06T19:38:12.729909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'=2.*'\t      entrypoint.sh\t    lib      mnt    run_jupyter.sh   usr\r\n",
      "'=2023.0.1'   etc\t\t    lib32    opt    sbin\t     var\r\n",
      " bin\t      home\t\t    lib64    proc   srv\r\n",
      " boot\t      install_packages.sh   libx32   root   sys\r\n",
      " dev\t      kaggle\t\t    media    run    tmp\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d7e674",
   "metadata": {
    "papermill": {
     "duration": 0.010605,
     "end_time": "2023-11-06T19:38:13.851213",
     "exception": false,
     "start_time": "2023-11-06T19:38:13.840608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd14ba6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:13.875138Z",
     "iopub.status.busy": "2023-11-06T19:38:13.874684Z",
     "iopub.status.idle": "2023-11-06T19:38:13.879805Z",
     "shell.execute_reply": "2023-11-06T19:38:13.878719Z"
    },
    "papermill": {
     "duration": 0.020122,
     "end_time": "2023-11-06T19:38:13.882122",
     "exception": false,
     "start_time": "2023-11-06T19:38:13.862000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # Create TfidfVectorizer object\n",
    "# vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b64694d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:13.905459Z",
     "iopub.status.busy": "2023-11-06T19:38:13.905040Z",
     "iopub.status.idle": "2023-11-06T19:38:13.909789Z",
     "shell.execute_reply": "2023-11-06T19:38:13.908490Z"
    },
    "papermill": {
     "duration": 0.019424,
     "end_time": "2023-11-06T19:38:13.912181",
     "exception": false,
     "start_time": "2023-11-06T19:38:13.892757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vectorizer.vocabulary_\n",
    "\n",
    "## kekurangan, harus di stemming dulu biar accurate dan accurately satu vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10a22c84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:13.935747Z",
     "iopub.status.busy": "2023-11-06T19:38:13.935347Z",
     "iopub.status.idle": "2023-11-06T19:38:13.940276Z",
     "shell.execute_reply": "2023-11-06T19:38:13.939022Z"
    },
    "papermill": {
     "duration": 0.020037,
     "end_time": "2023-11-06T19:38:13.943125",
     "exception": false,
     "start_time": "2023-11-06T19:38:13.923088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Generate matrix of word vectors\n",
    "# tfidf_matrix = vectorizer.fit_transform(df['prompt'])\n",
    "\n",
    "# # Print the shape of tfidf_matrix\n",
    "# print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67445a93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:13.966887Z",
     "iopub.status.busy": "2023-11-06T19:38:13.966506Z",
     "iopub.status.idle": "2023-11-06T19:38:13.971604Z",
     "shell.execute_reply": "2023-11-06T19:38:13.970292Z"
    },
    "papermill": {
     "duration": 0.01925,
     "end_time": "2023-11-06T19:38:13.973682",
     "exception": false,
     "start_time": "2023-11-06T19:38:13.954432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fe16b86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:13.998068Z",
     "iopub.status.busy": "2023-11-06T19:38:13.997636Z",
     "iopub.status.idle": "2023-11-06T19:38:14.002604Z",
     "shell.execute_reply": "2023-11-06T19:38:14.001567Z"
    },
    "papermill": {
     "duration": 0.020154,
     "end_time": "2023-11-06T19:38:14.004897",
     "exception": false,
     "start_time": "2023-11-06T19:38:13.984743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# indices = pd.Series(df.index, index=df['prompt']).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6f878a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:14.029271Z",
     "iopub.status.busy": "2023-11-06T19:38:14.028817Z",
     "iopub.status.idle": "2023-11-06T19:38:14.034602Z",
     "shell.execute_reply": "2023-11-06T19:38:14.033434Z"
    },
    "papermill": {
     "duration": 0.020994,
     "end_time": "2023-11-06T19:38:14.036921",
     "exception": false,
     "start_time": "2023-11-06T19:38:14.015927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #ubah jadi ambil top 3 answer yang paling relevan\n",
    "# def get_recommendations(title, cosine_sim, indices):\n",
    "#     # Get the index of the movie that matches the title\n",
    "#     idx = indices[title]\n",
    "#     # Get the pairwsie similarity scores\n",
    "#     sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "#     # Sort the movies based on the similarity scores\n",
    "#     sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "#     # Get the scores for 10 most similar movies\n",
    "#     sim_scores = sim_scores[1:11]\n",
    "#     # Get the movie indices\n",
    "#     movie_indices = [i[0] for i in sim_scores]\n",
    "#     # Return the top 10 most similar movies\n",
    "#     return metadata['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244a44b5",
   "metadata": {
    "papermill": {
     "duration": 0.010811,
     "end_time": "2023-11-06T19:38:14.058932",
     "exception": false,
     "start_time": "2023-11-06T19:38:14.048121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Similarity using Word2Vec and Glove Encoding\n",
    "\n",
    "https://stackoverflow.com/questions/56071689/whats-the-major-difference-between-glove-and-word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb816998",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:14.082345Z",
     "iopub.status.busy": "2023-11-06T19:38:14.081884Z",
     "iopub.status.idle": "2023-11-06T19:38:14.086705Z",
     "shell.execute_reply": "2023-11-06T19:38:14.085556Z"
    },
    "papermill": {
     "duration": 0.019877,
     "end_time": "2023-11-06T19:38:14.089478",
     "exception": false,
     "start_time": "2023-11-06T19:38:14.069601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "# doc0 = nlp(df['prompt'].loc[0])\n",
    "# doc1 = nlp(df['prompt'].loc[1])\n",
    "# doc2 = nlp(df['prompt'].loc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "041c6c7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:14.113394Z",
     "iopub.status.busy": "2023-11-06T19:38:14.112936Z",
     "iopub.status.idle": "2023-11-06T19:38:14.118063Z",
     "shell.execute_reply": "2023-11-06T19:38:14.116994Z"
    },
    "papermill": {
     "duration": 0.019964,
     "end_time": "2023-11-06T19:38:14.120557",
     "exception": false,
     "start_time": "2023-11-06T19:38:14.100593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(doc0.similarity(doc1))\n",
    "\n",
    "# print(doc1.similarity(doc2))\n",
    "\n",
    "# print(doc0.similarity(doc2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f249e407",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:38:14.209765Z",
     "iopub.status.busy": "2023-11-06T19:38:14.209312Z",
     "iopub.status.idle": "2023-11-06T19:38:14.215185Z",
     "shell.execute_reply": "2023-11-06T19:38:14.213664Z"
    },
    "papermill": {
     "duration": 0.085569,
     "end_time": "2023-11-06T19:38:14.217597",
     "exception": false,
     "start_time": "2023-11-06T19:38:14.132028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# By default spaCy calculates cosine similarity. Similarity is determined by comparing word vectors or word embeddings, multi-dimensional meaning representations of a word.\n",
    "# print(np.dot(doc1.vector, doc2.vector) / (np.linalg.norm(doc1.vector) * np.linalg.norm(doc2.vector)))\n",
    "# It seems that spaCy's .vector method created the vectors. \n",
    "# Documentation says that spaCy's models are trained from GloVe's vectors.\n",
    "\n",
    "# spaCy provides a mapping from a vocabular of common words to vectors.\n",
    "# These vectors, sometimes called \"word embeddings,\" are designed (using the GloVe algorithm) to map semantic meaning into numeric proximity.\n",
    "\n",
    "# The most notable difference between Word2vec \n",
    "# and GloVe is the training process. Word2vec uses a shallow neural network to create vectors, while GloVe uses a global matrix factorization technique."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 39.598088,
   "end_time": "2023-11-06T19:38:15.253036",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-06T19:37:35.654948",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
